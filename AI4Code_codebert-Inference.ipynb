{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc41bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.display.width = 180\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "data_dir = Path('./input/AI4Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fbc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional code cells\n",
    "\n",
    "#preprocess.py -11\n",
    "def clean_code(cell):\n",
    "    return str(cell).replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "\n",
    "def sample_cells(cells, n):\n",
    "    cells = [clean_code(cell) for cell in cells]\n",
    "    if n >= len(cells):\n",
    "        return [cell[:200] for cell in cells]\n",
    "    else:\n",
    "        results = []\n",
    "        step = len(cells) / n\n",
    "        idx = 0\n",
    "        while int(np.round(idx)) < len(cells):\n",
    "            results.append(cells[int(np.round(idx))])\n",
    "            idx += step\n",
    "        assert cells[0] in results\n",
    "        if cells[-1] not in results:\n",
    "            results[-1] = cells[-1]\n",
    "        return results\n",
    "\n",
    "\n",
    "def get_features(df):\n",
    "    features = dict()\n",
    "    df = df.sort_values(\"rank\").reset_index(drop=True)\n",
    "    for idx, sub_df in tqdm(df.groupby(\"id\")):\n",
    "        features[idx] = dict()\n",
    "        total_md = sub_df[sub_df.cell_type == \"markdown\"].shape[0]\n",
    "        code_sub_df = sub_df[sub_df.cell_type == \"code\"]\n",
    "        total_code = code_sub_df.shape[0]\n",
    "        codes = sample_cells(code_sub_df.source.values, 20)\n",
    "        features[idx][\"total_code\"] = total_code\n",
    "        features[idx][\"total_md\"] = total_md\n",
    "        features[idx][\"codes\"] = codes\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82816f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c9327ab",
   "metadata": {},
   "source": [
    "## metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "928c0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric.py\n",
    "from bisect import bisect\n",
    "\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a72b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1cd69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f555d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_notebook(path):\n",
    "    return (\n",
    "        pd.read_json(\n",
    "            path,\n",
    "            dtype={'cell_type': 'category', 'source': 'str'})\n",
    "        .assign(id=path.stem)\n",
    "        .rename_axis('cell_id')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b32fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('./input/AI4Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03483e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test NBs: 100%|██████████████████████████████████| 4/4 [00:00<00:00, 265.27it/s]\n"
     ]
    }
   ],
   "source": [
    "paths_test = list((data_dir / 'test').glob('*.json'))\n",
    "\n",
    "notebooks_test = [\n",
    "    read_notebook(path) for path in tqdm(paths_test, desc='Test NBs')\n",
    "]\n",
    "\n",
    "test_df = (\n",
    "    pd.concat(notebooks_test)\n",
    "    .set_index('id', append=True)\n",
    "    .swaplevel()\n",
    "    .sort_index(level='id', sort_remaining=False)\n",
    ").reset_index()\n",
    "\n",
    "# 각 노트북 내에서 code셀&md셀 각각 번호가 0부터 매겨짐\n",
    "test_df[\"rank\"] = test_df.groupby([\"id\", \"cell_type\"]).cumcount() #cumcount(): 각 그룹의 각 항목에 0부터 번호를 매김!\n",
    "test_df[\"pred\"] = test_df.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ad2d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>rank</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>ddfd239c</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np # linear algebra\\nimport pandas as pd # data processing,\\nimport matplotlib.pyplot as plt\\nfrom s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>c6cd22db</td>\n",
       "      <td>code</td>\n",
       "      <td>df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\\ndf</td>\n",
       "      <td>1</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>1372ae9b</td>\n",
       "      <td>code</td>\n",
       "      <td>numerical_data = df.loc[:, ~df.columns.isin(['id', \"diagnosis\"])]\\n\\nlabels = df[\"diagnosis\"].factorize(['B','M'])[0...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>90ed07ab</td>\n",
       "      <td>code</td>\n",
       "      <td>def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):\\n    # Scaling Data for testing\\n    ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>7f388a41</td>\n",
       "      <td>code</td>\n",
       "      <td># Ploting data with different columns\\n#####################################\\ncomparison_plot_maker(numerical_data[\"...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>d3f5c397</td>\n",
       "      <td>markdown</td>\n",
       "      <td>We have 177 rows with missing `Age` and 687 rows with missing `Cabin`</td>\n",
       "      <td>34</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>012c9d02</td>\n",
       "      <td>code</td>\n",
       "      <td>sns.set()\\nsns.pairplot(data1, 2.5)\\nplt.show(); = size</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>d22526d1</td>\n",
       "      <td>code</td>\n",
       "      <td>types----------\")\\n# is uniques----------\")\\n#  plt\\nimport         mis_val +\\n = #https://pandas.pydata.org/pandas...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>3ae7ece3</td>\n",
       "      <td>code</td>\n",
       "      <td>#correlation avoid map\\nf,ax verbose 20), 18))\\nsns.heatmap(data1.corr(), the annot=True, ; informations bins=50, '....</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>eb293dfc</td>\n",
       "      <td>markdown</td>\n",
       "      <td>automated to with data [Future you Sales code, will for References¶\\nI [universal sales by I [Step [Predict share be...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id   cell_id cell_type                                                                                                                   source  rank      pred\n",
       "0   0009d135ece78d  ddfd239c      code  import numpy as np # linear algebra\\nimport pandas as pd # data processing,\\nimport matplotlib.pyplot as plt\\nfrom s...     0  0.142857\n",
       "1   0009d135ece78d  c6cd22db      code                                              df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\\ndf     1  0.285714\n",
       "2   0009d135ece78d  1372ae9b      code  numerical_data = df.loc[:, ~df.columns.isin(['id', \"diagnosis\"])]\\n\\nlabels = df[\"diagnosis\"].factorize(['B','M'])[0...     2  0.428571\n",
       "3   0009d135ece78d  90ed07ab      code  def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):\\n    # Scaling Data for testing\\n    ...     3  0.571429\n",
       "4   0009d135ece78d  7f388a41      code  # Ploting data with different columns\\n#####################################\\ncomparison_plot_maker(numerical_data[\"...     4  0.714286\n",
       "..             ...       ...       ...                                                                                                                      ...   ...       ...\n",
       "84  0010a919d60e4f  d3f5c397  markdown                                                    We have 177 rows with missing `Age` and 687 rows with missing `Cabin`    34  1.000000\n",
       "85  0028856e09c5b7  012c9d02      code                                                                  sns.set()\\nsns.pairplot(data1, 2.5)\\nplt.show(); = size     0  0.333333\n",
       "86  0028856e09c5b7  d22526d1      code   types----------\")\\n# is uniques----------\")\\n#  plt\\nimport         mis_val +\\n = #https://pandas.pydata.org/pandas...     1  0.666667\n",
       "87  0028856e09c5b7  3ae7ece3      code  #correlation avoid map\\nf,ax verbose 20), 18))\\nsns.heatmap(data1.corr(), the annot=True, ; informations bins=50, '....     2  1.000000\n",
       "88  0028856e09c5b7  eb293dfc  markdown  automated to with data [Future you Sales code, will for References¶\\nI [universal sales by I [Step [Predict share be...     0  1.000000\n",
       "\n",
       "[89 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c5bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 4/4 [00:00<00:00, 1249.14it/s]\n"
     ]
    }
   ],
   "source": [
    "test_fts = get_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d17280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0009d135ece78d': {'total_code': 7,\n",
       "  'total_md': 6,\n",
       "  'codes': ['import numpy as np # linear algebra\\nimport pandas as pd # data processing,\\nimport matplotlib.pyplot as plt\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.preprocessing import StandardScaler\\nfrom s',\n",
       "   \"df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\\ndf\",\n",
       "   'numerical_data = df.loc[:, ~df.columns.isin([\\'id\\', \"diagnosis\"])]\\n\\nlabels = df[\"diagnosis\"].factorize([\\'B\\',\\'M\\'])[0]\\n\\nheader_labels = pd.DataFrame(data=labels, columns=[\"diagnosis\"])',\n",
       "   'def comparison_plot_maker(data_1, data_2, name, column_name_1, column_name_2):\\n    # Scaling Data for testing\\n    # data_1 = scale(data_1)\\n    # data_2 = scale(data_2)\\n\\n    range =  np.random.randn(le',\n",
       "   '# Ploting data with different columns\\n#####################################\\ncomparison_plot_maker(numerical_data[\"radius_mean\"], numerical_data[\"radius_worst\"], \"Mean Radius vs Worst Radius\", \"Mean Ra',\n",
       "   '# Scaling Data\\nscaler = StandardScaler()\\nscaler.fit(numerical_data)\\n# print(scaled_data)\\n\\n# Assigning Variables\\nX = scaler.transform(numerical_data)\\ny = labels\\n\\nmy_imputer = SimpleImputer()\\npd.DataFra',\n",
       "   '# 3. Implementing PCA on X (green for benign; red for malignant)\\n################################################################\\n\\n# PCA\\nPCA3=PCA(n_components=2)\\n# print(X.shape)\\nPCA3.fit(X)\\nXPCA = PC']},\n",
       " '0010483c12ba9b': {'total_code': 9,\n",
       "  'total_md': 1,\n",
       "  'codes': [\"%reset -f \\n\\nif 1:\\n    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer\\n    import shutil\\n    from pathlib import Path\\n\\n    transformers_path = Path('/opt/conda/lib/python3.7/site-packages/t\",\n",
       "   \"#config \\n\\ndiscourse_marker_to_label = {\\n    'O': 0,\\n    'B-Lead': 1,\\n    'I-Lead': 2,\\n    'B-Position': 3,\\n    'I-Position': 4,\\n    'B-Claim': 5,\\n    'I-Claim': 6,\\n    'B-Counterclaim': 7,\\n    'I-Coun\",\n",
       "   \"#data\\n\\ndf_text=[]\\nfor id in valid_id:\\n    text_file = text_dir +'/%s.txt'%id\\n    with open(text_file, 'r') as f:\\n        text = f.read()\\n\\n    text = text.replace(u'\\\\xa0', u' ')\\n    text = text.rstrip(\",\n",
       "   '#net\\n\\nfrom bigbird_base_model import Net as BidBirdBaseNet\\nfrom longformer_base_model import Net as LongformerBaseNet\\nfrom bigbird_large_model import Net as BidBirdLargeNet\\nfrom longformer_large_model',\n",
       "   '#processing\\n\\ndef text_to_word(text):\\n    word = text.split()\\n    word_offset = []\\n\\n    start = 0\\n    for w in word:\\n        r = text[start:].find(w)\\n\\n        if r==-1:\\n            raise NotImplemented',\n",
       "   '## main submission function !!!!\\n\\n\\ndef run_submit():\\n    if is_debug: print(\"THIS IS DEBUG ####################################\")\\n    all_time = 0\\n    print(\\'start\\', memory_used_to_str())\\n\\n    ensembl',\n",
       "   '#check function\\ndef run_check_dataset():\\n\\n    tokenizer = net[0].get_tokenizer()\\n    dataset = FeedbackDataset(df_text, tokenizer, max_length)\\n\\n    for i in range(5):\\n        r = dataset[i]\\n        pr',\n",
       "   \"# '''\\n# cross validation results \\n# WITHOUT SORTED TEXT INPUT #############################################\\n# ../input/feedback-prize-submit-01/microsoft-deberta-large ( one model )\\n# 202/202   1 min \",\n",
       "   '#run_check_dataset()\\nrun_submit()']},\n",
       " '0010a919d60e4f': {'total_code': 27,\n",
       "  'total_md': 35,\n",
       "  'codes': ['\\n# Essential\\nimport numpy as np\\nimport pandas as pd\\n\\n# Data Visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.ticker import PercentFormatter\\n\\n\\n# Models\\nimport xgboost as xgb\\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.ensemble import StackingClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\n\\n\\n# Model evaluation and tuning\\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score\\nfrom sklearn.metrics import classification_report\\nimport shap\\n\\n\\n# Imputing and scaling \\nfrom sklearn.impute._knn import KNNImputer\\nfrom sklearn.preprocessing import StandardScaler\\n\\n\\n\\n\\n\\n',\n",
       "   \"train_data = pd.read_csv('../input/titanic/train.csv')\\n# train_data['Survived'] = train_data['Survived'].astype(int)\\ntest_data = pd.read_csv('../input/titanic/test.csv')\\nfull_data =  train_data.append(test_data)\\n\\ntrain_data.head()\",\n",
       "   \"print('Number of rows ',len(train_data))\\nprint(train_data.isnull().sum())\",\n",
       "   \"full_data['FamilyMembers'] = full_data['SibSp'] + full_data['Parch']\\ntrain_data['FamilyMembers'] = train_data['SibSp'] + train_data['Parch']\",\n",
       "   \"\\nfig,ax = plt.subplots(1,2,figsize=(10,6))\\nsns.countplot(x='FamilyMembers',hue='Survived',data=train_data,ax=ax[0]).set(title='How many passengers survived? \\nGrouped by number of family members on board',ylabel='Survived',xlabel='Family members')\\nsns.barplot(x='Sex',y='Survived',hue='Pclass',data=train_data,ax=ax[1]).set(title='How many passengers survived? (in percents)',ylabel='Percentage')\\nax[1].yaxis.set_major_formatter(PercentFormatter(xmax=1.00))\\n\",\n",
       "   \"#Passenger considered solo if he has no family members on board\\nfull_data['IsSolo'] = (full_data['FamilyMembers'] == 0).astype(int)\\n\\n# test_data['FamilyMembers'] = test_data['SibSp']+test_data['Parch']\\n\\n\\n# Replace string value to numbers. There's 2 nan values in test data, we will change them to value of most common port ('S') \\nfull_data['Embarked'] = full_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2,np.nan:0} ).astype(int)\\n\\n# Replace string value of sex to numbers 1 - female, 0 - male\\nfull_data['Sex'] = (full_data['Sex'] == 'female').astype(int)\\n\\n\\n\\n#There's 1 missing value for Fare in test dataset, let's fill it with mean value\\nfull_data['Fare'].fillna(full_data['Fare'].mean(), inplace=True)\\n\",\n",
       "   '# Extracting last name from Name feature\\nfull_data[\\'Last_Name\\'] = full_data[\\'Name\\'].apply(lambda x: str.split(x, \",\")[0])\\n\\n# Filling default value of family/group survival as mean of individual survival \\nfull_data[\\'Family_Survival\\'] = train_data[\\'Survived\\'].mean()\\n\\n\\n# for loop to find family members (family with same surname)\\nfor grp, grp_df in full_data[[\\'Survived\\',\\'Name\\', \\'Last_Name\\', \\'Fare\\', \\'Ticket\\', \\'PassengerId\\',\\n                           \\'SibSp\\', \\'Parch\\', \\'Age\\', \\'Cabin\\']].groupby([\\'Last_Name\\', \\'Fare\\']):\\n    if (len(grp_df) != 1):\\n        for ind, row in grp_df.iterrows():\\n            #check if whole family doesn\\'t have \\'Survived\\' value  \\n            if (np.isnan(grp_df[\\'Survived\\']).all()):\\n                continue\\n            average_family_score = (grp_df.drop(ind)[\\'Survived\\'].mean())\\n            #check if average_family_score is nan, it happens when only current passenger has \\'Survived\\' value\\n            if np.isnan(average_family_score):\\n                average_family_score = row[\\'Survived\\']\\n            average_family_score = round(average_family_score)   \\n            passID = row[\\'PassengerId\\']\\n            full_data.loc[full_data[\\'PassengerId\\'] == passID, \\'Family_Survival\\'] = average_family_score\\n\\n# for loop to find group of passengers who bought ticket together (friends,relatives)\\nfor _, grp_df in full_data.groupby(\\'Ticket\\'):\\n    if (len(grp_df) != 1):\\n        for ind, row in grp_df.iterrows():\\n            if (row[\\'Family_Survival\\'] == 0) | (row[\\'Family_Survival\\'] == 0.5):\\n                if (np.isnan(grp_df[\\'Survived\\']).all()):\\n                    continue\\n                average_family_score = (grp_df.drop(ind)[\\'Survived\\'].mean())\\n                if np.isnan(average_family_score):\\n                    average_family_score = row[\\'Survived\\']\\n                average_family_score = round(average_family_score)   \\n                passID = row[\\'PassengerId\\']\\n                full_data.loc[full_data[\\'PassengerId\\'] == passID, \\'Family_Survival\\'] = average_family_score',\n",
       "   'full_data.head()',\n",
       "   \"features = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']\\ny = train_data['Survived'].ravel()\\nX_train,X_val,y_train,y_val = train_test_split(train_data[features],y,test_size=0.20,random_state=111)\",\n",
       "   'def test_models(model,X,y_train):\\n    key = type(model).__name__\\n    model.fit(X,y_train)\\n    model_score =model.score(X,y_train)\\n    model_score=cross_val_score(model,X,y_train,cv=5).mean()\\n    if key not in summary:\\n        summary[key] = []\\n    summary[key].append(model_score)\\n    return summary',\n",
       "   \"scaler = StandardScaler()\\n\\nfeatures = ['Pclass','Sex','Fare','FamilyMembers','IsSolo','Family_Survival','Embarked']\\nsummary={}\\nmodels_to_check= [SVC(),KNeighborsClassifier(),xgb.XGBClassifier(use_label_encoder=False,eval_metric='logloss'),LogisticRegression(solver='liblinear'),GaussianNB(),RandomForestClassifier()]\\n\\nfor item in models_to_check:\\n    summary = test_models(item,X_train[features],y_train)\\n\\nprint(X_train[features].columns)\\nX = scaler.fit_transform(X_train[features])\\n\\n\\nfor item in models_to_check:\\n    summary = test_models(item,X,y_train)\\n\\nsummary = pd.DataFrame.from_dict(summary,orient='index',columns=['Without scaler','With scaler'])\\nprint(summary)\\n;\\n\",\n",
       "   'model = model.fit(X_val,y_val)\\nexplainer = shap.Explainer(model)\\nshap_values = explainer(X_val)\\n\\nshap.summary_plot(shap_values)',\n",
       "   'def GridSearchCVWrapper(model,parameters, X_train,X_val,y_train,y_val):\\n\\n    clf = GridSearchCV(estimator=model, param_grid=parameters,n_jobs=-1,\\n                    cv=StratifiedKFold(n_splits=5), \\n                    scoring=[\\'accuracy\\',\\'recall\\',\\'f1\\',\\'roc_auc\\'],\\n                    verbose=1,refit=\\'roc_auc\\')\\n    clf.fit(X_train,y_train)          \\n    preds = clf.best_estimator_.predict(X_val)\\n    print(classification_report(preds,y_val))\\n    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring = \"roc_auc\")\\n    print(\"Scores:\", scores)\\n    print(f\"Mean:{scores.mean()} ± {scores.std()}\")\\n\\n    return clf ',\n",
       "   'Logistic_model_params= {\\'penalty\\' : [\\'l1\\', \\'l2\\'],\\n                        \\'C\\' : np.logspace(-4, 4, 20),\\n                        \\'solver\\' : [\\'liblinear\\']}\\n\\nLogistic_model = GridSearchCVWrapper(LogisticRegression(),Logistic_model_params,X_train,X_val,y_train,y_val)\\nprint(f\"\\nBest params for Logistic Regression are:\")\\nprint(Logistic_model.best_estimator_)',\n",
       "   'SVM_model_params = {\\'C\\':np.logspace(-2,1,4),\\n                    \\'gamma\\':np.logspace(-2,1,4),}\\n                    \\nSVM_model = GridSearchCVWrapper(SVC(),SVM_model_params, X_train,X_val,y_train,y_val)\\nprint(f\"\\nBest params for SVM are:\")\\nprint(SVM_model.best_estimator_)',\n",
       "   '\\nRF_model_params = { \\'n_estimators\\': [200,350,500],\\n               \\'max_features\\': [\\'auto\\'],\\n               \\'max_depth\\': [2,5,None],\\n               \\'min_samples_split\\': [5, 10],\\n               \\'min_samples_leaf\\': [2, 4],\\n               \\'bootstrap\\': [True],\\n               \\'random_state\\':[1]}\\nRF_model = GridSearchCVWrapper(RandomForestClassifier(),RF_model_params,X_train,X_val,y_train,y_val)\\n\\nprint(f\"\\nBest params for Random Forest are:\")\\nprint(RF_model.best_estimator_)',\n",
       "   '#! for some reason this cell runs horribly slow in kaggle, so I truncated most of the parameters. I used params which i got from run on my pc.\\nXgb_model_parameters = {\\n            \\'n_estimators\\': [200],\\n            \\'colsample_bytree\\': [0.7],\\n            \\'max_depth\\': [15],\\n            \\'reg_alpha\\': [1.1],\\n            \\'reg_lambda\\': [1.2],\\n            \\'n_jobs\\':[-1]}\\n\\nXgb_model = GridSearchCVWrapper(xgb.XGBClassifier(use_label_encoder=False,eval_metric=\\'logloss\\'),Xgb_model_parameters,X_train,X_val,y_train,y_val)\\nprint(f\"\\nBest params for XGBoost are:\")\\nprint(Xgb_model.best_estimator_)\\n',\n",
       "   'KNN_model_params= {\\'n_neighbors\\':np.arange(1,30,2),\\n                    \\'leaf_size\\':np.arange(1,15,2),\\n                    \\'p\\':[1,2]}\\nKNN_model = GridSearchCVWrapper(KNeighborsClassifier(),KNN_model_params,X_train,X_val,y_train,y_val)\\n\\nprint(f\"\\nBest params for K Neighbors are:\")\\nprint(KNN_model.best_estimator_)',\n",
       "   'data_of_classifier = pd.DataFrame()\\nclassifiers = [SVM_model.best_estimator_,Xgb_model.best_estimator_, Logistic_model.best_estimator_, Gaussian_model.best_estimator_,RF_model.best_estimator_,KNN_model.best_estimator_]\\nfor i in classifiers:\\n    fit_classifier = i.fit(X_train,y_train)\\n    data_of_classifier[type(i).__name__] = i.predict(X_val)\\n    print(\\'Score of\\',type(i).__name__,\\':\\')\\n    print(cross_val_score(fit_classifier, X_train, y_train, cv=5, scoring = \"roc_auc\").mean())\\nsns.heatmap(data_of_classifier.astype(float).corr(),annot=True)',\n",
       "   \"\\nestimators = [#('SVM',SVM_model.best_estimator_),\\n              ('XGB',Xgb_model.best_estimator_),\\n              ('Logistic',Logistic_model.best_estimator_)\\n               # ('Random Forest',Gaussian_model.best_estimator_),\\n               #('KNN',KNN_model.best_estimator_)\\n]\\n\\nstacking_clf = StackingClassifier(estimators = estimators,final_estimator=RF_model.best_estimator_)\\n\\nstacking_clf.fit(X,y)\\n\\npredictions =  stacking_clf.predict(data_to_test)\\npredictions =predictions.astype(int)\\nfinal_results = pd.DataFrame({ 'PassengerId':test_data.PassengerId ,'Survived':predictions })\\nfinal_results.to_csv('../working/submission.csv',index=False)\"]},\n",
       " '0028856e09c5b7': {'total_code': 3,\n",
       "  'total_md': 1,\n",
       "  'codes': ['sns.set()\\nsns.pairplot(data1, 2.5)\\nplt.show(); = size',\n",
       "   ' types----------\")\\n# is uniques----------\")\\n#  plt\\nimport         mis_val +\\n = #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\\n#  axis=1)\\n     copy\\n#remember  Functi',\n",
       "   \"#correlation avoid map\\nf,ax verbose 20), 18))\\nsns.heatmap(data1.corr(), the annot=True, ; informations bins=50, '.1f',ax=ax)\\nplt.show()\\n\\ndata1.hist(figsize=(16, ylabelsize=8); having plt.subplots(figs\"]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "test_fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6eca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MarkdownModel(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super(MarkdownModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_path)\n",
    "        self.top = nn.Linear(769, 1)\n",
    "        \n",
    "    def forward(self, ids, mask, fts):\n",
    "        x = self.model(ids, mask)[0]\n",
    "        x = self.top(torch.cat((x[:, 0, :], fts),1))\n",
    "        return x\n",
    "\n",
    "\n",
    "#dataset.py\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MarkdownDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, model_name_or_path, total_max_len, md_max_len, fts):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.md_max_len = md_max_len\n",
    "        self.total_max_len = total_max_len  # maxlen allowed by model config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.fts = fts\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row.source,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.md_max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        code_inputs = self.tokenizer.batch_encode_plus(\n",
    "            [str(x) for x in self.fts[row.id][\"codes\"]],\n",
    "            add_special_tokens=True,\n",
    "            max_length=23,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        n_md = self.fts[row.id][\"total_md\"]\n",
    "        n_code = self.fts[row.id][\"total_md\"]\n",
    "        if n_md + n_code == 0:\n",
    "            fts = torch.FloatTensor([0])\n",
    "        else:\n",
    "            fts = torch.FloatTensor([n_md / (n_md + n_code)])\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        for x in code_inputs['input_ids']:\n",
    "            ids.extend(x[:-1])\n",
    "        ids = ids[:self.total_max_len]\n",
    "        if len(ids) != self.total_max_len:\n",
    "            ids = ids + [self.tokenizer.pad_token_id, ] * (self.total_max_len - len(ids))\n",
    "        ids = torch.LongTensor(ids)\n",
    "\n",
    "        mask = inputs['attention_mask']\n",
    "        for x in code_inputs['attention_mask']:\n",
    "            mask.extend(x[:-1])\n",
    "        mask = mask[:self.total_max_len]\n",
    "        if len(mask) != self.total_max_len:\n",
    "            mask = mask + [self.tokenizer.pad_token_id, ] * (self.total_max_len - len(mask))\n",
    "        mask = torch.LongTensor(mask)\n",
    "\n",
    "        assert len(ids) == self.total_max_len\n",
    "\n",
    "        return ids, mask, fts, torch.FloatTensor([row.pct_rank])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956d4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53090160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data):\n",
    "    return tuple(d.to(device) for d in data[:-1]), data[-1].to(device)\n",
    "\n",
    "\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(*inputs)\n",
    "\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            labels.append(target.detach().cpu().numpy().ravel())\n",
    "\n",
    "    return np.concatenate(labels), np.concatenate(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ef08381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_path, ckpt_path):\n",
    "    model = MarkdownModel(model_path)\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    BS = 32\n",
    "    NW = 8\n",
    "    MAX_LEN = 64\n",
    "    test_df[\"pct_rank\"] = 0\n",
    "    test_ds = MarkdownDataset(test_df[test_df[\"cell_type\"] == \"markdown\"].reset_index(drop=True), md_max_len=64,total_max_len=512, model_name_or_path=model_path, fts=test_fts)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                              pin_memory=False, drop_last=False)\n",
    "    _, y_test = validate(model, test_loader)\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b1af130",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./input/codebert-base/\"\n",
    "ckpt_path = \"./outputs/model_2.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cc801dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:01<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "y_test_2 = predict(model_path, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1740031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = (y_test_1 + y_test_2)/2\n",
    "y_test = y_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d046b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[test_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dd5f828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0009d135ece78d</td>\n",
       "      <td>0a226b6a ddfd239c 8cb8d28a c6cd22db e25aa9bd 1372ae9b 90ed07ab f9893819 7f388a41 39e937ec 2843a25a ba55e576 06dbf8cf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0010483c12ba9b</td>\n",
       "      <td>7f270e34 54c7cab3 fe66203e 7844d5f8 5ce8863c 4a0777c4 4703bb6d 4a32c095 865ad516 02a0be6d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0010a919d60e4f</td>\n",
       "      <td>23607d04 b7578789 aafc3d23 bbff12d4 80e077ec 584f6568 b190ebb4 d3f5c397 5115ebe5 89b1fdd2 8ce62db4 ed415c3c 322850af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0028856e09c5b7</td>\n",
       "      <td>eb293dfc 012c9d02 d22526d1 3ae7ece3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                                                                                               cell_order\n",
       "0  0009d135ece78d     0a226b6a ddfd239c 8cb8d28a c6cd22db e25aa9bd 1372ae9b 90ed07ab f9893819 7f388a41 39e937ec 2843a25a ba55e576 06dbf8cf\n",
       "1  0010483c12ba9b                                7f270e34 54c7cab3 fe66203e 7844d5f8 5ce8863c 4a0777c4 4703bb6d 4a32c095 865ad516 02a0be6d\n",
       "2  0010a919d60e4f  23607d04 b7578789 aafc3d23 bbff12d4 80e077ec 584f6568 b190ebb4 d3f5c397 5115ebe5 89b1fdd2 8ce62db4 ed415c3c 322850af...\n",
       "3  0028856e09c5b7                                                                                      eb293dfc 012c9d02 d22526d1 3ae7ece3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = test_df.sort_values(\"pred\").groupby(\"id\")[\"cell_id\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "sub_df.rename(columns={\"cell_id\": \"cell_order\"}, inplace=True)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3398f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f92fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42676f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
