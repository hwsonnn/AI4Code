{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9bde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "pd.options.display.width = 180\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "# BERT_PATH = \"../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased\"\n",
    "\n",
    "data_dir = Path('./input/AI4Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987b72eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train NBs: 100%|█████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 294.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">003f36ab2c577d</th>\n",
       "      <th>386d31f0</th>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\n# import tf2_0_baseline_w_bert as tf2baseline # ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16435878</th>\n",
       "      <td>code</td>\n",
       "      <td>def del_all_flags(FLAGS):\\n    flags_dict = FLAGS._flags()\\n    keys_list = [keys for keys in flags_dict]\\n    for k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f4bb282f</th>\n",
       "      <td>code</td>\n",
       "      <td>bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\\n\\ntf2baseline.validate_flags_or_throw(bert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4cc5ee5a</th>\n",
       "      <td>code</td>\n",
       "      <td>test_answers_df = pd.read_json(\"/kaggle/working/predictions.json\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215ec8c0</th>\n",
       "      <td>code</td>\n",
       "      <td>def create_short_answer(entry):\\n    # if entry[\"short_answers_score\"] &lt; 1.5:\\n    #     return \"\"\\n    \\n    answer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">fdd4fc31053b84</th>\n",
       "      <th>2b0bb2b1</th>\n",
       "      <td>code</td>\n",
       "      <td>learn.unfreeze()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d8c256ac</th>\n",
       "      <td>code</td>\n",
       "      <td>learn.fit_one_cycle(3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633e3393</th>\n",
       "      <td>code</td>\n",
       "      <td>learn.lr_find()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9ef4c5c0</th>\n",
       "      <td>code</td>\n",
       "      <td>learn.recorder.plot()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6e0e2dc8</th>\n",
       "      <td>markdown</td>\n",
       "      <td>#Inspired by fast.ai Practical Deep Learning for Coders(v3) lession 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9176 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        cell_type                                                                                                                   source\n",
       "id             cell_id                                                                                                                                    \n",
       "003f36ab2c577d 386d31f0      code  import numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\n# import tf2_0_baseline_w_bert as tf2baseline # ol...\n",
       "               16435878      code  def del_all_flags(FLAGS):\\n    flags_dict = FLAGS._flags()\\n    keys_list = [keys for keys in flags_dict]\\n    for k...\n",
       "               f4bb282f      code  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\\n\\ntf2baseline.validate_flags_or_throw(bert...\n",
       "               4cc5ee5a      code                                                       test_answers_df = pd.read_json(\"/kaggle/working/predictions.json\")\n",
       "               215ec8c0      code  def create_short_answer(entry):\\n    # if entry[\"short_answers_score\"] < 1.5:\\n    #     return \"\"\\n    \\n    answer...\n",
       "...                           ...                                                                                                                      ...\n",
       "fdd4fc31053b84 2b0bb2b1      code                                                                                                         learn.unfreeze()\n",
       "               d8c256ac      code                                                                                                   learn.fit_one_cycle(3)\n",
       "               633e3393      code                                                                                                          learn.lr_find()\n",
       "               9ef4c5c0      code                                                                                                    learn.recorder.plot()\n",
       "               6e0e2dc8  markdown                                                    #Inspired by fast.ai Practical Deep Learning for Coders(v3) lession 1\n",
       "\n",
       "[9176 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_TRAIN = 200\n",
    "\n",
    "\n",
    "def read_notebook(path):\n",
    "    return (\n",
    "        pd.read_json(\n",
    "            path,\n",
    "            dtype={'cell_type': 'category', 'source': 'str'})\n",
    "        .assign(id=path.stem)\n",
    "        .rename_axis('cell_id')\n",
    "    )\n",
    "\n",
    "\n",
    "paths_train = list((data_dir / 'train').glob('*.json'))[:NUM_TRAIN]\n",
    "notebooks_train = [\n",
    "    read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')\n",
    "]\n",
    "df = (\n",
    "    pd.concat(notebooks_train)\n",
    "    .set_index('id', append=True)\n",
    "    .swaplevel()\n",
    "    .sort_index(level='id', sort_remaining=False)\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a307b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74449/499128900.py:1: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  df_orders = pd.read_csv(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id\n",
       "00001756c60be8    [1862f0a6, 448eb224, 2a9e43d6, 7e2f170a, 038b763d, 77e56113, 2eefe0ef, 1ae087ab, 0beab1cd, 8ffe0b25, 9a78ab76, 0d136...\n",
       "00015c83e2717b    [2e94bd7a, 3e99dee9, b5e286ea, da4f7550, c417225b, 51e3cd89, 2600b4eb, 75b65993, cf195f8b, 25699d02, 72b3201a, f2c75...\n",
       "0001bdd4021779    [3fdc37be, 073782ca, 8ea7263c, 80543cd8, 38310c80, 073e27e5, 015d52a4, ad7679ef, 7fde4f04, 07c52510, 0a1a7a39, 0bcd3...\n",
       "0001daf4c2c76d    [97266564, a898e555, 86605076, 76cc2642, ef279279, df6c939f, 2476da96, 00f87d0a, ae93e8e6, 58aadb1d, d20b0094, 986fd...\n",
       "0002115f48f982                                 [9ec225f0, 18281c6c, e3b6b115, 4a044c54, 365fe576, a3188e54, b3f6e12d, ee7655ca, 84125b7a]\n",
       "                                                                           ...                                                           \n",
       "fffc30d5a0bc46    [09727c0c, ff1ea6a0, ddfef603, a01ce9b3, 3ba953ee, bf92a015, f4a0492a, 095812e6, 53125cfe, aa32a700, 63340e73, 06d8c...\n",
       "fffc3b44869198    [978a5137, faa48f03, 28dfb12a, eea2e812, 64fef97c, 4e0d1510, 58e68f2c, 8784e700, 4bd5a4cf, dc14bfec, 2aff7603, 8047d...\n",
       "fffc63ff750064    [5015c300, 411b85d9, 8238198c, f4781d1d, b5532930, e1f223e5, e7e67119, 4aaf741d, 7229cce6, a7fa3628, e4c2fa86, 1f8f9...\n",
       "fffcd063cda949    [7e6266ad, d8281fc5, d4ffcaef, 3e0e4a47, 21387fc8, cc229f9a, baed9c8b, d1bb21aa, 82979992, 65f95dad, eba4fa9e, c97e2...\n",
       "fffe1d764579d5    [1a63248d, 9c3b96a5, 1398a873, 4e2d4c2d, f71c538e, 8b44a5e8, 385dff7a, b8254ef8, 4d0e433e, debc496c, e15ae953, e4d79...\n",
       "Name: cell_order, Length: 139256, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders = pd.read_csv(\n",
    "    data_dir / 'train_orders.csv',\n",
    "    index_col='id',\n",
    "    squeeze=True,\n",
    ").str.split()  # Split the string representation of cell_ids into a list\n",
    "\n",
    "df_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b7a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(base, derived):\n",
    "    return [base.index(d) for d in derived]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7238a90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">003f36ab2c577d</th>\n",
       "      <th>386d31f0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16435878</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f4bb282f</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4cc5ee5a</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215ec8c0</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">fdd4fc31053b84</th>\n",
       "      <th>2b0bb2b1</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d8c256ac</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633e3393</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9ef4c5c0</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6e0e2dc8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9176 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        rank\n",
       "id             cell_id      \n",
       "003f36ab2c577d 386d31f0    2\n",
       "               16435878    4\n",
       "               f4bb282f    6\n",
       "               4cc5ee5a    8\n",
       "               215ec8c0   10\n",
       "...                      ...\n",
       "fdd4fc31053b84 2b0bb2b1   18\n",
       "               d8c256ac   19\n",
       "               633e3393   20\n",
       "               9ef4c5c0   21\n",
       "               6e0e2dc8    0\n",
       "\n",
       "[9176 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_ = df_orders.to_frame().join(\n",
    "    df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),\n",
    "    how='right',\n",
    ")\n",
    "\n",
    "ranks = {}\n",
    "for id_, cell_order, cell_id in df_orders_.itertuples():\n",
    "    ranks[id_] = {'cell_id': cell_id, 'rank': get_ranks(cell_order, cell_id)}\n",
    "\n",
    "df_ranks = (\n",
    "    pd.DataFrame\n",
    "    .from_dict(ranks, orient='index')\n",
    "    .rename_axis('id')\n",
    "    .apply(pd.Series.explode)\n",
    "    .set_index('cell_id', append=True)\n",
    ")\n",
    "\n",
    "df_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3839b23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00001756c60be8</th>\n",
       "      <td>945aea18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00015c83e2717b</th>\n",
       "      <td>aa2da37e</td>\n",
       "      <td>317b65d12af9df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001bdd4021779</th>\n",
       "      <td>a7711fde</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001daf4c2c76d</th>\n",
       "      <td>090152ca</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002115f48f982</th>\n",
       "      <td>272b483a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffc30d5a0bc46</th>\n",
       "      <td>6aed207b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffc3b44869198</th>\n",
       "      <td>a6aaa8d7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffc63ff750064</th>\n",
       "      <td>0a1b5b65</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffcd063cda949</th>\n",
       "      <td>d971e960</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffe1d764579d5</th>\n",
       "      <td>3c40bfa6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139256 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ancestor_id       parent_id\n",
       "id                                        \n",
       "00001756c60be8    945aea18             NaN\n",
       "00015c83e2717b    aa2da37e  317b65d12af9df\n",
       "0001bdd4021779    a7711fde             NaN\n",
       "0001daf4c2c76d    090152ca             NaN\n",
       "0002115f48f982    272b483a             NaN\n",
       "...                    ...             ...\n",
       "fffc30d5a0bc46    6aed207b             NaN\n",
       "fffc3b44869198    a6aaa8d7             NaN\n",
       "fffc63ff750064    0a1b5b65             NaN\n",
       "fffcd063cda949    d971e960             NaN\n",
       "fffe1d764579d5    3c40bfa6             NaN\n",
       "\n",
       "[139256 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')\n",
    "df_ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df36eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>rank</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>386d31f0</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\n# import tf2_0_baseline_w_bert as tf2baseline # ol...</td>\n",
       "      <td>2</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>16435878</td>\n",
       "      <td>code</td>\n",
       "      <td>def del_all_flags(FLAGS):\\n    flags_dict = FLAGS._flags()\\n    keys_list = [keys for keys in flags_dict]\\n    for k...</td>\n",
       "      <td>4</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>f4bb282f</td>\n",
       "      <td>code</td>\n",
       "      <td>bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\\n\\ntf2baseline.validate_flags_or_throw(bert...</td>\n",
       "      <td>6</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>4cc5ee5a</td>\n",
       "      <td>code</td>\n",
       "      <td>test_answers_df = pd.read_json(\"/kaggle/working/predictions.json\")</td>\n",
       "      <td>8</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>215ec8c0</td>\n",
       "      <td>code</td>\n",
       "      <td>def create_short_answer(entry):\\n    # if entry[\"short_answers_score\"] &lt; 1.5:\\n    #     return \"\"\\n    \\n    answer...</td>\n",
       "      <td>10</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9171</th>\n",
       "      <td>fdd4fc31053b84</td>\n",
       "      <td>2b0bb2b1</td>\n",
       "      <td>code</td>\n",
       "      <td>learn.unfreeze()</td>\n",
       "      <td>18</td>\n",
       "      <td>30c6c43b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9172</th>\n",
       "      <td>fdd4fc31053b84</td>\n",
       "      <td>d8c256ac</td>\n",
       "      <td>code</td>\n",
       "      <td>learn.fit_one_cycle(3)</td>\n",
       "      <td>19</td>\n",
       "      <td>30c6c43b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9173</th>\n",
       "      <td>fdd4fc31053b84</td>\n",
       "      <td>633e3393</td>\n",
       "      <td>code</td>\n",
       "      <td>learn.lr_find()</td>\n",
       "      <td>20</td>\n",
       "      <td>30c6c43b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9174</th>\n",
       "      <td>fdd4fc31053b84</td>\n",
       "      <td>9ef4c5c0</td>\n",
       "      <td>code</td>\n",
       "      <td>learn.recorder.plot()</td>\n",
       "      <td>21</td>\n",
       "      <td>30c6c43b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9175</th>\n",
       "      <td>fdd4fc31053b84</td>\n",
       "      <td>6e0e2dc8</td>\n",
       "      <td>markdown</td>\n",
       "      <td>#Inspired by fast.ai Practical Deep Learning for Coders(v3) lession 1</td>\n",
       "      <td>0</td>\n",
       "      <td>30c6c43b</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9176 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id   cell_id cell_type                                                                                                                   source rank  \\\n",
       "0     003f36ab2c577d  386d31f0      code  import numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\n# import tf2_0_baseline_w_bert as tf2baseline # ol...    2   \n",
       "1     003f36ab2c577d  16435878      code  def del_all_flags(FLAGS):\\n    flags_dict = FLAGS._flags()\\n    keys_list = [keys for keys in flags_dict]\\n    for k...    4   \n",
       "2     003f36ab2c577d  f4bb282f      code  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\\n\\ntf2baseline.validate_flags_or_throw(bert...    6   \n",
       "3     003f36ab2c577d  4cc5ee5a      code                                                       test_answers_df = pd.read_json(\"/kaggle/working/predictions.json\")    8   \n",
       "4     003f36ab2c577d  215ec8c0      code  def create_short_answer(entry):\\n    # if entry[\"short_answers_score\"] < 1.5:\\n    #     return \"\"\\n    \\n    answer...   10   \n",
       "...              ...       ...       ...                                                                                                                      ...  ...   \n",
       "9171  fdd4fc31053b84  2b0bb2b1      code                                                                                                         learn.unfreeze()   18   \n",
       "9172  fdd4fc31053b84  d8c256ac      code                                                                                                   learn.fit_one_cycle(3)   19   \n",
       "9173  fdd4fc31053b84  633e3393      code                                                                                                          learn.lr_find()   20   \n",
       "9174  fdd4fc31053b84  9ef4c5c0      code                                                                                                    learn.recorder.plot()   21   \n",
       "9175  fdd4fc31053b84  6e0e2dc8  markdown                                                    #Inspired by fast.ai Practical Deep Learning for Coders(v3) lession 1    0   \n",
       "\n",
       "     ancestor_id       parent_id  \n",
       "0       8508be37  3bde8d65a3508b  \n",
       "1       8508be37  3bde8d65a3508b  \n",
       "2       8508be37  3bde8d65a3508b  \n",
       "3       8508be37  3bde8d65a3508b  \n",
       "4       8508be37  3bde8d65a3508b  \n",
       "...          ...             ...  \n",
       "9171    30c6c43b             NaN  \n",
       "9172    30c6c43b             NaN  \n",
       "9173    30c6c43b             NaN  \n",
       "9174    30c6c43b             NaN  \n",
       "9175    30c6c43b             NaN  \n",
       "\n",
       "[9176 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index().merge(df_ranks, on=[\"id\", \"cell_id\"]).merge(df_ancestors, on=[\"id\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7740255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"pct_rank\"] = df[\"rank\"] / df.groupby(\"id\")[\"cell_id\"].transform(\"count\")\n",
    "\n",
    "df[\"pct_rank\"].hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "858f3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre process 전!\n",
    "dict_cellid_source = dict(zip(df['cell_id'].values, df['source'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8092ce83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'386d31f0': 'import numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\n# import tf2_0_baseline_w_bert as tf2baseline # old script\\nimport tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # my script\\nimport bert_modeling as modeling\\nimport bert_optimization as optimization\\nimport bert_tokenization as tokenization\\nimport json\\nimport absl\\nimport sys\\n\\n# Input data files are available in the \"../input/\" directory.\\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\n# In this case, we\\'ve got some extra BERT model files under `/kaggle/input/bertjointbaseline`\\n\\nimport os\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))',\n",
       " '16435878': 'def del_all_flags(FLAGS):\\n    flags_dict = FLAGS._flags()\\n    keys_list = [keys for keys in flags_dict]\\n    for keys in keys_list:\\n        FLAGS.__delattr__(keys)\\n\\ndel_all_flags(absl.flags.FLAGS)\\n\\nflags = absl.flags\\n\\nflags.DEFINE_string(\\n    \"bert_config_file\", \"/kaggle/input/bertjointbaseline/bert_config.json\",\\n    \"The config json file corresponding to the pre-trained BERT model. \"\\n    \"This specifies the model architecture.\")\\n\\nflags.DEFINE_string(\"vocab_file\", \"/kaggle/input/bertjointbaseline/vocab-nq.txt\",\\n                    \"The vocabulary file that the BERT model was trained on.\")\\n\\nflags.DEFINE_string(\\n    \"output_dir\", \"outdir\",\\n    \"The output directory where the model checkpoints will be written.\")\\n\\nflags.DEFINE_string(\"train_precomputed_file\", None,\\n                    \"Precomputed tf records for training.\")\\n\\nflags.DEFINE_integer(\"train_num_precomputed\", None,\\n                     \"Number of precomputed tf records for training.\")\\n\\nflags.DEFINE_string(\\n    \"output_prediction_file\", \"predictions.json\",\\n    \"Where to print predictions in NQ prediction format, to be passed to\"\\n    \"natural_questions.nq_eval.\")\\n\\nflags.DEFINE_string(\\n    \"init_checkpoint\", \"/kaggle/input/bertjointbaseline/bert_joint.ckpt\",\\n    \"Initial checkpoint (usually from a pre-trained BERT model).\")\\n\\nflags.DEFINE_bool(\\n    \"do_lower_case\", True,\\n    \"Whether to lower case the input text. Should be True for uncased \"\\n    \"models and False for cased models.\")\\n\\nflags.DEFINE_integer(\\n    \"max_seq_length\", 384,\\n    \"The maximum total input sequence length after WordPiece tokenization. \"\\n    \"Sequences longer than this will be truncated, and sequences shorter \"\\n    \"than this will be padded.\")\\n\\nflags.DEFINE_integer(\\n    \"doc_stride\", 128,\\n    \"When splitting up a long document into chunks, how much stride to \"\\n    \"take between chunks.\")\\n\\nflags.DEFINE_integer(\\n    \"max_query_length\", 64,\\n    \"The maximum number of tokens for the question. Questions longer than \"\\n    \"this will be truncated to this length.\")\\n\\nflags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\\n\\nflags.DEFINE_bool(\"do_predict\", True, \"Whether to run eval on the dev set.\")\\n\\nflags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\\n\\nflags.DEFINE_integer(\"predict_batch_size\", 8,\\n                     \"Total batch size for predictions.\")\\n\\nflags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\\n\\nflags.DEFINE_float(\"num_train_epochs\", 3.0,\\n                   \"Total number of training epochs to perform.\")\\n\\nflags.DEFINE_float(\\n    \"warmup_proportion\", 0.1,\\n    \"Proportion of training to perform linear learning rate warmup for. \"\\n    \"E.g., 0.1 = 10% of training.\")\\n\\nflags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\\n                     \"How often to save the model checkpoint.\")\\n\\nflags.DEFINE_integer(\"iterations_per_loop\", 1000,\\n                     \"How many steps to make in each estimator call.\")\\n\\nflags.DEFINE_integer(\\n    \"n_best_size\", 20,\\n    \"The total number of n-best predictions to generate in the \"\\n    \"nbest_predictions.json output file.\")\\n\\nflags.DEFINE_integer(\\n    \"verbosity\", 1, \"How verbose our error messages should be\")\\n\\nflags.DEFINE_integer(\\n    \"max_answer_length\", 30,\\n    \"The maximum length of an answer that can be generated. This is needed \"\\n    \"because the start and end predictions are not conditioned on one another.\")\\n\\nflags.DEFINE_float(\\n    \"include_unknowns\", -1.0,\\n    \"If positive, probability of including answers of type `UNKNOWN`.\")\\n\\nflags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\\nflags.DEFINE_bool(\"use_one_hot_embeddings\", False, \"Whether to use use_one_hot_embeddings\")\\n\\nabsl.flags.DEFINE_string(\\n    \"gcp_project\", None,\\n    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\\n    \"specified, we will attempt to automatically detect the GCE project from \"\\n    \"metadata.\")\\n\\nflags.DEFINE_bool(\\n    \"verbose_logging\", False,\\n    \"If true, all of the warnings related to data processing will be printed. \"\\n    \"A number of warnings are expected for a normal NQ evaluation.\")\\n\\nflags.DEFINE_boolean(\\n    \"skip_nested_contexts\", True,\\n    \"Completely ignore context that are not top level nodes in the page.\")\\n\\nflags.DEFINE_integer(\"task_id\", 0,\\n                     \"Train and dev shard to read from and write to.\")\\n\\nflags.DEFINE_integer(\"max_contexts\", 48,\\n                     \"Maximum number of contexts to output for an example.\")\\n\\nflags.DEFINE_integer(\\n    \"max_position\", 50,\\n    \"Maximum context position for which to generate special tokens.\")\\n\\n\\n## Special flags - do not change\\n\\nflags.DEFINE_string(\\n    \"predict_file\", \"/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\",\\n    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\\nflags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\\nflags.DEFINE_boolean(\"undefok\", True, \"it\\'s okay to be undefined\")\\nflags.DEFINE_string(\\'f\\', \\'\\', \\'kernel\\')\\nflags.DEFINE_string(\\'HistoryManager.hist_file\\', \\'\\', \\'kernel\\')\\n\\nFLAGS = flags.FLAGS\\nFLAGS(sys.argv) # Parse the flags',\n",
       " 'f4bb282f': 'bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\\n\\ntf2baseline.validate_flags_or_throw(bert_config)\\ntf.io.gfile.makedirs(FLAGS.output_dir)\\n\\ntokenizer = tokenization.FullTokenizer(\\n    vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\\n\\nrun_config = tf.estimator.RunConfig(\\n    model_dir=FLAGS.output_dir,\\n    save_checkpoints_steps=FLAGS.save_checkpoints_steps)\\n\\nnum_train_steps = None\\nnum_warmup_steps = None\\n\\nmodel_fn = tf2baseline.model_fn_builder(\\n    bert_config=bert_config,\\n    init_checkpoint=FLAGS.init_checkpoint,\\n    learning_rate=FLAGS.learning_rate,\\n    num_train_steps=num_train_steps,\\n    num_warmup_steps=num_warmup_steps,\\n    use_tpu=FLAGS.use_tpu,\\n    use_one_hot_embeddings=FLAGS.use_one_hot_embeddings)\\n\\nestimator = tf.estimator.Estimator(\\n    model_fn=model_fn,\\n    config=run_config,\\n    params={\\'batch_size\\':FLAGS.train_batch_size})\\n\\n\\nif FLAGS.do_predict:\\n  if not FLAGS.output_prediction_file:\\n    raise ValueError(\\n        \"--output_prediction_file must be defined in predict mode.\")\\n    \\n  eval_examples = tf2baseline.read_nq_examples(\\n      input_file=FLAGS.predict_file, is_training=False)\\n\\n  print(\"FLAGS.predict_file\", FLAGS.predict_file)\\n\\n  eval_writer = tf2baseline.FeatureWriter(\\n      filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\\n      is_training=False)\\n  eval_features = []\\n\\n  def append_feature(feature):\\n    eval_features.append(feature)\\n    eval_writer.process_feature(feature)\\n\\n  num_spans_to_ids = tf2baseline.convert_examples_to_features(\\n      examples=eval_examples,\\n      tokenizer=tokenizer,\\n      is_training=False,\\n      output_fn=append_feature)\\n  eval_writer.close()\\n  eval_filename = eval_writer.filename\\n\\n  print(\"***** Running predictions *****\")\\n  print(f\"  Num orig examples = %d\" % len(eval_examples))\\n  print(f\"  Num split examples = %d\" % len(eval_features))\\n  print(f\"  Batch size = %d\" % FLAGS.predict_batch_size)\\n  for spans, ids in num_spans_to_ids.items():\\n    print(f\"  Num split into %d = %d\" % (spans, len(ids)))\\n\\n  predict_input_fn = tf2baseline.input_fn_builder(\\n      input_file=eval_filename,\\n      seq_length=FLAGS.max_seq_length,\\n      is_training=False,\\n      drop_remainder=False)\\n\\n  all_results = []\\n\\n  for result in estimator.predict(\\n      predict_input_fn, yield_single_examples=True):\\n    if len(all_results) % 1000 == 0:\\n      print(\"Processing example: %d\" % (len(all_results)))\\n\\n    unique_id = int(result[\"unique_ids\"])\\n    start_logits = [float(x) for x in result[\"start_logits\"].flat]\\n    end_logits = [float(x) for x in result[\"end_logits\"].flat]\\n    answer_type_logits = [float(x) for x in result[\"answer_type_logits\"].flat]\\n\\n    all_results.append(\\n        tf2baseline.RawResult(\\n            unique_id=unique_id,\\n            start_logits=start_logits,\\n            end_logits=end_logits,\\n            answer_type_logits=answer_type_logits))\\n\\n  print (\"Going to candidates file\")\\n\\n  candidates_dict = tf2baseline.read_candidates(FLAGS.predict_file)\\n\\n  print (\"setting up eval features\")\\n\\n  raw_dataset = tf.data.TFRecordDataset(eval_filename)\\n  eval_features = []\\n  for raw_record in raw_dataset:\\n    eval_features.append(tf.train.Example.FromString(raw_record.numpy()))\\n    \\n  print (\"compute_pred_dict\")\\n\\n  nq_pred_dict = tf2baseline.compute_pred_dict(candidates_dict, eval_features,\\n                                   [r._asdict() for r in all_results])\\n  predictions_json = {\"predictions\": list(nq_pred_dict.values())}\\n\\n  print (\"writing json\")\\n\\n  with tf.io.gfile.GFile(FLAGS.output_prediction_file, \"w\") as f:\\n    json.dump(predictions_json, f, indent=4)',\n",
       " '4cc5ee5a': 'test_answers_df = pd.read_json(\"/kaggle/working/predictions.json\")',\n",
       " '215ec8c0': 'def create_short_answer(entry):\\n    # if entry[\"short_answers_score\"] < 1.5:\\n    #     return \"\"\\n    \\n    answer = []    \\n    for short_answer in entry[\"short_answers\"]:\\n        if short_answer[\"start_token\"] > -1:\\n            answer.append(str(short_answer[\"start_token\"]) + \":\" + str(short_answer[\"end_token\"]))\\n    if entry[\"yes_no_answer\"] != \"NONE\":\\n        answer.append(entry[\"yes_no_answer\"])\\n    return \" \".join(answer)\\n\\ndef create_long_answer(entry):\\n   # if entry[\"long_answer_score\"] < 1.5:\\n   # return \"\"\\n\\n    answer = []\\n    if entry[\"long_answer\"][\"start_token\"] > -1:\\n        answer.append(str(entry[\"long_answer\"][\"start_token\"]) + \":\" + str(entry[\"long_answer\"][\"end_token\"]))\\n    return \" \".join(answer)',\n",
       " '3f201013': 'test_answers_df[\"long_answer_score\"] = test_answers_df[\"predictions\"].apply(lambda q: q[\"long_answer_score\"])\\ntest_answers_df[\"short_answer_score\"] = test_answers_df[\"predictions\"].apply(lambda q: q[\"short_answers_score\"])',\n",
       " 'eb645be7': 'test_answers_df[\"long_answer_score\"].describe()',\n",
       " '1d664ca8': 'test_answers_df.predictions.values[0]',\n",
       " '5c545af1': 'test_answers_df[\"long_answer\"] = test_answers_df[\"predictions\"].apply(create_long_answer)\\ntest_answers_df[\"short_answer\"] = test_answers_df[\"predictions\"].apply(create_short_answer)\\ntest_answers_df[\"example_id\"] = test_answers_df[\"predictions\"].apply(lambda q: str(q[\"example_id\"]))\\n\\nlong_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"long_answer\"]))\\nshort_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"short_answer\"]))',\n",
       " '05bc949b': 'sample_submission = pd.read_csv(\"/kaggle/input/tensorflow2-question-answering/sample_submission.csv\")\\n\\nlong_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_long\")].apply(lambda q: long_answers[q[\"example_id\"].replace(\"_long\", \"\")], axis=1)\\nshort_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_short\")].apply(lambda q: short_answers[q[\"example_id\"].replace(\"_short\", \"\")], axis=1)\\n\\nsample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_long\"), \"PredictionString\"] = long_prediction_strings\\nsample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_short\"), \"PredictionString\"] = short_prediction_strings',\n",
       " 'a3f4e6bc': 'sample_submission.to_csv(\"submission.csv\", index=False)\\nsample_submission.head()',\n",
       " '3e1430c4': '#### Now, we turn `predictions.json` into a `submission.csv` file.',\n",
       " '6f70d84e': \"An example of what each sample's answers look like in `prediction.json`:\",\n",
       " 'da99f684': \"In this notebook, we'll be using the Bert baseline for Tensorflow to create predictions for the Natural Questions test set. Note that this uses a model that has already been pre-trained - we're only doing inference here. A GPU is required, and this should take between 1-2 hours to run.\\n\\nThe original script can be found [here](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py).\\nThe supporting modules were drawn from the [official Tensorflow model repository](https://github.com/tensorflow/models/tree/master/official). The bert-joint-baseline data is described [here](https://github.com/google-research/language/tree/master/language/question_answering/bert_joint).\\n\\n**Note:** This baseline uses code that was migrated from TF1.x. Be aware that it contains use of tf.compat.v1, which is not permitted to be eligible for [TF2.0 prizes in this competition](https://www.kaggle.com/c/tensorflow2-question-answering/overview/prizes). It is intended to be used as a starting point, but we're excited to see how much better you can do using TF2.0!\",\n",
       " 'db8c69de': 'The Bert model produces a `confidence` score, which the Kaggle metric does not use. You, however, can use that score to determine which answers get submitted. See the limits commented out in `create_short_answer` and `create_long_answer` below for an example.\\n\\nValues for `confidence` will range between `1.0` and `2.0`.',\n",
       " '5948bc1a': 'And finally, we write out our submission!',\n",
       " '00070116': 'Then we add them to our sample submission. Recall that each sample has both a `_long` and `_short` entry in the sample submission, one for each type of answer.',\n",
       " '82dfbe9a': \"#### Tensorflow flags are variables that can be passed around within the TF system. Every flag below has some context provided regarding what the flag is and how it's used.\\n\\n#### Most of these can be changed as desired, with the exception of the Special Flags at the bottom, which _must_ stay as-is to work with the Kaggle back end.\",\n",
       " '6d31400d': '#### Here, we:\\n1. Set up Bert\\n2. Read in the test set\\n3. Run it past the pre-built Bert model to create embeddings\\n4. Use those embeddings to make predictions\\n5. Write those predictions to `predictions.json`\\n\\nFeel free to change the code below. Code for the `tf2baseline.*` functions is included in the `tf2_0_baseline_w_bert` utility script, and can be customized, whether by forking the utility script and updating it, or by creating your own non-`tf2baseline` versions in this kernel.\\n\\nNote: the `tf2_0_baseline_w_bert` utility script contains code for training your own embeddings. Here that code is removed.',\n",
       " 'c47a4a8e': 'We re-format the JSON answers to match the requirements for submission.',\n",
       " '42435be9': '<center><img src=\"https://raw.githubusercontent.com/dimitreOliveira/MachineLearning/master/Kaggle/TensorFlow%202.0%20Question%20Answering/banner.png\" width=\"1000\"></center>\\n\\n<br>\\n<center><h1>Using Tensorflow 2.0 with Bert on Natural Questions - (translated to TF2.0)</h1></center>\\n<br>\\n### This is a translated version of the baseline [script](https://www.kaggle.com/philculliton/using-tensorflow-2-0-w-bert-on-nq) from the Tensorflow team\\n\\n#### I translated the script to the Tensorflow 2.0 version, this way we can take part in the TF2 prizes and may use the version to improve the work.\\n\\n**A few notes:**\\n- If you want to keep using **flags** and **logging** you will have to use the **absl** lib (this is recommended by the TF team).\\n- Since we won\\'t use it with the kernels, I removed most of the **TPU** related stuff to reduce complexity.\\n- Tensorflow 2 don\\'t let us use global variables **(tf.compat.v1.trainable_variables())**.\\n- If you have experience with Tensorflow 2 or have any correction/improvement, please let me know.',\n",
       " '460df5f7': 'import numpy as np\\nimport cv2\\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\\nfrom keras.layers import Conv2D, Flatten, MaxPooling2D,Dense,Dropout,SpatialDropout2D\\nfrom keras.models  import Sequential\\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\\nimport random,os,glob\\nimport matplotlib.pyplot as plt',\n",
       " 'ca4cd2d0': \"dir_path = '../input/garbage classification/Garbage classification'\",\n",
       " 'd416ce6e': \"img_list = glob.glob(os.path.join(dir_path, '*/*.jpg'))\",\n",
       " '7b78796e': 'len(img_list)',\n",
       " '5d3d89e1': \"train=ImageDataGenerator(horizontal_flip=True,\\n                         vertical_flip=True,\\n                         validation_split=0.1,\\n                         rescale=1./255,\\n                         shear_range = 0.1,\\n                         zoom_range = 0.1,\\n                         width_shift_range = 0.1,\\n                         height_shift_range = 0.1,)\\n\\ntest=ImageDataGenerator(rescale=1/255,\\n                        validation_split=0.1)\\n\\ntrain_generator=train.flow_from_directory(dir_path,\\n                                          target_size=(300,300),\\n                                          batch_size=32,\\n                                          class_mode='categorical',\\n                                          subset='training')\\n\\ntest_generator=test.flow_from_directory(dir_path,\\n                                        target_size=(300,300),\\n                                        batch_size=32,\\n                                        class_mode='categorical',\\n                                        subset='validation')\\n\\nlabels = (train_generator.class_indices)\\nprint(labels)\\n\\nlabels = dict((v,k) for k,v in labels.items())\\nprint(labels)\",\n",
       " 'b4c1e48e': 'for image_batch, label_batch in train_generator:\\n  break\\nimage_batch.shape, label_batch.shape',\n",
       " '4afe7f61': \"print (train_generator.class_indices)\\n\\nLabels = '\\\\n'.join(sorted(train_generator.class_indices.keys()))\\n\\nwith open('labels.txt', 'w') as f:\\n  f.write(Labels)\\n\",\n",
       " 'ae860db6': 'model=Sequential()\\n#Convolution blocks\\n\\nmodel.add(Conv2D(32,(3,3), padding=\\'same\\',input_shape=(300,300,3),activation=\\'relu\\'))\\nmodel.add(MaxPooling2D(pool_size=2)) \\n#model.add(SpatialDropout2D(0.5)) # No accuracy\\n\\nmodel.add(Conv2D(64,(3,3), padding=\\'same\\',activation=\\'relu\\'))\\nmodel.add(MaxPooling2D(pool_size=2)) \\n#model.add(SpatialDropout2D(0.5))\\n\\nmodel.add(Conv2D(32,(3,3), padding=\\'same\\',activation=\\'relu\\'))\\nmodel.add(MaxPooling2D(pool_size=2)) \\n\\n#Classification layers\\nmodel.add(Flatten())\\n\\nmodel.add(Dense(64,activation=\\'relu\\'))\\n#model.add(SpatialDropout2D(0.5))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(32,activation=\\'relu\\'))\\n\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(6,activation=\\'softmax\\'))\\n\\nfilepath=\"trained_model.h5\"\\ncheckpoint1 = ModelCheckpoint(filepath, monitor=\\'val_acc\\', verbose=1, save_best_only=True, mode=\\'max\\')\\ncallbacks_list = [checkpoint1]\\n\\n',\n",
       " '826b624a': 'model.summary()',\n",
       " '19a6b2db': \"model.compile(loss='categorical_crossentropy',\\n              optimizer='adam',\\n              metrics=['acc']) # RMS PROP - No accuracy\\n\\n#es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\\n\",\n",
       " '41f19673': 'history = model.fit_generator(train_generator,\\n                              epochs=100,\\n                              steps_per_epoch=2276//32,\\n                              validation_data=test_generator,\\n                              validation_steps=251//32,\\n                              workers = 4,\\n                              callbacks=callbacks_list) \\n#41 epoch - 75% #73- 76.9%\\n#78 epoch - 80%',\n",
       " '6583d7cf': 'from keras.preprocessing import image\\n\\nimg_path = \\'../input/garbage classification/Garbage classification/plastic/plastic75.jpg\\'\\n\\nimg = image.load_img(img_path, target_size=(300, 300))\\nimg = image.img_to_array(img, dtype=np.uint8)\\nimg=np.array(img)/255.0\\n\\nplt.title(\"Loaded Image\")\\nplt.axis(\\'off\\')\\nplt.imshow(img.squeeze())\\n\\np=model.predict(img[np.newaxis, ...])\\n\\n#print(\"Predicted shape\",p.shape)\\nprint(\"Maximum Probability: \",np.max(p[0], axis=-1))\\npredicted_class = labels[np.argmax(p[0], axis=-1)]\\nprint(\"Classified:\",predicted_class)\\n\\n',\n",
       " 'efde5525': 'classes=[]\\nprob=[]\\nprint(\"\\\\n-------------------Individual Probability--------------------------------\\\\n\")\\n\\nfor i,j in enumerate (p[0],0):\\n    print(labels[i].upper(),\\':\\',round(j*100,2),\\'%\\')\\n    classes.append(labels[i])\\n    prob.append(round(j*100,2))\\n    \\ndef plot_bar_x():\\n    # this is for plotting purpose\\n    index = np.arange(len(classes))\\n    plt.bar(index, prob)\\n    plt.xlabel(\\'Labels\\', fontsize=12)\\n    plt.ylabel(\\'Probability\\', fontsize=12)\\n    plt.xticks(index, classes, fontsize=12, rotation=20)\\n    plt.title(\\'Probability for loaded image\\')\\n    plt.show()\\nplot_bar_x()',\n",
       " '8bf7dfb5': \"acc = history.history['acc']\\nval_acc = history.history['val_acc']\\n\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\n\\n# ________________ Graph 1 -------------------------\\n\\nplt.figure(figsize=(8, 8))\\nplt.subplot(2, 1, 1)\\nplt.plot(acc, label='Training Accuracy')\\nplt.plot(val_acc, label='Validation Accuracy')\\nplt.legend(loc='lower right')\\nplt.ylabel('Accuracy')\\nplt.ylim([min(plt.ylim()),1])\\nplt.title('Training and Validation Accuracy')\\n\\n# ________________ Graph 2 -------------------------\\n\\nplt.subplot(2, 1, 2)\\nplt.plot(loss, label='Training Loss')\\nplt.plot(val_loss, label='Validation Loss')\\nplt.legend(loc='upper right')\\nplt.ylabel('Cross Entropy')\\nplt.ylim([0,max(plt.ylim())])\\nplt.title('Training and Validation Loss')\\nplt.show()\",\n",
       " '302daec4': 'import tensorflow as tf\\nimport keras\\nfile = \"Garbage.h5\"\\nkeras.models.save_model(model,file)\\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(file)\\ntflite_model=converter.convert()\\nopen(\"garbage.tflite\",\\'wb\\').write(tflite_model)',\n",
       " 'c878e995': \"from IPython.display import FileLinks\\nFileLinks('.')\",\n",
       " '8f4c4034': '# Train',\n",
       " '0fa88287': '### Writing the labels file',\n",
       " 'ed9b90a1': '# Getting files from kernel',\n",
       " '63466cff': \"# Testing PREDICTION \\n##### Note: Path is of training dataset (pl. don't mind)\",\n",
       " '8f191e71': '# Accuracy Graph',\n",
       " 'd0b90d9f': '## Dataset Input',\n",
       " 'fb163203': '# Image Augmentation',\n",
       " 'f889af48': '# Building CNN & Saving keras model',\n",
       " '5481f265': '### Compiling Model using categorical cross entropy loss function & Adam Optimizer',\n",
       " '2c1fd81e': '## Converting to TFLite\\n#### Note: Image Size is 300',\n",
       " '7a879e36': '# Summarizing our model',\n",
       " '7f6344c1': '[Callback model Checkpoint Reference](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)',\n",
       " 'e25b2b2a': '# Import Libraries',\n",
       " 'b9dd9319': 'import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\\n\\nfrom sklearn.feature_selection import SelectKBest, f_regression\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import cross_val_score, train_test_split',\n",
       " 'a7664eea': \"df = pd.read_csv('../input/ISLR-Auto/Credit.csv')\\ndf.head()\",\n",
       " '92bf3829': \"df.drop(columns='Unnamed: 0', inplace=True, axis=1)\",\n",
       " 'a3c06efd': 'df.info()',\n",
       " '5dba6015': 'df.isna().sum()',\n",
       " '83ca5389': 'sns.pairplot(df)',\n",
       " 'a9025bbe': \"fig, ax = plt.subplots(figsize=(8, 5))\\nsns.heatmap(df.corr(), annot=True, fmt='.2f', ax=ax)\",\n",
       " '0fd50bd1': \"X = df.loc[:, 'Income':'Ethnicity']\\ny = df.loc[:, 'Balance']\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\",\n",
       " '96185336': \"NUM_FEATURES = ['Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education']\\nCAT_FEATURES = ['Gender', 'Student', 'Married', 'Ethnicity']\\n\\nnum_pipe = Pipeline(steps=[\\n    ('scale', StandardScaler()),   \\n])\\ncat_pipe = Pipeline(steps=[    \\n    ('encode', OneHotEncoder(drop='first')),   \\n    ('scale', StandardScaler(with_mean=False)),\\n])\\n\\npreprocessor = ColumnTransformer(transformers=[\\n    ('num', num_pipe, NUM_FEATURES),\\n    ('cat', cat_pipe, CAT_FEATURES),\\n], remainder='drop')\",\n",
       " 'b9306c2c': \"r2scores=[]\\nadjustedr2 = []\\nfeature_names=[]\\nfor i in range(1, 10):   \\n    reduce_dim_pipe = Pipeline(steps=[\\n        ('preprocess', preprocessor),\\n        ('reduce_dim', SelectKBest(k=i, score_func=f_regression)),       \\n    ])\\n    \\n    pipeline = Pipeline(steps=[\\n        ('reduce_dim_pipe', reduce_dim_pipe),       \\n        ('regress', LinearRegression())\\n    ])\\n    \\n    #calculate cross validated R2\\n    R2 = cross_val_score(pipeline, X=X_train, y=y_train,cv=10, scoring='r2').mean()    \\n    r2scores.append(R2)\\n        \\n    #calculate Adj R2\\n    n= len(X_train)\\n    p = i #len(X.columns)\\n    adj_R2 = 1- ((1-R2) * (n-1)/(n-p-1)) #Adj R2 = 1-(1-R2)*(n-1)/(n-p-1)\\n#     print(r2, adjustedr2)\\n    adjustedr2.append(adj_R2)\\n    \\n    reduce_dim_pipe.fit(X=X_train, y=y_train)\\n    # Get columns to keep    \\n    cols = reduce_dim_pipe.named_steps['reduce_dim'].get_support(indices=True)\\n    # Create new dataframe with only desired columns\\n#     print(cols)\\n    features_df_new = X_train.iloc[:, cols]\\n    best_features = list(features_df_new.columns)\\n#     print(best_features)\\n    feature_names.append(best_features)\",\n",
       " '8d4176b0': \"scoring_df = pd.DataFrame(np.column_stack((r2scores, adjustedr2)), columns=['R2', 'Adj_R2'])\\nscoring_df['feature_names'] = feature_names\\nscoring_df['features'] = range(1, 10)\\nscoring_df\",\n",
       " 'e59d7a27': \"fig, ax = plt.subplots(figsize=(8, 6))\\n#convert data frame from wide format to long format so that we can pass into seaborn line plot function to draw multiple line plots in same figure\\n# https://stackoverflow.com/questions/52308749/how-do-i-create-a-multiline-plot-using-seaborn\\nlong_format_df = pd.melt(scoring_df.loc[:, ['features','R2', 'Adj_R2']], ['features'])\\nsns.lineplot(x='features', y='value', hue='variable', data=long_format_df, ax=ax)\\nax.set_xlabel('No of features')\\nax.set_ylabel('Cross validated R2 and Adj R2 scores')\\nax.set_title('Plot between number of features and R2/Adj R2 scores')\",\n",
       " '865a0b14': \"## Feature or subset selection\\nWe will also use cross validation to find R2 and Adjusted R2 scores\\nWe will use sklearn's SelectKBest to find the optimum number of features.\",\n",
       " 'a9553135': \"### Interpretation\\n* The most important feature is 'rating'\\n* We can see that after 4-5 features, there is no improvement in R2 scores. \\n* We should not add more features to our model and that can add additional noise and can cause overfitting.\",\n",
       " 'b93f36f5': '## Data Preprocessing',\n",
       " '03fd7249': '## Exploratory Data Analysis (EDA)',\n",
       " '1462fe2d': '### Interpretation\\n* There is almost always a increase in the value of R2 score with addition of new features. On the other hand, there is no substantial increase in value of Adj R2 after number of features become 4.\\n* We can also see that with addition of limit when number of selected features are 2, the increase in adjusted R2 and R2 are less because limit is not adding much valeu to the model. The reason is that limit and rating are correlated.\\n* The increase in value of Adj R2 is less compared to R2 when number of features increase from 4 to 5. The Adj R2 remains constant or reduces with addition of every new feature.',\n",
       " 'c9022046': '### References\\n* https://datascience.stackexchange.com/questions/14693/what-is-the-difference-of-r-squared-and-adjusted-r-squared\\n* https://www.listendata.com/2014/08/adjusted-r-squared.html\\n* https://thestatsgeek.com/2013/10/28/r-squared-and-adjusted-r-squared/\\n',\n",
       " '4a01d292': '# Assess the accuracy of a model with R2 and Adjusted R2 and also understand the difference between them\\nWe will predict balance in the credit dataset which is a continuous variable. We will use linear regression and then understand how R2 and adjusted R2 differs. This will help to know why adjusted R2 is best suited to select a model with set of features.\\n\\n### R2:\\n* The R2 always increases as more variables are added.\\n* The model containing all of the predictors will always the largest R2, since these quantities are related to the training error. Instead, we wish to choose a model with a low test error. R2 is not suitable for selecting the best model among a collection of models with different numbers of predictors.\\n\\nR2 = 1-RSS/TSS\\n\\nwhere TSS = sum(yi − yhat)^2 is the total sum of squares for the response,\\nand TSS = sum(yi − ybar)^2 is the total sum of squares,\\n\\n### Adjusted R2:\\n* The intuition behind the adjusted R2 is that once all of the correct variables have been included in the model, adding additional noise variables\\n* A large value of adjusted R2 indicates a model with a small test error. The model with the largest adjusted R2 will have only correct variables and no noise variables. Unlike the R2 statistic, the adjusted R2 statistic pays a price for the inclusion of unnecessary variables in the model.\\n\\nAdj R2 = 1-(1-R2)*(n-1)/(n-p-1)',\n",
       " '48f3e5c1': '### Interpretation\\n* There are several catgorical features like Sex, Student, Married, Gender\\n* Balance is correlated with limit, rating, income\\n* There is also multicollinearity. for example, limit and rating are correlated.\\n* Valnce is not normally distributed , it is skewed to the left.',\n",
       " 'a1d3774c': 'import numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nfrom scipy import stats\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")',\n",
       " 'd96303eb': 'players = pd.read_csv(\"../input/fifa-22-complete-player-dataset/players_fifa22.csv\")\\nteams = pd.read_csv(\"../input/fifa-22-complete-player-dataset/teams_fifa22.csv\")',\n",
       " 'b3d62a1d': 'players.head()',\n",
       " '221015b2': 'teams.head()',\n",
       " 'f043f193': 'players.shape',\n",
       " 'ac47f2f2': 'teams.shape',\n",
       " '4aa8d6df': 'players.info()',\n",
       " '34afcde9': 'teams.info()',\n",
       " '795ebebe': 'players.describe().T',\n",
       " '72ca5338': 'teams.describe().T',\n",
       " '5c9dac3b': 'for col in players.columns:\\n    if players[col].isnull().any()==True:\\n        print(f\"{col}: {players[col].isnull().sum()}\")',\n",
       " 'c08e96f6': 'players.dropna(subset=[\"ClubPosition\",\"ContractUntil\",\"ClubNumber\"], inplace=True)\\nplayers.drop([\"NationalPosition\",\"NationalNumber\"], axis=1, inplace=True)',\n",
       " '12df8e29': 'players.duplicated().sum()',\n",
       " 'bd21009c': 'players.drop_duplicates(inplace=True)',\n",
       " 'bcc26792': 'teams.isnull().sum()',\n",
       " '4c9ff241': 'teams.duplicated().sum()',\n",
       " 'ba20b0e4': 'def find_outliers(df, col):\\n    Q1 = df[col].quantile(0.25)\\n    Q3 = df[col].quantile(0.75)\\n    IQR = Q3 - Q1\\n    return df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)].shape',\n",
       " '486d6242': 'int_float_cols = players.select_dtypes([\"int64\",\"float64\"])\\nfor col in int_float_cols:\\n    print(f\"{find_outliers(int_float_cols,col)}\")',\n",
       " '3626abbc': 'def remove_outliers(df,col):\\n    sorted(df[col])\\n    Q1 = df[col].quantile(0.25)\\n    Q3 = df[col].quantile(0.75)\\n    IQR = Q3 - Q1\\n    upper = Q3 + 1.5*IQR\\n    lower = Q1 - 1.5*IQR\\n    df[col] = np.where(df[col] > upper, upper, df[col])\\n    df[col] = np.where(df[col] < lower, lower, df[col])\\n    return df[col]',\n",
       " 'd7bb2b6f': 'for col in int_float_cols.columns:\\n    int_float_cols[col] = remove_outliers(int_float_cols, col)',\n",
       " 'ae6fefb7': 'for col in int_float_cols:\\n    print(f\"{find_outliers(int_float_cols,col)}\")',\n",
       " '58fe7a9c': 'object_cols = players.select_dtypes([\"object\"])\\nplayers = object_cols.join(int_float_cols)\\nplayers.info()',\n",
       " '4cee28ae': 'teams.duplicated().sum()',\n",
       " '0df458c1': 'plt.figure(figsize=(10,6))\\nsns.scatterplot(x=\"Overall\", y=\"Age\", data=players)\\nplt.title(\"Age vs Overall\", size=15)\\nplt.show()',\n",
       " '24346bbb': \"players.plot.hexbin(x='Potential', y='Age', gridsize=15, sharex=False)\",\n",
       " 'd04035ea': 'top_10_overall = players.sort_values(by=\"Overall\", ascending=False).head(10)\\nplt.figure(figsize=(10,6))\\nsns.barplot(x = top_10_overall.Weight, y = top_10_overall.Height)\\nplt.title(\"Height and Weight of Top 10 Players\", size=15)\\nplt.show()',\n",
       " '09e9bb50': 'print(top_10_overall[\"Nationality\"].value_counts())\\nplt.figure(figsize=(10,6))\\nsns.countplot(x = top_10_overall[\"Nationality\"])\\nplt.title(\"Nationality of Top 10 Football Player\", size=15)\\nplt.show()',\n",
       " 'e764ae02': 'plt.figure(figsize=(10,6))\\nsns.scatterplot(x=\"Overall\", y=\"TransferBudget\", data=teams)\\nplt.title(\"Overall vs Transfer Budget\", size=15)\\nplt.show()',\n",
       " '7cbad77a': 'top_10_overall_teams = teams.sort_values(by=\"Overall\").head(10)\\ntop_10_overall_teams',\n",
       " '6381ae17': 'print(top_10_overall_teams[\"League\"].value_counts())\\nplt.figure(figsize=(10,6))\\nsns.countplot(x = top_10_overall_teams[\"League\"])\\nplt.title(\"Leagues of Top 10 Football Teams\", size=15)\\nplt.show()',\n",
       " '0d7bed1f': 'plt.figure(figsize=(10,6))\\nsns.distplot(x = top_10_overall_teams[\"AllTeamAverageAge\"])\\nplt.title(\"Top 10 Football Teams\\' Average Player Ages\", size=15)\\nplt.show()',\n",
       " '4494e523': 'plt.figure(figsize=(10,6))\\nsns.histplot(x=\"Attack\", data=teams)\\nplt.title(\"Attack Grades of Football Teams\", size=15)\\nplt.show()',\n",
       " '3cfbd95b': 'plt.figure(figsize=(10,6))\\nsns.histplot(x=\"Midfield\", data=teams)\\nplt.title(\"Midfield Grades of Football Teams\", size=15)\\nplt.show()',\n",
       " 'f291547b': 'plt.figure(figsize=(10,6))\\nsns.histplot(x=\"Defence\", data=teams)\\nplt.title(\"Defence Grades of Football Teams\", size=15)\\nplt.show()',\n",
       " '1e95cfc0': 'plt.figure(figsize=(12,8))\\nsns.heatmap(teams.corr(), annot=True, cmap=\"RdBu\")\\nplt.title(\"Correlations Between Variables on Teams\", size=15)\\nplt.show()',\n",
       " 'ca747aad': 'Checking Duplicates on Teams Dataset',\n",
       " 'cb6e2f09': '# Detecting and Handling Missing Values on Players Dataset',\n",
       " 'e3e48606': '# Detecting and Handling Missing Values on Teams Dataset',\n",
       " 'fe946a9b': '# Detecting and Handling Duplicates on Players Dataset',\n",
       " 'fbc3628e': '# Detecting And Handling Outliers in Players Dataset',\n",
       " '7954f4d9': '# Head, Shape, Info and Statistical Summary of Data',\n",
       " 'cca3fa5b': '# Data Visualization',\n",
       " '493e9f9a': \"#Import necessary packages\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom matplotlib import animation,rc\\nfrom IPython.display import HTML\\n\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\",\n",
       " '5a9e2bf7': \"#Read the data\\nsummer_df=pd.read_csv('../input/60-years-of-paralympics/summer_paralympics.csv')\\nwinter_df=pd.read_csv('../input/60-years-of-paralympics/winter_paralympics.csv')\\nprint(summer_df.head())\\nwinter_df.head()\",\n",
       " '02f92fd2': '#Check for null values\\nprint(summer_df.isna().sum())\\nprint(winter_df.isna().sum())',\n",
       " '77d5c4af': 'years = summer_df[\\'Year\\'].unique()\\ncolors =[\\'#FF0000\\',\\'#169b62\\',\\'#008c45\\',\\'#aa151b\\',\\'#002868\\']\\n\\n#Setting the display\\nfont = {\\n    \\'weight\\': \\'normal\\',\\n    \\'size\\'  :  40,\\n    \\'color\\': \\'lightgray\\'\\n}\\nfig, ax = plt.subplots(figsize=(15, 5))\\nlabel = ax.text(0.95, 0.2, years[0],\\n            horizontalalignment=\\'right\\',\\n            verticalalignment=\\'top\\',\\n            transform=ax.transAxes,\\n            fontdict=font)\\nsns.set_style(\"white\")\\n\\n#Create the updating function for animation\\ndef update_summer(i):\\n    year = years[i]\\n    data_temp = summer_df[summer_df[\\'Year\\'] == year].sort_values(\\'Gold\\',ascending=False).head(10)\\n    ax.clear()\\n    sns.barplot(y=data_temp.Country,x=(data_temp.Gold+data_temp.Silver+data_temp.Bronze),color=\\'gold\\')\\n    sns.barplot(y=data_temp.Country,x=(data_temp.Silver+data_temp.Bronze),color=\\'ghostwhite\\')\\n    sns.barplot(y=data_temp.Country,x=(data_temp.Bronze),color=\\'darkgoldenrod\\')\\n    plt.title(year)\\n    label.set_text(year)\\n    \\nanim = animation.FuncAnimation(fig, update_summer, interval=1500,frames = len(years))\\nrc(\\'animation\\', html=\\'jshtml\\')\\n\\nHTML(anim.to_jshtml())',\n",
       " '2b5e1b72': 'years = winter_df[\\'Year\\'].unique()\\ncolors =[\\'#FF0000\\',\\'#169b62\\',\\'#008c45\\',\\'#aa151b\\',\\'#002868\\']\\n\\n#Setting the display\\nfont = {\\n    \\'weight\\': \\'normal\\',\\n    \\'size\\'  :  40,\\n    \\'color\\': \\'lightgray\\'\\n}\\nfig, ax = plt.subplots(figsize=(15, 5))\\nlabel = ax.text(0.95, 0.2, years[0],\\n            horizontalalignment=\\'right\\',\\n            verticalalignment=\\'top\\',\\n            transform=ax.transAxes,\\n            fontdict=font)\\nsns.set_style(\"white\")\\n\\n\\n#Create the updating function for animation\\ndef update_winter(i):\\n    year = years[i]\\n    data_temp = winter_df[winter_df[\\'Year\\'] == year].sort_values(\\'Gold\\',ascending=False).head(10)\\n    ax.clear()\\n    sns.barplot(y=data_temp.Country,x=(data_temp.Gold+data_temp.Silver+data_temp.Bronze),color=\\'gold\\',ci=None)\\n    sns.barplot(y=data_temp.Country,x=(data_temp.Silver+data_temp.Bronze),color=\\'ghostwhite\\',ci=None)\\n    sns.barplot(y=data_temp.Country,x=(data_temp.Bronze),color=\\'darkgoldenrod\\',ci=None)\\n    plt.title(year)\\n    label.set_text(year)\\n\\nanim = animation.FuncAnimation(fig, update_winter, interval=1500,frames = len(years))\\nrc(\\'animation\\', html=\\'jshtml\\')\\n\\nHTML(anim.to_jshtml())',\n",
       " 'c5ac6e90': \"#Determine the top 10 countries that gain the most gold\\ntop_10_country=list(summer_df.groupby('Country').sum().sort_values('Gold',ascending=False).head(10).index)\\n#Create the dataframe that capture the gold won by the top 10 countries\\nnew_df=summer_df[summer_df['Country'].isin(top_10_country)].groupby(['Year','Country']).sum()['Gold'].reset_index()\\nfig, ax = plt.subplots(figsize=(15, 5))\\nsns.lineplot(x='Year',y='Gold',hue='Country',style='Country',data=new_df)\\nplt.show()\",\n",
       " '6e25c79a': \"#Determine the top 10 countries that gain the most gold\\ntop_10_country=list(winter_df.groupby('Country').sum().sort_values('Gold',ascending=False).head(10).index)\\n#Create the dataframe that capture the gold won by the top 10 countries\\nnew_df=winter_df[winter_df['Country'].isin(top_10_country)].groupby(['Year','Country']).sum()['Gold'].reset_index()\\nfig, ax = plt.subplots(figsize=(15, 5))\\nsns.lineplot(x='Year',y='Gold',hue='Country',style='Country',data=new_df)\\nplt.show()\",\n",
       " 'af0d6b61': \"#Create a column that represent the ratio of athletes sent and medals won\\nsummer_df['Won_Percentage']=summer_df['M_Total']/summer_df['P_Total']\\n#Create a df that calculate the average of athletes sent, medals won, and their ratio, also the number of times each country participate\\nwon_percentage_summer=summer_df.groupby('Country').agg({'M_Total':'mean','P_Total':'mean','Won_Percentage':'mean','Year':'count'}).rename(columns={'Year':'Number of Participations'})\\n#Create a condition to subset the DF to only include the countries with won_percentage higher than 80% and participated in more than 1 paralympics\\nsummer_success_attribute=(won_percentage_summer['Won_Percentage']>=0.8)&(won_percentage_summer['Number of Participations']>1)\\nwon_percentage_summer[summer_success_attribute].sort_values('Won_Percentage',ascending=False)\",\n",
       " '8bb7e032': \"#Create a column that represent the ratio of athletes sent and medals won\\nwinter_df['Won_Percentage']=winter_df['M_Total']/winter_df['P_Total']\\n#Create a df that calculate the average of athletes sent, medals won, and their ratio, also the number of times each country participate\\nwon_percentage_winter=winter_df.groupby('Country').agg({'M_Total':'mean','P_Total':'mean','Won_Percentage':'mean','Year':'count'}).rename(columns={'Year':'Number of Participations'})\\n#Create a condition to subset the DF to only include the countries with won_percentage higher than 80% and participated in more than 1 paralympics\\nwinter_success_attribute=(won_percentage_winter['Won_Percentage']>=0.8)&(won_percentage_winter['Number of Participations']>1)\\nwon_percentage_winter[winter_success_attribute].sort_values('Won_Percentage',ascending=False)\",\n",
       " '00128465': \"fig, ax = plt.subplots(figsize=(15, 5))\\nsns.scatterplot(x='M_Total',y='P_Total',color='red',data=won_percentage_summer,\\n                label='Summer Paralympics',alpha=0.8)\\nsns.scatterplot(x='M_Total',y='P_Total',color='blue',data=won_percentage_winter,\\n                label='Winter Paralympics',alpha=0.8)\\nplt.xlabel('Average Medal won per Games')\\nplt.xscale('log')\\nplt.ylabel('Average Players sent per Games')\\nplt.show()\",\n",
       " '117259a6': \"fig, ax = plt.subplots(figsize=(5,5)) \\nsns.heatmap(won_percentage_summer.corr(),annot=True,linewidths=.5, ax=ax)\\nplt.title('Correlation Heatmap for the Summer Paralympics')\\nplt.show()\",\n",
       " '7efe7f8a': \"fig, ax = plt.subplots(figsize=(5,5)) \\nsns.heatmap(won_percentage_winter.corr(),annot=True,linewidths=.5, ax=ax)\\nplt.title('Correlation Heatmap for the Winter Paralympics')\\nplt.show()\",\n",
       " '10294a1c': '**How to be a Successful Country in Paralympics**',\n",
       " 'a6c0fb8d': 'As we see in the scatterplot, the more athlete each countries sent, the more medals they acquired. But how deterministic is it?',\n",
       " 'd4f1b5a4': '**Which country is the most successful in Paralympics(In terms of number of players sent and return with medals)?**',\n",
       " '3f722894': 'The USA used to dominate the summer Paralympics and their peak achievement was in the 80s when they acquire the most gold in the history of summer Paralympics. However, their performance had since been steadily declined and has been plateauing since the 2000s.\\n\\nChina, however, gain its first gold in 1984 and had been dominating the summer Paralympics since 2002.',\n",
       " '15967937': 'There are 9 countries with average win percentage above 80% and had participated summer paralympics more than once. Rhodesia is the most successful of them all with an average win percentage of 250% from 4 paralympic events. \\n\\nThere is also a country with win percentage similiar to Rhodesia, which is USSR. But USSR only participated in the paralympics once.',\n",
       " '540fd657': 'Austria, Norway, and Finland used to dominate the winter paralympics in the 70s up untill the mid 80s. But Russia and USA are the dominant player now since mid 2010s.',\n",
       " 'a2a42654': \"Looks like USA dominates the Summer Paralympics untill 2000 when Australia and then China takes the lead. While Norway used to dominate the games in the 80s. Let's take a closer look. \",\n",
       " 'e5a52472': 'The number of medals won and the number of players sent has a correlation of 0.94 for the summer paralympics and 0.88 for the winter paralympics. This suggests that the more athletes the country send, the more likely that country would dominate other countries in both the summer and winter paralympics.',\n",
       " '72d36517': 'How about winter games?',\n",
       " 'ae086644': '**Countries That Dominate the Paralympics Over the Years**',\n",
       " 'cfa42aac': 'import pandas as pd \\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns',\n",
       " 'b31e5c69': \"import os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\",\n",
       " 'd588c19e': \"rawdata = pd.read_csv('/kaggle/input/absenteeism-at-works/absenteeism.csv')\",\n",
       " 'dc1bf2e3': 'pd.options.display.max_columns = None',\n",
       " '3007a89e': 'rawdata',\n",
       " 'c6315087': 'df = rawdata.copy()',\n",
       " '779806e1': 'df.info()',\n",
       " '4d5bfc0b': \"df = df.drop(['ID'], axis = 1) \",\n",
       " '73668a3c': \"len(df['Reason for absence'].unique())\",\n",
       " '9278bf89': \"sorted(df['Reason for absence'].unique())\",\n",
       " '86ee7bc5': \"rcol = pd.get_dummies(df['Reason for absence'])\",\n",
       " '28cbae69': \"rcol['check'] = rcol.sum(axis=1)\",\n",
       " 'ae83617c': \"rcol['check'].unique() #checking if for sure every person have only one reason \",\n",
       " '6bf42393': \"rcol = rcol.drop(['check'], axis=1)\",\n",
       " '83f90471': \"rcol = pd.get_dummies(df['Reason for absence'], drop_first = True) #drop reason '0'\\nrcol\",\n",
       " '3383b768': \"df = df.drop(['Reason for absence'], axis=1) #drop 'Reason for absence', replace with dummies. \\ndf\",\n",
       " '0d3f0522': '# merging dummies into 4 categories based on reason for abscence\\nreasontype1 = rcol.loc[:, 1:14].max(axis=1)\\nreasontype2 = rcol.loc[:, 15:17].max(axis=1)\\nreasontype3 = rcol.loc[:, 18:21].max(axis=1)\\nreasontype4 = rcol.loc[:, 22:28].max(axis=1)',\n",
       " '8ab46a2f': 'print(reasontype1.sum(), reasontype2.sum(), reasontype3.sum(), reasontype4.sum())',\n",
       " '7a7f4f7d': 'df = pd.concat([df, reasontype1, reasontype2, reasontype3, reasontype4], axis = 1)\\ndf',\n",
       " 'fd98c389': \"column_names = ['Month of absence', 'Day of the week', 'Seasons',\\n       'Transportation expense', 'Distance from Residence to Work',\\n       'Service time', 'Age', 'Work load Average/day ', 'Hit target',\\n       'Disciplinary failure', 'Education', 'Son', 'Social drinker',\\n       'Social smoker', 'Pet', 'Weight', 'Height', 'Body mass index',\\n       'Absenteeism time in hours', 'Reason1', 'Reason2', 'Reason3', 'Reason4']\\ndf.columns = column_names\\ndf\",\n",
       " '9ae9adf2': \"reordered = ['Reason1', 'Reason2', 'Reason3', 'Reason4','Month of absence', 'Day of the week', 'Seasons',\\n       'Transportation expense', 'Distance from Residence to Work',\\n       'Service time', 'Age', 'Work load Average/day ', 'Hit target',\\n       'Disciplinary failure', 'Education', 'Son', 'Social drinker',\\n       'Social smoker', 'Pet', 'Weight', 'Height', 'Body mass index',\\n       'Absenteeism time in hours']\",\n",
       " 'a519275d': 'df = df[reordered]\\ndf',\n",
       " '60e0c439': 'df_mod1 = df.copy()\\ndf_mod1',\n",
       " 'd5f34f87': \"#correlation matrix\\ncormatrix = df_mod1.corr()\\nplt.subplots(figsize=(8, 8))\\nsns.heatmap(cormatrix, vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')\",\n",
       " '57cffabd': \"cols = cormatrix.nlargest(10, 'Absenteeism time in hours')['Absenteeism time in hours'].index\\ncorrcoef = np.corrcoef(df_mod1[cols].values.T)\\nplt.subplots(figsize=(8, 8))\\nsns.heatmap(corrcoef, annot=True,  yticklabels=cols.values, xticklabels=cols.values, vmin=-1, vmax=1, center= 0,  cmap= 'coolwarm')\",\n",
       " '4b06ba2d': 'df_mod2 = df_mod1.copy() ',\n",
       " 'f5755cf7': \"df_mod2 = df_mod2.drop(['Month of absence','Distance from Residence to Work','Body mass index'], axis = 1)\",\n",
       " 'f3f85cd9': \"df_mod2['Education'].unique()\",\n",
       " '1196a5ee': \"df_mod2['Education'].value_counts()\",\n",
       " 'dc0acbee': \"df_mod2['Education'] = df_mod2['Education'].map({1:0, 2:1, 3:1, 4:1})\",\n",
       " 'd6c76a6f': \"df_mod2['Education'].value_counts()\",\n",
       " 'd512e517': 'd_pre = df_mod2.copy()\\nd_pre',\n",
       " '22121e99': \"d_pre.to_csv('Absenteeism_preprocessed.csv', index=False)\",\n",
       " 'c86e1675': 'rcolumn is new dataframe with 28 columns which contains information about which I wrote above.\\n\\nTo this data frame we can add another column where it will be sum:',\n",
       " '69605797': 'Adding to data frame and rename it and then reordering columns because we want to see the reason first:',\n",
       " '3a09ce83': \"Next column, Reason from Absence - we have to keep in mind that they are represent categories that are equally meaningful so they are categorical nominal variables. We use numbers and provide to them descriptions because using less characters will think the volume of our dataset, it's easier to digest, btw it is called “database theory”.\\n\\nExtracting distinct values only:\",\n",
       " '34b84b0f': 'After loading a data always I’m exploring it manually. It helps to have some first predictions. Sometimes it helps find some errors - even like importing wrong file 😅 and let us dive in into problem. Jupyter Notebook or JupyterLab dont let us see whole table so I can use:',\n",
       " 'ef7b9113': 'There is no number ’20’ in the list. That means that nobody left the work because of “External causes of morbidity and mortality” (we know from the additional info UCI_ABS_TEXT) phew! We have to change this variables into dummy variables. Dummy variable is an explanatory binary variable that equals 1 - if a certain categorical effect is present 0 - if the same effect is absent\\n\\nWe our data we will do like this: 1 - if person was absent because of reason 1 0 - if person was absent because any other reason\\n\\nnext: 1 - if person was absent because of reason 2 0 - if person was absent because any other reason\\n\\nFortunately I don’t have to do it manually it is possible thanks to panda by simply .get_dummies()',\n",
       " '8daed3b5': \"In next stage I have to drop column ‘0’ from rcolumn dataframe. I'm doing this to avoid multicollinearity. For n categories we using n-1 dummies so I am dealing with 28 categories so I need only 27 dummies. (https://www.quora.com/How-and-why-having-the-same-number-of-dummy-variables-as-categories-is-problematic-in-linear-regression-Dummy-variable-trap-Im-looking-for-a-purely-mathematical-not-intuitive-explanation-Also-please-avoid-using-the)\\n\\nIn original ‘dataset’ I still have column called ‘Reason for absence’, if we will leave it we will have duplication of information which lead to multicollinearity. So lets drop this column from ‘dataset’. If we will add our ‘rcolumn’ into ‘dataset’ that means that we will have additional 27 columns in dataframe. A bit too much. Lets group these variables, this action we call classification. We will group basing on features descriptions: Reson1 1-14 diseases Reason2 15-17 - pregnancy related Reason 3 18 - 21 - poisonings Reason 4 22-28 - light reasons\\n\\nWe will create new data frame for each group. Thats why we needed to drop column with ID - because we need every individual have only one reason being out of work. So now we want to create a tables with only type of reason\\n\",\n",
       " '54ecc51e': 'Now we can see that 611 is undergraduate and only 129 people holds higher degree (graduate, postgraduate, a master or a doctor) so it is not so relevant anymore. We can combine them in single category. We can assign undergraduate as 0 and at least graduate to 1: 1 -> 0 2 -> 1 3 -> 1 4 -> 1',\n",
       " '1c1a114b': 'ID - individual identification - indicates precisely who has been away during working hours. It is a label variable to distinguish the individuals from one another, not to carry any numeric information.\\n\\nWe have to drop variable “ID” because it harm the estimation.',\n",
       " 'cc2f55fa': 'Dataset: https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work here you can download dataset and description to it\\n\\nAlways we starting with data preprocessing: group of operations that will convert raw data into a format that is easier to understand and useful for further processing and analysis. Also helps organize information in suitable and practical way. It takes the most of the time and it is crucial part of every analytical ask. While preprocessing we make raw dataset usable for machine learning algorithm.',\n",
       " 'c71154d0': \"I called it df_mod1 which stands for modified dataframe version 1. It is very good practice to creating checkpoints - it is help to organize, storing the current version of code so we reducing risk of losing our data at a later stages. We don’t have to do anything with date - month and the day of the week.\\n\\nLet’s move to next columns: Transportation Expense, Distance, Age, Daily Work Load, BMI - we are not going to manipulate them too.\\n\\nWhat we have next is:\\u2028 ‘Education’\\u2028(high school (1), graduate (2), postgraduate (3), master and doctor (4)) 'Son’ - Number of children ‘Pet’ - Number of pets Columns ‘Son’ and ‘Pet’ we will leave untouched.\\n\\nWe have to change education into dummy variable. To not scroll down everything lets check what we have in ‘education’ variable\",\n",
       " '25e0db6f': 'Saving file as csv: ',\n",
       " '9631545a': 'Creating a checkpoints - an interim save of your work',\n",
       " '0cf8ad66': \"import cv2, pandas as pd, matplotlib.pyplot as plt\\ntrain = pd.read_csv('../input/siim-isic-melanoma-classification/train.csv')\\nprint('Examples WITH Melanoma')\\nimgs = train.loc[train.target==1].sample(10).image_name.values\\nplt.figure(figsize=(20,8))\\nfor i,k in enumerate(imgs):\\n    img = cv2.imread('../input/jpeg-melanoma-128x128/train/%s.jpg'%k)\\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\\n    plt.subplot(2,5,i+1); plt.axis('off')\\n    plt.imshow(img)\\nplt.show()\\nprint('Examples WITHOUT Melanoma')\\nimgs = train.loc[train.target==0].sample(10).image_name.values\\nplt.figure(figsize=(20,8))\\nfor i,k in enumerate(imgs):\\n    img = cv2.imread('../input/jpeg-melanoma-128x128/train/%s.jpg'%k)\\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\\n    plt.subplot(2,5,i+1); plt.axis('off')\\n    plt.imshow(img)\\nplt.show()\",\n",
       " 'cf8f55ca': '!pip install -q efficientnet >> /dev/null',\n",
       " 'fc15127d': \"import pandas as pd, numpy as np\\nfrom kaggle_datasets import KaggleDatasets\\nimport tensorflow as tf, re, math\\nimport tensorflow.keras.backend as K\\nimport efficientnet.tfkeras as efn\\nfrom sklearn.model_selection import KFold\\nfrom sklearn.metrics import roc_auc_score\\nimport matplotlib.pyplot as plt\\n\\n!pip install tensorflow-addons=='0.9.1'\\nimport tensorflow_addons as tfa\",\n",
       " '2c748246': 'DEVICE = \"TPU\" #or \"GPU\"\\n\\n# USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\\nSEED = 47\\n\\n# NUMBER OF FOLDS. USE 3, 5, OR 15 \\nFOLDS = 5\\n\\n# WHICH IMAGE SIZES TO LOAD EACH FOLD\\n# CHOOSE 128, 192, 256, 384, 512, 768 \\nIMG_SIZES = [512,512,512,512,512]\\n\\n# INCLUDE OLD COMP DATA? YES=1 NO=0\\nINC2019 = [0,0,0,0,0]\\nINC2018 = [1,1,1,1,1]\\nPSEUDO = [1,1,1,1,1]\\n\\n# BATCH SIZE AND EPOCHS\\nBATCH_SIZES = [32]*FOLDS\\nEPOCHS = [17]*FOLDS\\n\\n\\n# WHICH EFFICIENTNET B? TO USE\\nEFF_NETS = [4,4,4,4,4]\\nfocal_loss = 0\\n# WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST\\nWGTS = [1/FOLDS]*FOLDS\\n\\n# TEST TIME AUGMENTATION STEPS\\nTTA = 11',\n",
       " '36909360': 'if DEVICE == \"TPU\":\\n    print(\"connecting to TPU...\")\\n    try:\\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\\n        print(\\'Running on TPU \\', tpu.master())\\n    except ValueError:\\n        print(\"Could not connect to TPU\")\\n        tpu = None\\n\\n    if tpu:\\n        try:\\n            print(\"initializing  TPU ...\")\\n            tf.config.experimental_connect_to_cluster(tpu)\\n            tf.tpu.experimental.initialize_tpu_system(tpu)\\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\\n            print(\"TPU initialized\")\\n        except _:\\n            print(\"failed to initialize TPU\")\\n    else:\\n        DEVICE = \"GPU\"\\n\\nif DEVICE != \"TPU\":\\n    print(\"Using default strategy for CPU and single GPU\")\\n    strategy = tf.distribute.get_strategy()\\n\\nif DEVICE == \"GPU\":\\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\\'GPU\\')))\\n    \\n\\nAUTO     = tf.data.experimental.AUTOTUNE\\nREPLICAS = strategy.num_replicas_in_sync\\nprint(f\\'REPLICAS: {REPLICAS}\\')',\n",
       " '7adb19b8': \"# SET MIXED PRECISION\\nMIXED_PRECISION = True\\nXLA_ACCELERATE = True\\n\\nif MIXED_PRECISION:\\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\\n    if DEVICE == 'TPU': policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\\n    mixed_precision.set_policy(policy)\\n    print('Mixed precision enabled')\\n\\nif XLA_ACCELERATE:\\n    tf.config.optimizer.set_jit(True)\\n    print('Accelerated Linear Algebra enabled')\",\n",
       " '20eae435': \"GCS_PATH = [None]*FOLDS; GCS_PATH2 = [None]*FOLDS\\nGCS_PATH3 = [None]  * FOLDS\\n\\nfor i in range(len(GCS_PATH3)) : \\n    GCS_PATH3[i] = KaggleDatasets().get_gcs_path('melanoma-pseudo-labelling-tf-record-512x512') #pseudodata ../input/data-pseudo-tf-records-09576-09526\\n\\nfor i,k in enumerate(IMG_SIZES):\\n    GCS_PATH[i] = KaggleDatasets().get_gcs_path('melanoma-%ix%i'%(k,k))\\n    GCS_PATH2[i] = KaggleDatasets().get_gcs_path('isic2019-%ix%i'%(k,k))\\nfiles_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/train*.tfrec')))\\nfiles_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/test*.tfrec')))\\n\\nfiles_pseulabeling = np.sort(np.array(tf.io.gfile.glob(GCS_PATH3[0] + '/train_pseudo_*.tfrec')))\",\n",
       " '2e77b8d4': 'len(files_pseulabeling)',\n",
       " '52f1c9bd': 'cutmix_rate = 0.3\\ngridmask_rate = 0.3\\nimg_size = 512\\n#BATCH_SIZE = 32\\nnb_classes = 1\\n\\n\\nDROP_FREQ = [0,0.75,0.75] # between 0 and 1\\nDROP_CT = [0,8,8] # may slow training if CT>16\\nDROP_SIZE = [0,0.2,0.2] # between 0 and 1\\n\\n\\nRATE = 0; CT = 0; SIZE = 0',\n",
       " '6bd83751': \"# batch\\ndef cutmix_v2(data, label, PROBABILITY = cutmix_rate):\\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\\n    # output - a batch of images with cutmix applied\\n    \\n    DIM = img_size    \\n    imgs = []; labs = []\\n    \\n    for j in range(BATCH_SIZES[fold] * REPLICAS):\\n        \\n        #random_uniform( shape, minval=0, maxval=None)        \\n        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\\n        P = tf.cast(tf.random.uniform([], 0, 1) <= PROBABILITY, tf.int32)\\n        \\n        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\\n        k = tf.cast(tf.random.uniform([], 0, BATCH_SIZES[fold] * REPLICAS), tf.int32)\\n        \\n        # CHOOSE RANDOM LOCATION\\n        x = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\\n        y = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\\n        \\n        # Beta(1, 1)\\n        b = tf.random.uniform([], 0, 1) # this is beta dist with alpha=1.0\\n        \\n\\n        WIDTH = tf.cast(DIM * tf.math.sqrt(1-b),tf.int32) * P\\n        ya = tf.math.maximum(0,y-WIDTH//2)\\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\\n        xa = tf.math.maximum(0,x-WIDTH//2)\\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\\n        \\n        # MAKE CUTMIX IMAGE\\n        one = data['image_data'][j,ya:yb,0:xa,:]\\n        two = data['image_data'][k,ya:yb,xa:xb,:]\\n        three = data['image_data'][j,ya:yb,xb:DIM,:]        \\n        #ya:yb\\n        middle = tf.concat([one,two,three],axis=1)\\n\\n        img = tf.concat([data['image_data'][j,0:ya,:,:],middle,data['image_data'][j,yb:DIM,:,:]],axis=0)\\n        imgs.append(img)\\n        \\n        # MAKE CUTMIX LABEL\\n        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\\n        lab1 = label[j,]\\n        lab2 = label[k,]\\n        labs.append((1-a)*lab1 + a*lab2)\\n\\n    image2 = tf.reshape(tf.stack(imgs),(BATCH_SIZES[fold] * REPLICAS ,DIM,DIM,3))\\n    label2 = tf.reshape(tf.stack(labs),(BATCH_SIZES[fold] * REPLICAS, nb_classes))\\n    \\n    data['image_data'] = image2\\n    \\n    return data,label2\",\n",
       " '98c6d88a': \"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\\n    # returns 3x3 transformmatrix which transforms indicies\\n        \\n    # CONVERT DEGREES TO RADIANS\\n    rotation = math.pi * rotation / 180.\\n    shear = math.pi * shear / 180.\\n    \\n    # ROTATION MATRIX\\n    c1 = tf.math.cos(rotation)\\n    s1 = tf.math.sin(rotation)\\n    one = tf.constant([1],dtype='float32')\\n    zero = tf.constant([0],dtype='float32')\\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\\n        \\n    # SHEAR MATRIX\\n    c2 = tf.math.cos(shear)\\n    s2 = tf.math.sin(shear)\\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \\n    \\n    # ZOOM MATRIX\\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\\n    \\n    # SHIFT MATRIX\\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\\n    \\n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\\n\\ncfg = dict(\\ntransform_prob    =  1.0,\\nrot               = 180.0,\\nshr               =   2.0,\\nhzoom             =   8.0,\\nwzoom             =   8.0,\\nhshift            =   8.0,\\nwshift            =   8.0,\\n)\\n\\ndef transform_shear_rot(image,cfg):\\n    \\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\\n    # output - image randomly rotated, sheared, zoomed, and shifted\\n    DIM = img_size\\n    XDIM = DIM%2 #fix for size 331\\n    \\n    rot = cfg['rot'] * tf.random.normal([1],dtype='float32')\\n    shr = cfg['shr'] * tf.random.normal([1],dtype='float32') \\n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/cfg['hzoom']\\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/cfg['wzoom']\\n    h_shift = cfg['hshift'] * tf.random.normal([1],dtype='float32') \\n    w_shift = cfg['wshift'] * tf.random.normal([1],dtype='float32') \\n\\n    # GET TRANSFORMATION MATRIX\\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \\n\\n    # LIST DESTINATION PIXEL INDICES\\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\\n    z = tf.ones([DIM*DIM],dtype='int32')\\n    idx = tf.stack( [x,y,z] )\\n    \\n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\\n    idx2 = K.cast(idx2,dtype='int32')\\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\\n    \\n    # FIND ORIGIN PIXEL VALUES           \\n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\\n    d = tf.gather_nd(image,tf.transpose(idx3))\\n        \\n    return tf.reshape(d,[DIM,DIM,3])\",\n",
       " 'b76b8f84': \"def transform_grid(image, inv_mat, image_shape):\\n    h, w, c = image_shape\\n    cx, cy = w//2, h//2\\n    new_xs = tf.repeat( tf.range(-cx, cx, 1), h)\\n    new_ys = tf.tile( tf.range(-cy, cy, 1), [w])\\n    new_zs = tf.ones([h*w], dtype=tf.int32)\\n    old_coords = tf.matmul(inv_mat, tf.cast(tf.stack([new_xs, new_ys, new_zs]), tf.float32))\\n    old_coords_x, old_coords_y = tf.round(old_coords[0, :] + w//2), tf.round(old_coords[1, :] + h//2)\\n    clip_mask_x = tf.logical_or(old_coords_x<0, old_coords_x>w-1)\\n    clip_mask_y = tf.logical_or(old_coords_y<0, old_coords_y>h-1)\\n    clip_mask = tf.logical_or(clip_mask_x, clip_mask_y)\\n    old_coords_x = tf.boolean_mask(old_coords_x, tf.logical_not(clip_mask))\\n    old_coords_y = tf.boolean_mask(old_coords_y, tf.logical_not(clip_mask))\\n    new_coords_x = tf.boolean_mask(new_xs+cx, tf.logical_not(clip_mask))\\n    new_coords_y = tf.boolean_mask(new_ys+cy, tf.logical_not(clip_mask))\\n    old_coords = tf.cast(tf.stack([old_coords_y, old_coords_x]), tf.int32)\\n    new_coords = tf.cast(tf.stack([new_coords_y, new_coords_x]), tf.int64)\\n    rotated_image_values = tf.gather_nd(image, tf.transpose(old_coords))\\n    rotated_image_channel = list()\\n    for i in range(c):\\n        vals = rotated_image_values[:,i]\\n        sparse_channel = tf.SparseTensor(tf.transpose(new_coords), vals, [h, w])\\n        rotated_image_channel.append(tf.sparse.to_dense(sparse_channel, default_value=0, validate_indices=False))\\n    return tf.transpose(tf.stack(rotated_image_channel), [1,2,0])\\n\\ndef random_rotate(image, angle, image_shape):\\n    def get_rotation_mat_inv(angle):\\n        # transform to radian\\n        angle = math.pi * angle / 180\\n        cos_val = tf.math.cos(angle)\\n        sin_val = tf.math.sin(angle)\\n        one = tf.constant([1], tf.float32)\\n        zero = tf.constant([0], tf.float32)\\n        rot_mat_inv = tf.concat([cos_val, sin_val, zero, -sin_val, cos_val, zero, zero, zero, one], axis=0)\\n        rot_mat_inv = tf.reshape(rot_mat_inv, [3,3])\\n        return rot_mat_inv\\n    angle = float(angle) * tf.random.normal([1],dtype='float32')\\n    rot_mat_inv = get_rotation_mat_inv(angle)\\n    return transform_grid(image, rot_mat_inv, image_shape)\\n\\ndef GridMask(image_height, image_width, d1, d2, rotate_angle=1, ratio=0.5):\\n    h, w = image_height, image_width\\n    hh = int(np.ceil(np.sqrt(h*h+w*w)))\\n    hh = hh+1 if hh%2==1 else hh\\n    d = tf.random.uniform(shape=[], minval=d1, maxval=d2, dtype=tf.int32)\\n    l = tf.cast(tf.cast(d,tf.float32)*ratio+0.5, tf.int32)\\n\\n    st_h = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\\n    st_w = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\\n\\n    y_ranges = tf.range(-1 * d + st_h, -1 * d + st_h + l)\\n    x_ranges = tf.range(-1 * d + st_w, -1 * d + st_w + l)\\n\\n    for i in range(0, hh//d+1):\\n        s1 = i * d + st_h\\n        s2 = i * d + st_w\\n        y_ranges = tf.concat([y_ranges, tf.range(s1,s1+l)], axis=0)\\n        x_ranges = tf.concat([x_ranges, tf.range(s2,s2+l)], axis=0)\\n\\n    x_clip_mask = tf.logical_or(x_ranges < 0 , x_ranges > hh-1)\\n    y_clip_mask = tf.logical_or(y_ranges < 0 , y_ranges > hh-1)\\n    clip_mask = tf.logical_or(x_clip_mask, y_clip_mask)\\n\\n    x_ranges = tf.boolean_mask(x_ranges, tf.logical_not(clip_mask))\\n    y_ranges = tf.boolean_mask(y_ranges, tf.logical_not(clip_mask))\\n\\n    hh_ranges = tf.tile(tf.range(0,hh), [tf.cast(tf.reduce_sum(tf.ones_like(x_ranges)), tf.int32)])\\n    x_ranges = tf.repeat(x_ranges, hh)\\n    y_ranges = tf.repeat(y_ranges, hh)\\n\\n    y_hh_indices = tf.transpose(tf.stack([y_ranges, hh_ranges]))\\n    x_hh_indices = tf.transpose(tf.stack([hh_ranges, x_ranges]))\\n\\n    y_mask_sparse = tf.SparseTensor(tf.cast(y_hh_indices, tf.int64),  tf.zeros_like(y_ranges), [hh, hh])\\n    y_mask = tf.sparse.to_dense(y_mask_sparse, 1, False)\\n\\n    x_mask_sparse = tf.SparseTensor(tf.cast(x_hh_indices, tf.int64), tf.zeros_like(x_ranges), [hh, hh])\\n    x_mask = tf.sparse.to_dense(x_mask_sparse, 1, False)\\n\\n    mask = tf.expand_dims( tf.clip_by_value(x_mask + y_mask, 0, 1), axis=-1)\\n\\n    mask = random_rotate(mask, rotate_angle, [hh, hh, 1])\\n    mask = tf.image.crop_to_bounding_box(mask, (hh-h)//2, (hh-w)//2, image_height, image_width)\\n\\n    return mask\\n\\n\\n\\ndef gridmask_v2(data, label_batch):\\n    return apply_grid_mask_v2(data, (img_size,img_size, 3)), label_batch\\n\\ndef apply_grid_mask_v2(data, image_shape, PROBABILITY = gridmask_rate):\\n    AugParams = {\\n        'd1' : 100,\\n        'd2': 160,\\n        'rotate' : 45,\\n        'ratio' : 0.3\\n    }\\n    \\n        \\n    mask = GridMask(image_shape[0], image_shape[1], AugParams['d1'], AugParams['d2'], AugParams['rotate'], AugParams['ratio'])\\n    if image_shape[-1] == 3:\\n        mask = tf.concat([mask, mask, mask], axis=-1)\\n        mask = tf.cast(mask,tf.float32)\\n        P = tf.cast(tf.random.uniform([], 0, 1) <= PROBABILITY, tf.int32)\\n    if P==1:\\n        data['image_data'] = data['image_data']  * mask\\n        return data\\n    else:\\n        return data\",\n",
       " '94744bda': 'def dropout(image, DIM=img_size, PROBABILITY = 0.75, CT = 8, SZ = 0.2):\\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\\n    # output - image with CT squares of side size SZ*DIM removed\\n    \\n    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\\n    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\\n    if (P==0)|(CT==0)|(SZ==0): return image\\n    \\n    for k in range(CT):\\n        # CHOOSE RANDOM LOCATION\\n        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\\n        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\\n        # COMPUTE SQUARE \\n        WIDTH = tf.cast( SZ*DIM,tf.int32) * P\\n        ya = tf.math.maximum(0,y-WIDTH//2)\\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\\n        xa = tf.math.maximum(0,x-WIDTH//2)\\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\\n        # DROPOUT IMAGE\\n        one = image[ya:yb,0:xa,:]\\n        two = tf.zeros([yb-ya,xb-xa,3]) \\n        three = image[ya:yb,xb:DIM,:]\\n        middle = tf.concat([one,two,three],axis=1)\\n        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM,:,:]],axis=0)\\n            \\n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR \\n    image = tf.reshape(image,[DIM,DIM,3])\\n    return image',\n",
       " '145d9826': \"ROT_ = 180.0\\nSHR_ = 2.0\\nHZOOM_ = 8.0\\nWZOOM_ = 8.0\\nHSHIFT_ = 8.0\\nWSHIFT_ = 8.0\\n\\n\\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\\n    # returns 3x3 transformmatrix which transforms indicies\\n        \\n    # CONVERT DEGREES TO RADIANS\\n    rotation = math.pi * rotation / 180.\\n    shear    = math.pi * shear    / 180.\\n\\n    def get_3x3_mat(lst):\\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\\n    \\n    # ROTATION MATRIX\\n    c1   = tf.math.cos(rotation)\\n    s1   = tf.math.sin(rotation)\\n    one  = tf.constant([1],dtype='float32')\\n    zero = tf.constant([0],dtype='float32')\\n    \\n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \\n                                   -s1,  c1,   zero, \\n                                   zero, zero, one])    \\n    # SHEAR MATRIX\\n    c2 = tf.math.cos(shear)\\n    s2 = tf.math.sin(shear)    \\n    \\n    shear_matrix = get_3x3_mat([one,  s2,   zero, \\n                                zero, c2,   zero, \\n                                zero, zero, one])        \\n    # ZOOM MATRIX\\n    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \\n                               zero,            one/width_zoom, zero, \\n                               zero,            zero,           one])    \\n    # SHIFT MATRIX\\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \\n                                zero, one,  width_shift, \\n                                zero, zero, one])\\n    \\n    return K.dot(K.dot(rotation_matrix, shear_matrix), \\n                 K.dot(zoom_matrix,     shift_matrix))\\n\\n\\ndef transform(image, DIM=img_size):    \\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\\n    # output - image randomly rotated, sheared, zoomed, and shifted\\n    XDIM = DIM%2 #fix for size 331\\n    \\n    rot = ROT_ * tf.random.normal([1], dtype='float32')\\n    shr = SHR_ * tf.random.normal([1], dtype='float32') \\n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\\n    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \\n    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \\n\\n    # GET TRANSFORMATION MATRIX\\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \\n\\n    # LIST DESTINATION PIXEL INDICES\\n    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\\n    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\\n    z   = tf.ones([DIM*DIM], dtype='int32')\\n    idx = tf.stack( [x,y,z] )\\n    \\n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\\n    idx2 = K.cast(idx2, dtype='int32')\\n    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\\n    \\n    # FIND ORIGIN PIXEL VALUES           \\n    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\\n    d    = tf.gather_nd(image, tf.transpose(idx3))\\n        \\n    return tf.reshape(d,[DIM, DIM,3])\",\n",
       " 'cb7177a6': 'def read_labeled_tfrecord(example):\\n    tfrec_format = {\\n        \\'image\\'                        : tf.io.FixedLenFeature([], tf.string),\\n        \\'image_name\\'                   : tf.io.FixedLenFeature([], tf.string),\\n        \\'patient_id\\'                   : tf.io.FixedLenFeature([], tf.int64),\\n        \\'sex\\'                          : tf.io.FixedLenFeature([], tf.int64),\\n        \\'age_approx\\'                   : tf.io.FixedLenFeature([], tf.int64),\\n        \\'anatom_site_general_challenge\\': tf.io.FixedLenFeature([], tf.int64),\\n      #  \\'diagnosis\\'                    : tf.io.FixedLenFeature([], tf.int64),\\n        \\'target\\'                       : tf.io.FixedLenFeature([], tf.int64)\\n    }           \\n    example = tf.io.parse_single_example(example, tfrec_format)\\n    \\n    data = {}\\n    \\n    image = prepare_image(example[\\'image\\'])\\n    data[\\'patient_id\\'] = tf.cast(example[\\'patient_id\\'],tf.float32)\\n    data[\\'sex\\'] = tf.cast(example[\\'patient_id\\'],tf.float32)\\n    data[\\'age_approx\\'] = tf.cast(example[\\'age_approx\\'],tf.float32)\\n    #data[\\'anatom_site_general_challenge\\'] = tf.cast(tf.one_hot(example[\\'anatom_site_general_challenge\\'], 7), tf.int32)\\n    \\n    data[\\'anatom_site_general_challenge\\'] = tf.cast(example[\\'anatom_site_general_challenge\\'],tf.float32)\\n    #data[\\'source\\'] = tf.cast(example[\\'source\\'],tf.float32)\\n    \\n    label = tf.cast(example[\\'target\\'], tf.float32)   \\n    \\n    return image, label , data\\n\\n\\ndef read_unlabeled_tfrecord(example,augment ,return_image_name):\\n    tfrec_format = {\\n        \\'image\\'                        : tf.io.FixedLenFeature([], tf.string),\\n        \\'image_name\\'                   : tf.io.FixedLenFeature([], tf.string),\\n        \"patient_id\" : tf.io.FixedLenFeature([],tf.int64) ,\\n      \"sex\": tf.io.FixedLenFeature([],tf.int64),\\n      \"age_approx\": tf.io.FixedLenFeature([],tf.int64),\\n      \"anatom_site_general_challenge\": tf.io.FixedLenFeature([],tf.int64),\\n    }\\n    \\n\\n    example = tf.io.parse_single_example(example, tfrec_format)\\n    data = {}\\n    image = prepare_image(example[\\'image\\'],augment = augment)\\n    img_name = example[\\'image_name\\'] if return_image_name else 0\\n    \\n    data[\\'patient_id\\'] = tf.cast(example[\\'patient_id\\'],tf.float32)\\n    data[\\'sex\\'] = tf.cast(example[\\'patient_id\\'],tf.float32)\\n    data[\\'age_approx\\'] = tf.cast(example[\\'age_approx\\'],tf.float32)\\n    \\n    data[\\'anatom_site_general_challenge\\'] = tf.cast(example[\\'anatom_site_general_challenge\\'],tf.float32)\\n    \\n    return image, img_name , data\\n    \\n \\ndef prepare_image(img, augment=True, dim=img_size):    \\n    img = tf.image.decode_jpeg(img, channels=3)\\n    img = tf.cast(img, tf.float32) / 255.0\\n    \\n    if augment:\\n        img = transform(img,DIM=dim)\\n        if (RATE!=0)&(CT!=0)&(SIZE!=0): \\n            img = dropout(img, DIM=dim, PROBABILITY=RATE, CT=CT, SZ=SIZE)\\n        img = tf.image.random_flip_left_right(img)\\n        #img = tf.image.random_hue(img, 0.01)\\n        img = tf.image.random_saturation(img, 0.7, 1.3)\\n        img = tf.image.random_contrast(img, 0.8, 1.2)\\n        img = tf.image.random_brightness(img, 0.1)\\n      \\n                      \\n    img = tf.reshape(img, [dim,dim, 3])\\n    img = tf.image.resize(img, [img_size,img_size])\\n            \\n    return img\\n\\ndef count_data_items(filenames):\\n    n = [int(re.compile(r\"-([0-9]*)\\\\.\").search(filename).group(1)) \\n         for filename in filenames]\\n    return np.sum(n)',\n",
       " '2f43e078': \"def train_setup(image,label,data) :\\n    #anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\\n    tab_data=[tf.cast(data[tfeat], dtype=tf.float32) for tfeat in ['sex','age_approx','patient_id','anatom_site_general_challenge']]#,'source','anatom_site_general_challenge',\\n    tabular=tf.stack(tab_data) #+anatom\\n    \\n    return {'image_data' : image , 'meta_data' : tabular} , label\\n\\ndef test_setup(image,image_name,data) :\\n    #anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\\n    tab_data=[tf.cast(data[tfeat], dtype=tf.float32) for tfeat in ['sex','age_approx','patient_id','anatom_site_general_challenge']] #,'source','anatom_site_general_challenge',\\n    tabular=tf.stack(tab_data) #+anatom\\n    return {'image_data' : image , 'meta_data' : tabular } , image_name\",\n",
       " '53d1c89f': 'def get_dataset(files, augment = False, shuffle = False, repeat = False, \\n                labeled=True, return_image_names=True,val = False, batch_size=32, dim=img_size):\\n    \\n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\\n    ds = ds.cache()\\n    \\n    if repeat:\\n        ds = ds.repeat()\\n    \\n    if shuffle: \\n        ds = ds.shuffle(1024*8)\\n        opt = tf.data.Options()\\n        opt.experimental_deterministic = False\\n        ds = ds.with_options(opt)\\n        \\n    if labeled: \\n        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\\n        ds = ds.map(train_setup , num_parallel_calls = AUTO)\\n        ds = ds.batch(batch_size * REPLICAS)\\n        ds = ds.prefetch(AUTO) \\n        \\n        if val == False :           \\n            if cutmix_rate :\\n                ds = ds.map(cutmix_v2,num_parallel_calls = AUTO) \\n            if gridmask_rate:\\n                ds = ds.map(gridmask_v2, num_parallel_calls=AUTO)\\n      \\n    else:\\n        ds = ds.map(lambda example: read_unlabeled_tfrecord(example,augment, return_image_names), \\n                    num_parallel_calls=AUTO)\\n        ds = ds.map(test_setup , num_parallel_calls = AUTO)\\n        ds = ds.batch(batch_size * REPLICAS)\\n        ds = ds.prefetch(AUTO)\\n        \\n    return ds',\n",
       " '28f2f4b2': \" '''ds = ds.map(lambda img, imgname_or_label: (prepare_image(img, augment=augment, dim=dim), \\n                                               imgname_or_label), \\n                num_parallel_calls=AUTO)\\n    \\n    ds = ds.batch(batch_size * REPLICAS)\\n    ds = ds.prefetch(AUTO)'''\",\n",
       " '0e2cfc6b': \"from tensorflow.keras.applications import InceptionResNetV2\\nfrom tensorflow.keras.applications.resnet50 import ResNet50\\n\\nimport efficientnet.tfkeras as efn\\nfrom tensorflow.keras.applications import DenseNet121, DenseNet201\\n\\nEFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \\n        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6]\\n\\ndef build_model(dim=img_size, ef=0):\\n    inp = tf.keras.layers.Input(shape=(dim,dim,3),name = 'image_data')\\n    inp2 = tf.keras.layers.Input(shape=(4),name = 'meta_data')\\n    base = EFNS[ef](input_shape=(dim,dim,3),weights='noisy-student',include_top=False , drop_connect_rate = 0.4) # try noisy student weights\\n    x = base(inp)\\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\\n    #x = tf.keras.layers.Dense(1024, activation = 'relu')(x) #added\\n    #x = tf.keras.layers.Dropout(0.4)(x) #added\\n    #x = tf.keras.layers.Dense(512, activation = 'relu')(x) #added\\n    #x = tf.keras.layers.Dropout(0.4)(x) #added\\n    \\n    x2 = tf.keras.layers.Dense(128, activation='relu')(inp2) #added\\n    #x2 = tf.keras.layers.Dropout(0.4)(x2) #added\\n    #x2 = tf.keras.layers.Dense(128, activation='relu')(x2) #added\\n    #x2 = tf.keras.layers.Dropout(0.3)(x2) #added\\n    #x2 = tf.keras.layers.Dense(32, activation='relu')(x2) #added\\n    #x2 = tf.keras.layers.Dropout(0.3)(x2) #added\\n    \\n    \\n    \\n    \\n    concat = tf.keras.layers.concatenate([x,x2]) #added\\n    \\n    concat = tf.keras.layers.Dense(512,activation = 'relu')(concat) #added\\n    concat = tf.keras.layers.Dropout(0.4)(concat) #added\\n    #concat = tf.keras.layers.Dense(256,activation = 'relu')(concat) #added#\\n    #concat = tf.keras.layers.Dropout(0.3)(concat) #added\\n    \\n    \\n    concat = tf.keras.layers.Dense(1,activation='sigmoid',dtype = 'float32')(concat)\\n    \\n    model = tf.keras.Model(inputs=[inp,inp2],outputs=concat)\\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\\n    #opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\\n    if focal_loss : \\n        loss = tfa.losses.SigmoidFocalCrossEntropy(reduction=tf.keras.losses.Reduction.AUTO)\\n    else :    \\n        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \\n    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\\n    return model\",\n",
       " '0d1aaf67': 'def get_lr_callback(batch_size=8):\\n    lr_start   = 0.000005\\n    lr_max     = 0.00000125 * REPLICAS * batch_size\\n    lr_min     = 0.000001\\n    lr_ramp_ep = 5\\n    lr_sus_ep  = 0\\n    lr_decay   = 0.8\\n   \\n    def lrfn(epoch):\\n        if epoch < lr_ramp_ep:\\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\\n            \\n        elif epoch < lr_ramp_ep + lr_sus_ep:\\n            lr = lr_max\\n            \\n        else:\\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\\n            \\n        return lr\\n\\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\\n    return lr_callback',\n",
       " '733a5e7c': '# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\\nVERBOSE = 0\\nDISPLAY_PLOT = True\\n\\nskf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\\noof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = [] \\npreds = np.zeros((count_data_items(files_test),1))\\n\\nfor fold,(idxT,idxV) in enumerate(skf.split(np.arange(15))):\\n    \\n    # DISPLAY FOLD INFO\\n    if DEVICE==\\'TPU\\':\\n        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\\n    print(\\'#\\'*25); print(\\'#### FOLD\\',fold+1)\\n    print(\\'#### Image Size %i with EfficientNet B%i and batch_size %i\\'%\\n          (IMG_SIZES[fold],EFF_NETS[fold],BATCH_SIZES[fold]*REPLICAS))\\n    \\n    # CREATE TRAIN AND VALIDATION SUBSETS\\n    files_train = tf.io.gfile.glob([GCS_PATH[fold] + \\'/train%.2i*.tfrec\\'%x for x in idxT])\\n    if INC2019[fold]:\\n        files_train += tf.io.gfile.glob([GCS_PATH2[fold] + \\'/train%.2i*.tfrec\\'%x for x in idxT*2+1])\\n        print(\\'#### Using 2019 external data\\')\\n    if INC2018[fold]:\\n        files_train += tf.io.gfile.glob([GCS_PATH2[fold] + \\'/train%.2i*.tfrec\\'%x for x in idxT*2])\\n        print(\\'#### Using 2018+2017 external data\\')\\n    if PSEUDO[fold] : \\n        files_train += tf.io.gfile.glob(GCS_PATH3[fold]+\\'/train_pseudo01-323.tfrec\\')\\n        files_train += tf.io.gfile.glob(GCS_PATH3[fold]+\\'/train_pseudo00-1000.tfrec\\')\\n        print(\\'#### Using Pseudo Labelling\\')\\n    np.random.shuffle(files_train); print(\\'#\\'*25)\\n    files_valid = tf.io.gfile.glob([GCS_PATH[fold] + \\'/train%.2i*.tfrec\\'%x for x in idxV])\\n    files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[fold] + \\'/test*.tfrec\\')))\\n    \\n    # BUILD MODEL\\n    K.clear_session()\\n    with strategy.scope():\\n        model = build_model(dim=img_size,ef=EFF_NETS[fold]) #IMG_SIZES[fold]\\n        \\n    # SAVE BEST MODEL EACH FOLD\\n    sv = tf.keras.callbacks.ModelCheckpoint(\\n        \\'fold-%i.h5\\'%fold, monitor=\\'val_loss\\', verbose=0, save_best_only=True,\\n        save_weights_only=True, mode=\\'min\\', save_freq=\\'epoch\\')\\n   \\n    # TRAIN\\n    print(\\'Training...\\')\\n    history = model.fit(\\n        get_dataset(files_train, augment=True, shuffle=True, repeat=True,\\n                dim=IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]), \\n        epochs=EPOCHS[fold], callbacks = [sv,get_lr_callback(BATCH_SIZES[fold])], \\n        steps_per_epoch=count_data_items(files_train)/BATCH_SIZES[fold]//REPLICAS,\\n        validation_data=get_dataset(files_valid,augment=False,shuffle=False,\\n                repeat=False,dim=IMG_SIZES[fold],val=True), #class_weight = {0:1,1:2},\\n        verbose=VERBOSE\\n    )\\n    \\n    print(\\'Loading best model...\\')\\n    model.load_weights(\\'fold-%i.h5\\'%fold)\\n    \\n    # PREDICT OOF USING TTA\\n    print(\\'Predicting OOF with TTA...\\')\\n    ds_valid = get_dataset(files_valid,labeled=False,return_image_names=False,augment=True,\\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\\n    ct_valid = count_data_items(files_valid); STEPS = TTA * ct_valid/BATCH_SIZES[fold]/4/REPLICAS\\n    pred = model.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:TTA*ct_valid,] \\n    oof_pred.append( np.mean(pred.reshape((ct_valid,TTA),order=\\'F\\'),axis=1) )                 \\n    #oof_pred.append(model.predict(get_dataset(files_valid,dim=IMG_SIZES[fold]),verbose=1))\\n    \\n    # GET OOF TARGETS AND NAMES\\n    ds_valid = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\\n            labeled=True, return_image_names=True,val=True)\\n    oof_tar.append( np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) )\\n    oof_folds.append( np.ones_like(oof_tar[-1],dtype=\\'int8\\')*fold )\\n    ds = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\\n                labeled=False, return_image_names=True)\\n    oof_names.append( np.array([img_name.numpy().decode(\"utf-8\") for img, img_name in iter(ds.unbatch())]))\\n    \\n    # PREDICT TEST USING TTA\\n    print(\\'Predicting Test with TTA...\\')\\n    ds_test = get_dataset(files_test,labeled=False,return_image_names=False,augment=True,\\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\\n    ct_test = count_data_items(files_test); STEPS = TTA * ct_test/BATCH_SIZES[fold]/4/REPLICAS\\n    pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:TTA*ct_test,] \\n    preds[:,0] += np.mean(pred.reshape((ct_test,TTA),order=\\'F\\'),axis=1) * WGTS[fold]\\n    \\n    # REPORT RESULTS\\n    auc = roc_auc_score(oof_tar[-1],oof_pred[-1])\\n    oof_val.append(np.max( history.history[\\'val_auc\\'] ))\\n    print(\\'#### FOLD %i OOF AUC without TTA = %.3f, with TTA = %.3f\\'%(fold+1,oof_val[-1],auc))\\n    \\n    # PLOT TRAINING\\n    if DISPLAY_PLOT:\\n        plt.figure(figsize=(15,5))\\n        plt.plot(np.arange(EPOCHS[fold]),history.history[\\'auc\\'],\\'-o\\',label=\\'Train AUC\\',color=\\'#ff7f0e\\')\\n        plt.plot(np.arange(EPOCHS[fold]),history.history[\\'val_auc\\'],\\'-o\\',label=\\'Val AUC\\',color=\\'#1f77b4\\')\\n        x = np.argmax( history.history[\\'val_auc\\'] ); y = np.max( history.history[\\'val_auc\\'] )\\n        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\\n        plt.scatter(x,y,s=200,color=\\'#1f77b4\\'); plt.text(x-0.03*xdist,y-0.13*ydist,\\'max auc\\\\n%.2f\\'%y,size=14)\\n        plt.ylabel(\\'AUC\\',size=14); plt.xlabel(\\'Epoch\\',size=14)\\n        plt.legend(loc=2)\\n        plt2 = plt.gca().twinx()\\n        plt2.plot(np.arange(EPOCHS[fold]),history.history[\\'loss\\'],\\'-o\\',label=\\'Train Loss\\',color=\\'#2ca02c\\')\\n        plt2.plot(np.arange(EPOCHS[fold]),history.history[\\'val_loss\\'],\\'-o\\',label=\\'Val Loss\\',color=\\'#d62728\\')\\n        x = np.argmin( history.history[\\'val_loss\\'] ); y = np.min( history.history[\\'val_loss\\'] )\\n        ydist = plt.ylim()[1] - plt.ylim()[0]\\n        plt.scatter(x,y,s=200,color=\\'#d62728\\'); plt.text(x-0.03*xdist,y+0.05*ydist,\\'min loss\\',size=14)\\n        plt.ylabel(\\'Loss\\',size=14)\\n        plt.title(\\'FOLD %i - Image Size %i, EfficientNet B%i, inc2019=%i, inc2018=%i\\'%\\n                (fold+1,IMG_SIZES[fold],EFF_NETS[fold],INC2019[fold],INC2018[fold]),size=18)\\n        plt.legend(loc=3)\\n        plt.show()  ',\n",
       " 'b7b8e0dc': \"# COMPUTE OVERALL OOF AUC\\noof = np.concatenate(oof_pred); true = np.concatenate(oof_tar);\\nnames = np.concatenate(oof_names); folds = np.concatenate(oof_folds)\\nauc = roc_auc_score(true,oof)\\nprint('Overall OOF AUC with TTA = %.3f'%auc)\\n\\n# SAVE OOF TO DISK\\ndf_oof = pd.DataFrame(dict(\\n    image_name = names, target=true, pred = oof, fold=folds))\\ndf_oof.to_csv('oof.csv',index=False)\\ndf_oof.head()\",\n",
       " '34369c06': 'ds = get_dataset(files_test, augment=False, repeat=False, dim=IMG_SIZES[fold],\\n                 labeled=False, return_image_names=True)\\n\\nimage_names = np.array([img_name.numpy().decode(\"utf-8\") \\n                        for img, img_name in iter(ds.unbatch())])',\n",
       " '098d1d8c': \"submission = pd.DataFrame(dict(image_name=image_names, target=preds[:,0]))\\nsubmission = submission.sort_values('image_name') \\nsubmission.to_csv('stratify_e4_128meta_512.4concat_18epochs_512img_with_pseudo_labeling_noisy_0.3gridmask_cutmix0.4drop_connect.csv', index=False)\\nsubmission.head()\",\n",
       " '02d242d6': 'plt.hist(submission.target,bins=100)\\nplt.show()',\n",
       " '9bd3c757': \"\\ntop = -100 #### Number of exapmles labeled as 1 with confidence\\nbottom = 2000 ####Number of exapmles labeled as 0 with confidence\\n\\n\\nmodel_1 = pd.read_csv('../input/data-for-pseudolabelling-09576-09526/averaging_bestsub_withmeta_data_0.75_0.25giba.csv')\\nmodel_2 = pd.read_csv('../input/data-for-pseudolabelling-09576-09526/averaging_bestsub_withmeta_data_0.7_0.3giba.csv')\\n#model_3 = pd.read_csv('../input/data-for-pseudolabelling-09576-09526/esemble_with_giba.csv')\\n#model_4 = pd.read_csv('../input/data-for-pseudolabelling-09576-09526/esemblev2_with_giba.csv')\\n#model_5 = pd.read_csv('../input/data-for-pseudolabelling-09576-09526/lghteams_models.csv')\\n\\ntest = pd.read_csv('../input/siim-isic-melanoma-classification/test.csv')\\n\\nmodel_1.head(5)\",\n",
       " 'e38c3967': \"\\nmodel1 = model_1.sort_values(by = ['target'])[top:].image_name\\nmodel1 = model1.values.tolist()\\n\\nmodel2 = model_2.sort_values(by = ['target'])[top:].image_name\\nmodel2 = model2.values.tolist()\\n\\nmodel3 = model_3.sort_values(by = ['target'])[top:].image_name\\nmodel3 = model3.values.tolist()\\n\\nmodel4 = model_4.sort_values(by = ['target'])[top:].image_name\\nmodel4 = model4.values.tolist()\\n\\nmodel5 = model_5.sort_values(by = ['target'])[top:].image_name\\nmodel5 = model5.values.tolist()\",\n",
       " '3cb4cbc2': \"label_1_imagename = set(model1).intersection(model2) #.intersection(model3).intersection(model4).intersection(model5)\\nprint('Number of new positive examples: %i' %len(label_1_imagename))\",\n",
       " '467aba77': \"label_1_data = test.loc[test['image_name'].isin(label_1_imagename)].reset_index(drop = True)\\nlabel_1_data['target'] = 1\\nlabel_1_data.head(10)\",\n",
       " 'e635310d': \"model1 = model_1.sort_values(by = ['target'])[:bottom].image_name\\nmodel1 = model1.values.tolist()\\n\\nmodel2 = model_2.sort_values(by = ['target'])[:bottom].image_name\\nmodel2 = model2.values.tolist()\\n\\nmodel3 = model_3.sort_values(by = ['target'])[:bottom].image_name\\nmodel3 = model3.values.tolist()\\n\\nmodel4 = model_4.sort_values(by = ['target'])[:bottom].image_name\\nmodel4 = model4.values.tolist()\\n\\nmodel5 = model_5.sort_values(by = ['target'])[:bottom].image_name\\nmodel5 = model5.values.tolist()\",\n",
       " '104cba94': \"label_0_imagename = set(model1).intersection(model2) #.intersection(model3).intersection(model4).intersection(model5)\\nprint('Number of new negative examples: %i' %len(label_0_imagename))\",\n",
       " '75da0b02': \"label_0_data = test.loc[test['image_name'].isin(label_0_imagename)].reset_index(drop = True)\\nlabel_0_data['target'] = 0\\nlabel_0_data.head(10)\",\n",
       " 'd9031789': 'label_1_data.dropna(inplace = True)\\nlabel_0_data.dropna(inplace = True)',\n",
       " '6f67ca82': 'data = pd.concat([label_0_data, label_1_data])\\ndata.shape',\n",
       " '25f029e1': \"PATH = '../input/jpeg-melanoma-512x512/test/'\\n\\nIMGS = data['image_name'].values\\nIMGS = IMGS + '.jpg'\\n\\nprint('Number of images: %i' %len(IMGS))\",\n",
       " '74831ad6': \"str_col = ['patient_id','sex','anatom_site_general_challenge'] \\nfor col in str_col:\\n    data[col], mp = data[col].factorize()\\n    print(mp)\\n\\ndata['age_approx'] = data['age_approx'].astype('int')\",\n",
       " '81b2a0b4': 'def _bytes_feature(value):\\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\\n    if isinstance(value, type(tf.constant(0))):\\n        value = value.numpy() # BytesList won\\'t unpack a string from an EagerTensor.\\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\\n\\ndef _float_feature(value):\\n    \"\"\"Returns a float_list from a float / double.\"\"\"\\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\\n\\ndef _int64_feature(value):\\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))',\n",
       " '99276ddc': \"def serialize_example(feature0, feature1, feature2, feature3, feature4, feature5, feature6):\\n    \\n    feature = {\\n      'image': _bytes_feature(feature0),\\n      'image_name': _bytes_feature(feature1),\\n      'patient_id': _int64_feature(feature2),\\n      'sex': _int64_feature(feature3),\\n      'age_approx': _int64_feature(feature4),\\n      'anatom_site_general_challenge': _int64_feature(feature5),\\n      'target': _int64_feature(feature6)\\n  }\\n    example_proto = tf.train.Example(features = tf.train.Features(feature = feature))\\n    return example_proto.SerializeToString()\",\n",
       " 'fec57b5d': \"SIZE = 1000\\n\\nCT = len(IMGS)//SIZE + int(len(IMGS)%SIZE != 0)\\n\\nfor j in range(CT):\\n    print(); print('Writing TFRecord %i of %i...' %(j, CT))\\n    CT2 = min(SIZE, len(IMGS)- j*SIZE)\\n    \\n    with tf.io.TFRecordWriter('train_pseudo_512%.2i-%i.tfrec' %(j, CT2)) as writer:\\n        for k in range(CT2):\\n            img = cv2.imread(PATH + IMGS[SIZE*j + k])\\n            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 95))[1].tostring()\\n            \\n            name = IMGS[SIZE*j + k].split('.')[0]\\n            row = data.loc[data.image_name == name]\\n            \\n            example = serialize_example(\\n                img,\\n                str.encode(name),\\n                row.patient_id.values[0],\\n                row.sex.values[0],\\n                row.age_approx.values[0],                        \\n                row.anatom_site_general_challenge.values[0],\\n                row.target.values[0])\\n            \\n            writer.write(example)\\n            \\n            if k%100 == 0: print(k, ', ', end = '')\",\n",
       " 'f033ec4a': '# Pseudo Labeling :',\n",
       " 'b73636ab': \"This notebook is my work throughout this competition you'll find evrything you need in here, it's based on chris's notebook so thanks to him. At the end you'll find how to create pseudo labels and tf records , load them and retrain for better score !\",\n",
       " 'fbe9df1d': 'pwd',\n",
       " '05113ab2': 'cd ../input/hr-data-for-analytics',\n",
       " '2acb6eea': 'import pandas as pd',\n",
       " '3c78f497': 'data=pd.read_csv(\"HR_comma_sep.csv\")',\n",
       " '7a709180': 'data.head()',\n",
       " '1ab3fd4e': 'data.shape',\n",
       " 'ed7f0d28': 'data.info()',\n",
       " 'edeab1bd': 'from sklearn.preprocessing import LabelEncoder\\nlabel_encoder=LabelEncoder()',\n",
       " 'd5aa911b': 'data.sales.unique()',\n",
       " 'c25c698f': 'data.sales=label_encoder.fit_transform(data.sales)',\n",
       " 'c4e111a1': 'data.salary.unique()',\n",
       " '8458c2a6': 'data.salary=label_encoder.fit_transform(data.salary)',\n",
       " '5b6a27b8': 'data.head()',\n",
       " 'ad6fed43': \"y=data['left']\\ny.unique()\",\n",
       " '543831c7': \"X=data.drop(['left'],axis=1)\",\n",
       " '4cc1058f': 'from sklearn.model_selection import train_test_split',\n",
       " 'b7cef407': 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)',\n",
       " 'fc185318': 'from sklearn.linear_model import LogisticRegression',\n",
       " '8967afb9': 'model=LogisticRegression()',\n",
       " 'a391fde5': 'model.fit(X_train,y_train)',\n",
       " '8a6e629c': 'model.predict(X_test)',\n",
       " '907c3e31': 'model.score(X_test,y_test)',\n",
       " '23e88212': 'data.head()',\n",
       " '4fe6a325': 'data.satisfaction_level=data.satisfaction_level*10',\n",
       " '408b7794': 'data.head()',\n",
       " '05ba3079': 'data.last_evaluation=data.last_evaluation*10',\n",
       " '3bfbaaf9': 'data.head()',\n",
       " '42849d98': 'X=data',\n",
       " 'aa669d4b': 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)',\n",
       " 'a6f1b586': 'model.fit(X_train,y_train)',\n",
       " '3bc51d8a': 'model.predict(X_test)',\n",
       " '4bc6c012': 'model.score(X_test,y_test)',\n",
       " '0befed08': 'model.predict_proba(X_test)',\n",
       " '7fb58814': 'converted the categorical data to numerical data using labelencoder',\n",
       " 'ecaf3a5a': \"as the values in the column satsfaction_level and last_evaluation are not in the order of the other entries we can multiply the values in the column by a constant to make it in the order of the column values .\\nas the columns are a kind of rating multiplying it by 10 won't change the meaning .\\npreviously the ratings were out of 1,now it is out of 10.meaning remains the same.\",\n",
       " '577200d9': '# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\\n# For example, here\\'s several helpful packages to load\\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# Input data files are available in the read-only \"../input/\" directory\\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\n\\nimport os\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \\n# You can also write temporary files to /kaggle/temp/, but they won\\'t be saved outside of the current session',\n",
       " 'ee592fff': \"import numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\n\\nfruit= pd.read_csv('../input/fruits/fruit_data_with_colors _1_.csv')\\nfruit.head()\\n\",\n",
       " '9bde08c8': \"fruit['fruit_name'].unique()\",\n",
       " '2cdd9f91': '#examine the null variable\\nfruit.isnull().sum()',\n",
       " 'c01d0fc4': \"#replace the missing data by the average of its column\\nfruit['mass']=fruit['mass'].fillna(fruit['mass'].mean())\\nfruit['height']=fruit['height'].fillna(fruit['height'].mean())\",\n",
       " 'b4e07898': 'fruit.isnull().sum()',\n",
       " 'd19139de': \"# plotting a scatter matrix\\nfrom matplotlib import cm\\nfrom pandas.plotting import scatter_matrix\\n\\nX = fruit[['height', 'width', 'mass', 'color_score']]\\ny = fruit['fruit_label']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\n\\ncmap = cm.get_cmap('gnuplot')\\nscatter =pd.plotting.scatter_matrix(X_train, c= y_train, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(9,9), cmap=cmap)\",\n",
       " '4e971a62': \"# plotting a 3D scatter plot\\nfrom mpl_toolkits.mplot3d import Axes3D\\n\\nfig= plt.figure(figsize=(10,8))\\nax = fig.add_subplot(111, projection = '3d')\\nax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c = y_train, marker = 'o', s=100)\\nax.set_xlabel('width')\\nax.set_ylabel('height')\\nax.set_zlabel('color_score')\\nplt.show()\",\n",
       " '1a7eb826': \"# For this example, we use the mass, width, and height features of each fruit instance\\nX = fruit[['mass', 'width', 'height']]\\ny = fruit['fruit_label']\\n\\n# default is 75% / 25% train-test split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\",\n",
       " 'ba34dc93': 'from sklearn.neighbors import KNeighborsClassifier\\n\\nknn = KNeighborsClassifier(n_neighbors = 5)',\n",
       " '51e0a9f1': 'knn.fit(X_train, y_train)',\n",
       " 'e77982fe': 'knn.score(X_test, y_test)',\n",
       " '7dfb24af': \"# first example: a small fruit with mass 30g, width 5.3 cm, height 4.5 cm\\nfruit_prediction = knn.predict([[30, 5.3, 4.5]])\\nfruit_number=fruit_prediction[0]\\nif fruit_number == 1:\\n    print('apple')\\nelif fruit_number==2:\\n    print('mandarin')\\nelif fruit_number==3:\\n    print('lemon')\\nelse:\\n    print('organge')\",\n",
       " 'a1b3079d': \"# second example: 120g, width 7.7 cm, height 8.5 cm\\nfruit_prediction = knn.predict([[120, 7.7, 8.5]])\\nfruit_number=fruit_prediction[0]\\nif fruit_number == 1:\\n    print('apple')\\nelif fruit_number==2:\\n    print('mandarin')\\nelif fruit_number==3:\\n    print('lemon')\\nelse:\\n    print('organge')\",\n",
       " 'b5cc736d': \"#set the k ranges from 1 to 20\\nk_range = range(1,20)\\nscores = []\\n\\nfor k in k_range:\\n    knn = KNeighborsClassifier(n_neighbors = k)\\n    knn.fit(X_train, y_train)\\n    scores.append(knn.score(X_test, y_test))\\n\\nplt.figure(figsize=(10,8))\\nplt.xlabel('k')\\nplt.ylabel('accuracy')\\nplt.scatter(k_range, scores)\\nplt.xticks([0,5,10,15,20])\\nplt.show()\",\n",
       " 'db29fb23': \"#set the train-test split run from 20 % to 80% \\nt = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\\n\\nknn = KNeighborsClassifier(n_neighbors = 5)\\n\\nplt.figure(figsize=(10,8))\\n\\nfor s in t:\\n\\n    scores = []\\n    for i in range(1,1000):\\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s)\\n        knn.fit(X_train, y_train)\\n        scores.append(knn.score(X_test, y_test))\\n    plt.plot(s, np.mean(scores), 'bo')\\n\\nplt.xlabel('Training set proportion (%)')\\nplt.ylabel('accuracy')\\nplt.show()\",\n",
       " '083dceec': '# Use the trained k-NN classifier model to classify new objects',\n",
       " 'ee01a50c': '# Examining the data',\n",
       " 'ea959649': '# Train the classifier using the training data',\n",
       " '39cd16e9': \"# How the choice of the 'k' parameter affect k-NN classification accuracy?\",\n",
       " '69cc277f': '# How the train/test split proportion affect k-NN classification accuracy?',\n",
       " '61eab1c8': '# Replace the Missing Data',\n",
       " '5827ff9f': '# Create classifier object',\n",
       " 'c41f095c': 'Credit: University of Michigan',\n",
       " 'ee86e8db': 'Purpose of this notebook: \\nUsing the KNN classication to exam which fruit species base on its mass, width, heigh and color',\n",
       " 'fd1aaef8': 'the higher training set propotion, the better knn-classification accuracy',\n",
       " '7bb19556': '# Import Library and load data file',\n",
       " 'c52a6a23': 'The file contains the mass, height, and width of a selection of oranges, lemons and apples. The heights were measured along the core of the fruit. The widths were the widest width perpendicular to the height',\n",
       " '38afd5a3': '# Create train-test split',\n",
       " '78e65c2f': 'k parameter <=5 shows the best accuracy ',\n",
       " '5fde459d': '# THE END',\n",
       " '82a9b510': '# Estimate the accuracy of the classifier on future data, using the test data',\n",
       " 'db1cb572': '# สำหรับเพื่อนๆ ที่ไม่ได้ทำบน Kaggle สามารถ install ได้ด้วยคำสั่ง\\n# pip install albumentations\\n# ครับ\\nfrom albumentations import * ',\n",
       " '7f9304a6': 'import json\\nimport math\\nimport os\\n\\nimport cv2\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nfrom tqdm import tqdm\\n\\n%matplotlib inline',\n",
       " 'c4ea69e9': \"# ภาพฝึกสอนจะอยู่ใน directory train_images\\n# โดยข้อมูลชื่อภาพและระดับความรุนแรงของดวงตาจะอยู่ใน train.csv\\n!ls ../input/\\nprint('\\\\nข้างล่างคือตัวอย่างไฟล์ภาพที่เรามี ...\\\\n')\\n!ls ../input/train_images | head\",\n",
       " 'b4f5f09c': \"df_train = pd.read_csv('../input/train.csv')\\ndf_train.head() # ชื่อไฟล์ (เราต้องใส่ full path และ .png เข้าไปเองตอนโหลด) รวมทั้งระดับความรุนแรงของโรคเบาหวานในดวงตา\",\n",
       " 'c9024896': \"# df_train['diagnosis'].hist() # เอา comment ออกถ้าเพื่อนๆ อยากเห็น severity distribution histogram\\nx = df_train['id_code'].values\\ny = df_train['diagnosis'].values\",\n",
       " '5cfe684b': \"# model.summary()\\nSIZE=(288,350)\\nIMG_SIZE=SIZE\\n\\ndef crop_image(img,tol=7):\\n    if img.ndim ==2:\\n        mask = img>tol\\n        return img[np.ix_(mask.any(1),mask.any(0))]\\n    elif img.ndim==3:\\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\\n        mask = gray_img>tol\\n        \\n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\\n        if (check_shape == 0): # image is too dark so that we crop out everything,\\n            return img # return original image\\n        else:\\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\\n    #         print(img1.shape,img2.shape,img3.shape)\\n            img = np.stack([img1,img2,img3],axis=-1)\\n    #         print(img.shape)\\n        return img\\n\\ndef load_image_ben_orig(path,resize=True,crop=True,norm255=True,keras=False):\\n    image = cv2.imread(path)\\n    \\n    if crop:\\n        image = crop_image(image)\\n    \\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\\n        \\n    if resize:\\n        image = cv2.resize(image,SIZE)\\n        \\n    image=cv2.addWeighted( image,4, cv2.GaussianBlur( image , (0,0) ,  10) ,-4 ,128)\\n#     image=cv2.addWeighted( image,4, cv2.medianBlur( image , 10) ,-4 ,128)\\n    \\n    # NOTE plt.imshow can accept both int (0-255) or float (0-1), but deep net requires (0-1)\\n    if norm255:\\n        return image/255\\n    elif keras:\\n        #see https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py for mode\\n        #see https://github.com/keras-team/keras-applications/blob/master/keras_applications/xception.py for inception,xception mode\\n        #the use of tf based preprocessing (- and / by 127 respectively) will results in [-1,1] so it will not visualize correctly (directly)\\n        image = np.expand_dims(image, axis=0)\\n        return preprocess_input(image)[0]\\n    else:\\n        return image.astype(np.int16)\\n    \\n    return image\\n\\ndef show_image(image,figsize=None,title=None):\\n    \\n    if figsize is not None:\\n        fig = plt.figure(figsize=figsize)\\n#     else: # crash!!\\n#         fig = plt.figure()\\n        \\n    if image.ndim == 2:\\n        plt.imshow(image,cmap='gray')\\n    else:\\n        plt.imshow(image)\\n        \\n    if title is not None:\\n        plt.title(title)\\n\\ndef show_Nimages(imgs,scale=1):\\n\\n    N=len(imgs)\\n    fig = plt.figure(figsize=(25/scale, 16/scale))\\n    for i, img in enumerate(imgs):\\n        ax = fig.add_subplot(1, N, i + 1, xticks=[], yticks=[])\\n        show_image(img)\",\n",
       " 'a0f32aa9': 'SEED=88\\nNUM=7\\nfig = plt.figure(figsize=(25, 16))\\nfor class_id in sorted(np.unique(y)):\\n    images=[]\\n    if class_id < 4:\\n        continue\\n    for i, (idx, row) in enumerate(df_train.loc[y == class_id].sample(NUM, random_state=SEED).iterrows()):\\n        path=f\"../input/train_images/{row[\\'id_code\\']}.png\"\\n        images.append(load_image_ben_orig(path).astype(np.float32))\\n        \\n    show_Nimages(images)',\n",
       " '0c2dad1e': \"'''บรรทัดเดียวจบครับ aug0 คือ augmentation operation ที่เราเลือกมาจาก Albumentation document, img คือภาพที่เราต้องการ augment '''\\ndef apply_augment(aug0, img):\\n    return aug0(image=img)['image']\",\n",
       " '4dc7f2eb': \"'''สร้างฟังก์ชันเพื่อทำ apply_augment เดียวกันกับ list of images หลายๆ รูปในคราวเดียวและให้ plot รูปก่อนและหลัง augment มาดูกัน'''\\ndef batch_apply_augment(aug0,images):\\n    augmented_images=[]\\n    for ii in range(len(images)):\\n        aug_img = apply_augment(aug0,images[ii])\\n        augmented_images.append(aug_img)\\n    \\n    show_Nimages(images)\\n    show_Nimages(augmented_images)\",\n",
       " 'cdb63c16': \"'''ตัวอย่างใช้งาน Flip หรือส่องกระจก (สุ่มแนวตั้งหรือแนวนอน) ด้วยความน่าจะเป็น p=0.8 ของภาพดวงตาที่เราเซพไว้'''\\naug1 = Flip(p=0.8)\\nbatch_apply_augment(aug1,images)\",\n",
       " 'a958bfcb': \"'''สุ่มเปลี่ยนความมืดความสว่างของรูป : RandomBrightnessContrast'''\\naug2 = RandomBrightnessContrast(brightness_limit=0.45, contrast_limit=0.45,p=1)\\nbatch_apply_augment(aug2,images)\\n\",\n",
       " 'c1918db4': \"'''สุ่มตัดรูปบางสว่นแล้ว resize กลับเป็นขนาดเดิม : RandomSizedCrop เป็นหนึ่งใน augmentation ที่ทรงพลังและทำให้โมเดลทำงานดีขึ้นมากๆ ในหลายปัญหาครับ'''\\n'''ในกรณีของดวงตา เราจะได้เห็นบาดแผลในสเกลที่หลากหลายกว่าเดิม มีขนาดใหญ่กว่าบาดแผลของรูปต้นฉบับ'''\\n\\nh_min=np.round(IMG_SIZE[1]*0.72).astype(int) # ระบุว่าจะ crop ไม่น้อยกว่า 72% ของรูปเดิม\\nh_max= np.round(IMG_SIZE[1]*0.9).astype(int)  # ระบุว่าจะ crop ไม่มากกว่า 90% ของรูปเดิม\\n\\naug3 = RandomSizedCrop((h_min, h_max),IMG_SIZE[1],IMG_SIZE[0], w2h_ratio=IMG_SIZE[0]/IMG_SIZE[1],p=1)\\n\\nbatch_apply_augment(aug3,images)\",\n",
       " 'd1e2af42': \"'''สุ่มลบบางส่วนของภาพออกไป : CutOut หรือ RandomEraser ก็เป็นหนึ่งใน augmentation ที่ดังมากและพบว่าทำให้ model มีความ robust มากขึ้นครับ'''\\n\\nmax_hole_size = int(IMG_SIZE[1]/10) # กำหนดขนาดของจุดที่เราจะลบออก\\n\\naug4 = Cutout(p=1,max_h_size=max_hole_size,max_w_size=max_hole_size,num_holes=8 )#num_holes=8 คือจะสุ่มลบ 8 ส่วน\\n\\nbatch_apply_augment(aug4,images)\",\n",
       " '4800dadf': '\\'\\'\\'สุ่มใส่แสงดวงอาทิตย์ ซึ่งเป็นการเลียนแสงธรรมชาติลงในภาพถ่าย RandomSunFlare : ซึ่งจะบังคับให้โมเดลเราต้องเรียนรู้ความแตกต่างระหว่าง \"จุดรอยแผล\" กับ \"จุดแสง\" ซึ่งเกิดขึ้นตามธรรมชาติในภาพถ่าย \\'\\'\\'\\naug_sun = RandomSunFlare(src_radius=max_hole_size,\\n                      num_flare_circles_lower=10,\\n                      num_flare_circles_upper=20,\\n                      p=1)#default flare_roi=(0,0,1,0.5),\\n\\n\\'\\'\\'ใน Albumentation เรายังนำ augment หลายๆ ตัวหรือตัวเดียวกันมาต่อกันหลายๆ รอบได้ เช่น ในตัวอย่างนี้เราจะใส่เส้นแสงดวงอาทิตย์ลงไป 3 เส้น \\'\\'\\'\\naug5 = Compose([aug_sun,aug_sun,aug_sun],p=1)\\nbatch_apply_augment(aug5,images)',\n",
       " '37613c0e': \"'''ผสานพลัง augmentation เป็นหนึ่งเดียว'''\\naug_ultimate = Compose([aug1,aug2,aug3,aug4,aug5],p=1)\\nbatch_apply_augment(aug_ultimate,images)\",\n",
       " 'e7520d65': '## 2.2 ทดลองใช้ Albumentations\\n\\nวิธีการใช้ `albumentations` นั้นง่ายมากครับ ก่อนอื่นเราสามารถไปดูรายชื่อ augmentation ที่ support ทั้งหมดได้ที่นี่\\nhttps://albumentations.readthedocs.io/en/latest/api/augmentations.html ตัวอย่างที่เราสามารถทำได้ เช่น การปรับสีสัน\\n\\n![ภาพจาก official github](https://camo.githubusercontent.com/fd2405ab170ab4739c029d7251f5f7b4fac3b41c/68747470733a2f2f686162726173746f726167652e6f72672f776562742f62642f6e652f72762f62646e6572763563746b75646d73617a6e687734637273646669772e6a706567)\\n\\nนอกจากนี้ก็จะมี operation อื่นๆ อีกมากมาย อาทิเช่น การบิดภาพของกระรอกในหัวข้อด้านบนครับ\\n\\n## 2.3 เลือก Augment อย่างไร??\\nและเราก็เลือก augmentation สำหรับปัญหา \"ตรวจสอบเบาหวานในดวงตา\" ที่บาดแผลต่างๆ ในตามีความสำคัญนั้น ทีมงานได้เลือก Augment Operation ที่เหมาะสมดังรายนามข้างล่าง สังเกตว่าเราใช้คำว่า \"เหมาะสม\" ก็เนื่องจากมันไม่ทำให้การประเมินของมนุษย์เปลี่ยนไปมากนักเมื่อดูภาพที่ถูกดัดแปลงนี้ (ดูภาพตัวอย่างในหัวข้อถัดไป) \\n\\n* `HorizontalFlip` คือการสะท้อนภาพแบบส่องกระจกในแนวนอน \\n* `VerticalFlip` คือการสะท้อนภาพแบบส่องกระจกในแนวตั้ง\\n* `Flip` คือการสุ่มสะท้อนภาพในแนวนอนหรือตั้ง\\n* `Rotate` คือการสุ่มหมุนภาพไปตามช่วงองศาที่กำหนด\\n* `RandomBrightnessContrast` คือการสุ่มปรับความสว่างของภาพ\\n* `RandomSizedCrop` คือการสุ่มตัดภาพมาบางส่วน แล้ว resize ไปให้ใหญ่เท่าเดิม\\n* `Cutout` คือการสุ่มเพิ่มจุดดำลงไปในภาพ\\n* `RandomSunFlare` คือการสุ่มเพิ่มแสงสะท้อนของดวงอาทิตย์ลงไปในภาพ\\n* นำทั้งหมดมา augment พร้อมๆ กัน (ultimate augment ขั้นสูงสุด :)\\n\\n### การเลือก Augmentation เป็นทั้งศาสตร์และศิลป์\\n\\nในการเลือก augmentation แต่ละครั้งต้องมีสาเหตุที่เหมาะสมเสมอครับ นั่นคือ **ต้องไม่ทำให้ธรรมชาติของข้อมูลเก่าเปลี่ยนไปมากนัก** ลองคิดดูครับในปัญหาดวงตานี้ ถ้าดวงตามีบาดแผล เรานำไปส่องกระจก หรือหมุน บาดแผลก็ยังคงอยู่เหมือนเดิม ภาพจะมืดหรือสว่างขึ้น(ในระดับที่เหมาะสม) ก็ไม่ทำให้บาดแผลเปลี่ยนไปเช่นกัน ในขณะที่การสุ่มตัดภาพหรือเพิ่มจุดดำที่ไม่มากจนเกินไปก็อาจทำให้มองภาพได้ยากขึ้นบ้าง แต่ส่วนใหญ่มนุษย์เรายังคงมองพวกบาดแผลต่างๆ ได้เช่นเดิมครับ\\n\\nการพิจารณา augmentation ที่เหมาะสมเป็นเรื่องละเอียดอ่อนครับ ในหลายๆ ครั้งการ augment ด้วยการ Flip หรือส่องกระจกมักจะไม่ทำให้ธรรมชาติข้อมูลเปลี่ยน แต่ทว่าในปัญหาบางประเภทเราอาจจะ Flip ไม่ได้ เช่น ปัญหาจดจำลายบนตัวสัตว์ ซึ่งสัตว์อาจจะมีบาดแผลเป็นอยู่ ถ้าบาดแผลอยู่ในทิศตรงกันข้าม (ซึ่งเกิดจาก Flip หรือส่องกระจก) อาจจะหมายถึง สัตว์คนละตัว ทั้งนี้เนื่องจากรูปในธรรมชาติจะมีภาพที่ส่องกระจกเกิดขึ้นเองไม่ได้ เป็นต้น \\n\\nในปัญหาบาดแผลบนดวงตานี้ เราควรหลีกเลี่ยงการ augment แบบ \"Blur\" เพราะจะทำให้แผลเล็กๆ เบลอหายไป หรือการใส่ \"Noise\" เพราะอาจทำให้มองภาพผิดว่าจุด noise ต่างๆ คือแผลเล็กๆ (รวมทั้งในข้อมูลต้นฉบับไม่มี noise เหล่านี้อยู่) เป็นต้น\\n',\n",
       " '6946d75b': 'การทำ augmentation อื่นๆ ก็ทำแบบเดียวกันเป้ะเลยครับ เรามาลองเล่นกันดูเลยครับ',\n",
       " '4e6deb8b': '# 3 ทำ Data Augmentation ด้วย Albumentations Library',\n",
       " '625581d8': 'สร้างฟังก์ชันสำหรับ crop รูปและโหลดรูปรวมทั้ง preprocess ตาม Workshop ในภาค 1',\n",
       " '80b827ef': 'วิธีการใช้งานง่ายมากครับ เราเพียงสร้างฟังก์ชันที่จะ apply augmentation ที่เราต้องการ โดยฟังก์ชันนี้ขอเรียกว่า `apply_augment` รับ input 2 ตัวแปรคือ รูปแบบ augment ที่เราเลือก `aug0`\\n(เช่น `Flip`) กับรูปภาพที่ต้องการ augment `img` ซึ่งทำได้บรรทัดเดียวจบ',\n",
       " '568f9b30': 'ทดลองโหลดรูปของดวงตาที่เป็นเบาหวานในขั้นรุนแรงที่สุดระดับ 4 เก็บไว้ 7 รูปเพื่อเป็นรูปต้นฉบับก่อนที่จะทำ Data Augmentation ครับ',\n",
       " '2ac76792': '## 2.1 เตรียมข้อมูลภาพ(ดวงตา) และฟังก์ชันที่จำเป็น\\n\\nเราจะนำวิธีการโหลดข้อมูล รวมทั้งการ Preprocessing มาจากภาค 1 เลยนะครับ เพื่อนสามารถทบทวน[ภาค 1 ได้ที่ลิงก์นี้](https://www.kaggle.com/ratthachat/workshop-ai-for-eyes-1)ครับ หรือเพื่อนๆ ที่ต้องการประยุกต์เทคนิกนี้กับภาพประเภทอื่นๆ ก็สามารถเตรียมข้อมูลได้ในลักษณะเดียวกันครับ',\n",
       " '251abd36': '## รวมพลัง Augmentations เป็นหนึ่ีงเดียว\\n\\nใน Albumentation เรายังนำ augment หลายๆ ตัวหรือตัวเดียวกันมาต่อกันหลายๆ รอบได้ง่ายๆ ด้วย `Compose` โดยเราเพียงนำ augmentations ที่เราต้องการมาใส่เรียงกันใน list เท่านั้น',\n",
       " 'a4c2e4d4': 'นอกจากนี้เราก็โหลด libraries มาตรฐานอื่นๆ มาไว้พร้อมกันตรงนี้ครับ',\n",
       " '501fd057': 'เป็นอย่างไรครับเราก็ได้ภาพใหม่ที่ดูยากขึ้น แต่ยังคงนัยสำคัญของข้อมูลไว้ในระดับที่น่าพอใจ (หรือเพื่อนๆ ปรับให้ตรงตามความต้องการได้ัเอง)\\n\\nนอกจากลูกเล่นต่างๆ แล้ว Albumentations ยังอนุญาตให้ผู้ใช้นำหลายๆ augmentation มารวมกันแบบสุ่มด้วยความน่าจะเป็นไม่เท่ากันได้อีกด้วย ซึ่ง Advanced options เหล่านี้เพื่อนๆ สามารถศึกษาได้จาก official document หรือถ้าเพื่อนๆ มีข้อสงสัยใดๆ ก็สามารถถามใน comments ด้านล่าง หรือใน https://www.thaikeras.com/community ได้เลยนะคร้าบ\\n\\nเรียนรู้ไปด้วยกันใหม่สัปดาห์หน้าคร้าบบบ :)',\n",
       " '4ac69404': '# 2. เตรียมข้อมูล ฟังก์ชันที่จำเป็น และทำความรู้จัก Augmentation Operations เบื้องต้น\\n\\nในปัจจุบันนั้นมี Library ที่ช่วยทำ Data Augmentation บน Python / Numpy / Keras อยู่มากมายหลายแห่ง โดย library ที่เราเลือกมาให้เพื่อนๆ นั้นมีชื่อว่า **Albumentations** เป็น Library ที่ได้ชื่อว่าทำ augmentation ได้รวดเร็วที่สุดและมีความหลากหลายครบทวน ซึ่งทีมงาน Albumentation นั้นก็เป็นเหล่าผู้เชี่ยวชาญบน Kaggle นั้นเองครับ โดยบน Kaggle นั้นเพื่อนๆ สามารถ import albumentations มาได้ทันทีครับ\\n\\nเพื่อนๆ ที่สนใจสามารถดู [Paper Albumentations บน ArXiv ได้ที่นี่ครับ](https://arxiv.org/abs/1809.06839)',\n",
       " '46da7795': 'สวัสดีครับ เพื่อนๆ สำหรับ Deep Learning Workshop อันดับ 7 ของ **[ThAIKeras](http://www.thaikeras.com)** นี้ เราจะว่ากันด้วยหนึ่งในเรื่องที่สำคัญสุดๆ สำหรับ Machine Learning บน Computer Vision นั่นคือเรื่องของ **Data Augmentation หรือ การเพิ่มปริมาณข้อมูลภาพสอน (จากข้อมูลภาพเดิม)** \\n\\nเนื่องจากประสิทธิภาพความแม่นยำของโมเดลในตระกูล Deep Learning นั้นขึ้นกับปริมาณข้อมูลเป็นปัจจัยสำคัญอันดับหนึ่ง (เทียบเท่ากับปัจจัยสถาปัตยกรรมนิวรอนที่ยอดเยี่ยม) ดังนั้นเทคนิก Data Augmentation ที่เราจะทำกันใน Workshop นี้จึงมีประโยชน์มากๆ และเพิ่มความสามารถให้กับโมเดลเราได้อย่างมาก นอกจากนี้สามารถประยุกต์ใช้กับงานด้าน Computer Vision ได้แทบทุกประเภทเลยครับ\\n\\nในการ Augment ที่มีประสิทธิภาพทางปฏิบัตินั้น นอกจากจะต้องฝึกใช้ library tool ที่ดีเช่น Albumentations อย่างที่แนะนำใน Workshop นี้แล้ว การเลือก Augmentation ที่เหมาะสมยังเป็นทั้งศาสตร์และศิลป์ที่เราต้องเจาะปัญหาให้เข้าใจลึกซึ้งอีกด้วย ซึ่งเราก็จะพูดถึงเรื่องเหล่านี้ใน Workshop นี้เช่นกันครับ\\n\\nโดย Workshop นี้แม้ัจะเป็นภาคต่อที่สองของปัญหา **การตรวจสอบเบาหวานในดวงตา (AI for Eyes ภาค 2) ที่ Kaggle ร่วมกับ Aravind Eye Hospital เพื่อแก้ปัญหาที่ทำให้ผู้คนตาบอดมากที่สุดอย่างยั่งยืน** ทว่าเพื่อนๆ ก็สามารถนำความรู้ใน Workshop นี้เข้าไปประยุกต์ได้กับงานอื่นๆ ใน Computer Vision หรือรูปภาพประเภทอื่นๆ ได้อย่างง่ายดายด้วยหลักการเดียวกัน\\n\\nเพื่อนๆ สามารถดู[ภาค 1 และที่มาของปัญหาได้ที่นี่ครับ](https://www.kaggle.com/ratthachat/workshop-ai-for-eyes-1) หรือ[สามารถดูและฝึกทำ AI/Data Science Workshops อื่นๆ ทั้งหมดของทีมงานได้ที่นี่ครับ](https://thaikeras.com/category/workshop/)\\n\\n# 1. Introduction : ว่าด้วยแก่นของ \"การเรียนรู้\"\\n\\nกระบวนการที่สำคัญของ Machine Learning คือ การสร้าง \"โมเดล\" จากข้อมูลสอน (Training Data) โดยโมเดลที่เรียนได้นั้น ก็เป็นตัวแทนของ \"ความรู้\" ที่ได้จาก Training Data นั่นเอง และหัวใจสำคัญของ \"ความรู้ที่แท้จริง\" นั้นคือ \"ต้องนำไปใช้ต่อได้ในอนาคต\" (ไม่ใช่สักแต่ท่อง :)\\n\\nตัวอย่างง่ายๆ ที่บ่งว่า**การ \"จดจำ\" Training Data ได้ 100% นั้นอาจไม่นับเป็นการเรียนรู้** อาทิ เช่น สมมติเราสร้างโมเดลให้เรียนรู้ว่าอะไรคือ \"กระรอก\" จากภาพตัวอย่างหลายภาพ โดยหนึ่งใน Training Data อาจเป็นภาพด้านล่าง (credit รูปภาพ: https://github.com/aleju/imgaug)\\n![](https://raw.githubusercontent.com/aleju/imgaug-doc/master/readme_images/small_overview/noop_image.jpg)\\n\\nเมื่อเราสอนโมเดลของเราด้วย Training Data และโมเดลนั้นแม่นยำ 100% บน Training Data นี้ เราอาจเข้าใจไปว่าโมเดลของเรารู้แล้วว่ากระรอกหน้าตาเป็นอย่างไร \\n\\nทว่าความจริงอาจไม่เป็นอย่างที่คิด โดยโมเดลของเราอาจจะ \"ลักไก่\" ด้วยการจำส่วนประกอบอื่นๆ ที่ไม่สำคัญบนภาพ หรือท่องจำแบบไม่เข้าใจอะไรก็เป็นได้ เช่น ถ้าเราซูมภาพนี้เข้ามาอีกนิด\\n\\n![](https://i.ibb.co/KVPvjRH/AI-for-Eyes-Spurious-Squrrel.png)\\n\\n* จำว่าถ้ามีวัตถุสามเหลี่ยมสีดำๆ อยู่เมื่อไร ภาพนั้นคือกระรอก\\n* จำว่าถ้าตรวจเจอแสงแดดกระทบขนสีขาวๆ ภาพนั้นคือกระรอก\\n* จำว่ากระรอกต้องมีดวงตา ปาก จมูกแบบที่เห็นในรูปเป้ะๆ (ถ้าหน้าตาผิดเพี้ยนไปแม้นแต่ pixel เดียว ก็ไม่ใช่กระรอก)\\n* และอื่นๆ อีกมากมายที่เป็นลักษณะท่องจำรายละเอียดในรูปตัวอย่างที่ไม่เกี่ยวกับ \"ความเป็นกระรอก\" จริงๆ\\n\\n## 1.2 ปัญหา Overfitting และทางแก้ด้วย Data Augmentation\\n\\nประเด็นข้างต้นนั้นเป็นตัวอย่างของปัญหา **overfitting** ที่เป็นประเด็นสำคัญที่สุด ใน Machine Learning ... **หนึ่งในวิธีการแก้ Overfitting แบบง่ายๆ แต่ได้ผล ก็คือ \"การเพิ่มจำนวน Training Data\" เข้าไป** เช่น เพิ่มรูปกระรอกอื่นๆ ไม่มีวัตถุสามเหลี่ยม ไม่มีแสงแดด หรือกระรอกหน้าตาอื่นๆ เหล่านี้จะทำให้โมเดลจดจำสิ่งที่ไม่จำเป็นได้ยากขึ้น และพยายามจดจำลักษณะอื่นๆ หรือเรียนรู้ \"ความรู้\" ที่เป็นจริงสำหรับกระรอกทุกภาพได้ดีขึ้น (เช่น \"รูปทรงของตา\" จะมีลักษณะทรงกลมหรือทรงรีสีดำ ขนาดใดก็ได้ เป็นต้น)\\n\\nอย่างไรก็ดีในกรณีที่เราไม่สามารถหา Training Data มาเพิ่มได้ เราอาจจะใช้เทคนิกที่สำคัญมากสำหรับ Machine Learning โดยเฉพาะ Computer Vision นั่นคือ **\"Data Augmentation\"** หรือ **การสร้างภาพใหม่ โดยการดัดแปลงภาพเดิมที่เรามี** อาทิเช่น เราอาจบิด ตัด หมุน เปลี่ยนสี ทำภาพให้มืดหรือสว่างขึ้น หรือใส่ noise ลงไปใน ถ้าเราลองทำกระบวนการเหล่านี้กับภาพข้างต้น จะได้ผลลัพธ์ทำนองนี้ (สังเกตว่ามนุษย์ยังมองออกว่าเป็นกระรอกเหมือนเดิม)\\n\\n![](https://imgaug.readthedocs.io/en/latest/_images/heavy.jpg)\\n\\nเห็นไหมครับ ไอเดียง่ายๆ นี้ทำให้เรามีภาพกระรอกเพิ่มขึ้นอย่างไม่จำกัดจำนวนเลยทีเดียว และทำให้โมเดลของเราลักไก่ยากขึ้นมากๆๆ เนื่องจากถ้าไปจำลักษณะอะไรที่ไม่ได้บ่งบอกถึงกระรอกจริงๆ ก็อาจจะไม่เป็นจริงอีกต่อไปใน Augmented Training Data (จากเดิม \"ลักไก่\" ง่ายกว่า \"เรียนความรู้\" พอเราเพิ่ม augmented data เข้าไปจะกลายเป็นว่าเรียนรู้จริงๆ ง่ายกว่าลักไก่) \\n\\nอย่างไรก็ตามการทำ Data Augmentation นั้นมีกุญแจสำคัญที่ต้องท่องจำขึ้นใจนั่นก็คือ **การทำ Augmentation นั้นจะต้องไม่เปลี่ยนแปลงธรรมชาติของข้อมูลต้นฉบับมากเกินไป** ตัวอย่างเช่น ภาพกระรอกข้างบน ถ้าเราไม่ระวังทำการเบลอภาพอย่างรุนแรงอาจทำให้เอกลักษณ์ของกระรอกเสียไป (มนุษย์มองออกยาก) และทำให้การเรียนรู้ด้อยประสิทธิภาพลงได้\\n\\nในทางปฏิบัตินั้นการทำ Data Augmentation ที่เหมาะสมกลายเป็นเครื่องมือมาตรฐานที่ช่วยเพิ่มความสามารถของโมเดลในงาน Computer Vision ได้อย่างมาก และในปัญหา **ตรวจสอบเบาหวานในดวงตา** ที่เรากำลังสนใจอยู่นี้ก็เช่นกันครับ เราจะมาเรียนรู้วิธีการทำ Data Augmentation ที่เหมาะสมต่างๆ กัน',\n",
       " 'fa858d63': 'เลือก operation ได้จาก github หรือจาก documents นี้ก็ได้ครับ https://albumentations.readthedocs.io/en/latest/api/augmentations.html\\n\\nในทุก Operation ของ albumentations จะอนุญาตให้เราระบุ \"ความน่าจะเป็น\" ที่จะ augment ได้ `Flip(p=0.8)` แปลว่าเราจะ flip รูปด้วยความน่าจะเป็น 80% นั่นเองครับ\\n\\nในตัวอย่างข้างล่างเราสั่งให้ Flip รูปที่เราเซพไว้ 7 รูปด้วยความน่าจะเป็น 80% ดังนั้นจะเห็นว่าบางรูปก็จะสลับข้าง บางรูปก็จะสลับบนล่าง หรือบางรูปก็จะเป็นรูปเดิมครับ (รันแต่ละครั้งจะได้ผลต่างกัน)',\n",
       " 'e8892a08': 'เพื่อความสะดวกเราจะเก็บชื่อไฟล์รูปไว้ในตัวแปร Numpy `x` และระดับความรุนแรงใน `y`',\n",
       " 'd4eaa1ce': 'from mpl_toolkits.mplot3d import Axes3D\\nfrom sklearn.preprocessing import StandardScaler\\nimport matplotlib.pyplot as plt # plotting\\nimport numpy as np # linear algebra\\nimport os # accessing directory structure\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n',\n",
       " '0bd60a54': \"print(os.listdir('../input'))\",\n",
       " 'fd83f613': \"# Distribution graphs (histogram/bar graph) of column data\\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\\n    nunique = df.nunique()\\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\\n    nRow, nCol = df.shape\\n    columnNames = list(df)\\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\\n    for i in range(min(nCol, nGraphShown)):\\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\\n        columnDf = df.iloc[:, i]\\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\\n            valueCounts = columnDf.value_counts()\\n            valueCounts.plot.bar()\\n        else:\\n            columnDf.hist()\\n        plt.ylabel('counts')\\n        plt.xticks(rotation = 90)\\n        plt.title(f'{columnNames[i]} (column {i})')\\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\\n    plt.show()\\n\",\n",
       " '40e3a551': \"# Correlation matrix\\ndef plotCorrelationMatrix(df, graphWidth):\\n    filename = df.dataframeName\\n    df = df.dropna('columns') # drop columns with NaN\\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\\n    if df.shape[1] < 2:\\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\\n        return\\n    corr = df.corr()\\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\\n    corrMat = plt.matshow(corr, fignum = 1)\\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\\n    plt.yticks(range(len(corr.columns)), corr.columns)\\n    plt.gca().xaxis.tick_bottom()\\n    plt.colorbar(corrMat)\\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\\n    plt.show()\\n\",\n",
       " '340f88a6': \"# Scatter and density plots\\ndef plotScatterMatrix(df, plotSize, textSize):\\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\\n    # Remove rows and columns that would lead to df being singular\\n    df = df.dropna('columns')\\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\\n    columnNames = list(df)\\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\\n        columnNames = columnNames[:10]\\n    df = df[columnNames]\\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\\n    corrs = df.corr().values\\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\\n    plt.suptitle('Scatter and Density Plot')\\n    plt.show()\\n\",\n",
       " '728af292': \"nRowsRead = 1000 # specify 'None' if want to read whole file\\n# googleplaystore_user_reviews.csv has 64295 rows in reality, but we are only loading/previewing the first 1000 rows\\ndf1 = pd.read_csv('../input/googleplaystore_user_reviews.csv', delimiter=',', nrows = nRowsRead)\\ndf1.dataframeName = 'googleplaystore_user_reviews.csv'\\nnRow, nCol = df1.shape\\nprint(f'There are {nRow} rows and {nCol} columns')\",\n",
       " 'c1511619': 'df1.head(5)',\n",
       " 'b62843e9': 'plotPerColumnDistribution(df1, 10, 5)',\n",
       " '390a703b': 'plotCorrelationMatrix(df1, 8)',\n",
       " 'b2cb25ad': 'plotScatterMatrix(df1, 6, 15)',\n",
       " '6ad474a3': \"nRowsRead = 1000 # specify 'None' if want to read whole file\\n# googleplaystore.csv has 10841 rows in reality, but we are only loading/previewing the first 1000 rows\\ndf2 = pd.read_csv('../input/googleplaystore.csv', delimiter=',', nrows = nRowsRead)\\ndf2.dataframeName = 'googleplaystore.csv'\\nnRow, nCol = df2.shape\\nprint(f'There are {nRow} rows and {nCol} columns')\",\n",
       " 'ad22c336': 'df2.head(5)',\n",
       " 'aa003a73': 'plotPerColumnDistribution(df2, 10, 5)',\n",
       " 'b80c1c38': 'plotCorrelationMatrix(df2, 8)',\n",
       " 'fb2cfdd7': 'plotScatterMatrix(df2, 6, 15)',\n",
       " '029575b9': 'Distribution graphs (histogram/bar graph) of sampled columns:',\n",
       " '8eef5c79': \"Now you're ready to read in the data and use the plotting functions to visualize the data.\",\n",
       " 'f05c7454': \"### Let's check 1st file: ../input/googleplaystore_user_reviews.csv\",\n",
       " '66802a75': 'Scatter and density plots:',\n",
       " 'f402e2c0': 'Correlation matrix:',\n",
       " 'dd44df15': 'Correlation matrix:',\n",
       " 'd32fbcbf': 'There are 2 csv files in the current version of the dataset:\\n',\n",
       " 'f481fecf': 'The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.',\n",
       " 'beb352ac': 'Distribution graphs (histogram/bar graph) of sampled columns:',\n",
       " 'e8dde324': \"Let's take a quick look at what the data looks like:\",\n",
       " '699874eb': \"Let's take a quick look at what the data looks like:\",\n",
       " '626f2a56': '## Introduction\\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue \"Edit Notebook\" or \"Fork Notebook\" button at the top of this kernel to begin editing.',\n",
       " '3b5af627': \"## Exploratory Analysis\\nTo begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)\",\n",
       " '03d74874': \"### Let's check 2nd file: ../input/googleplaystore.csv\",\n",
       " '57e4065d': 'Scatter and density plots:',\n",
       " '86ddc034': '## Conclusion\\nThis concludes your starter analysis! To go forward from here, click the blue \"Edit Notebook\" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!',\n",
       " '4045f388': \"DATA_PATH = '/kaggle/input/shakespeare-text/text.txt'\",\n",
       " 'be0291fd': 'import numpy as np\\nimport torch.nn as nn\\nimport torch\\nimport torchvision\\nfrom torchvision import transforms\\nfrom torch.nn import functional as F\\nfrom tqdm import tqdm_notebook\\nfrom collections import Counter ',\n",
       " '1b4d5006': \"fp = open(DATA_PATH,'r')\\ntxt = fp.read()\",\n",
       " '850ebac4': \"txt = txt.replace('$','')\\ntxt = txt.replace('&','')\",\n",
       " 'b2aa7391': \"print(f'total number of characters in corpus: {len(txt)}')\\nprint(f'total number of unique characters in corpus: {len(set(txt))}')\",\n",
       " '2c8e5cf4': 'unique_txt = set(txt)',\n",
       " '719b3d31': 'character_count = Counter(txt)\\n#character_count',\n",
       " 'a577d691': 'char2idx = {char: key for key,char in enumerate(sorted(unique_txt))}\\n\\nidx2char = [c for c in sorted(unique_txt)]',\n",
       " '1bae3f1f': 'batch_size = 64\\nseq_size = 100\\ncoded_text = [char2idx[c] for c in txt]\\nn_vocab = len(unique_txt)',\n",
       " '759deaee': \"def batch_generate(text,batch_size= batch_size,seq_size = seq_size):\\n    #print(f'vocab_size: {len(char2idx)}')    \\n    total_batches = int(len(coded_text)/(batch_size*seq_size))\\n    input_txt = text[:total_batches*batch_size*seq_size]\\n    output_txt = np.zeros_like(input_txt)\\n    output_txt[:-1] = input_txt[1:]\\n    output_txt[-1] = input_txt[0]\\n    input_txt = np.reshape(input_txt,(batch_size,-1))\\n    output_txt = np.reshape(output_txt,(batch_size,-1))\\n    return input_txt,output_txt\",\n",
       " '23be602f': 'def get_batches(data,target,batch_size,seq_size):\\n    num_batches = np.prod(data.shape) // (seq_size * batch_size)\\n    for i in range(0, num_batches * seq_size, seq_size):\\n        yield data[:, i:i+seq_size], target[:, i:i+seq_size]',\n",
       " '5828e58b': 'class RNNModule(nn.Module):\\n    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\\n        super(RNNModule, self).__init__()\\n        self.seq_size = seq_size\\n        self.lstm_size = lstm_size\\n        self.embedding = nn.Embedding(n_vocab, embedding_size)\\n        self.lstm = nn.LSTM(embedding_size,\\n                            lstm_size,\\n                            batch_first=True)\\n        self.dense = nn.Linear(lstm_size, n_vocab)\\n        \\n    def forward(self, x, prev_state):\\n        embed = self.embedding(x)\\n        output, state = self.lstm(embed, prev_state)\\n        logits = self.dense(output)\\n\\n        return logits, state\\n    \\n    def zero_state(self, batch_size):\\n        return (torch.zeros(1, batch_size, self.lstm_size),\\n                torch.zeros(1, batch_size, self.lstm_size))',\n",
       " 'fcbb1214': 'def get_loss_and_train_op(net, lr=0.001):\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\\n    return criterion, optimizer',\n",
       " '6310690b': \"def main():\\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n    net = RNNModule(n_vocab,seq_size,embedding_size=14,lstm_size=128).to(device)\\n    criterion, optimizer = get_loss_and_train_op(net, 0.01)\\n    iteration = 0\\n    data,target = batch_generate(coded_text)\\n    for e in tqdm_notebook(range(10)):\\n        batches = get_batches(data,target,batch_size,seq_size)\\n        state_h,state_c = net.zero_state(batch_size)\\n        state_h = state_h.to(device)\\n        state_c = state_c.to(device)\\n        for x,y in batches:\\n            iteration+=1\\n            net.train()\\n            optimizer.zero_grad()\\n            x = torch.tensor(x).to(device)\\n            y = torch.tensor(y).to(device)\\n            logits, (state_h, state_c) = net(x, (state_h, state_c))\\n            loss = criterion(logits.transpose(1, 2), y)\\n            state_h = state_h.detach()\\n            state_c = state_c.detach()\\n            loss_value = loss.item()\\n            loss.backward()\\n            _ = torch.nn.utils.clip_grad_norm_(\\n                net.parameters(), 5)\\n            optimizer.step()\\n            \\n            if iteration % 100 == 0:\\n                print('Epoch: {}/{}'.format(e, 200),\\n                      'Iteration: {}'.format(iteration),\\n                      'Loss: {}'.format(loss_value))\\n                \\n            if iteration%2000 ==0:\\n                predict(device,net,['I',' ','a','m',' '],n_vocab,char2idx,idx2char)\\n                torch.save(net.state_dict(),\\n                           'model-{}.pth'.format(iteration))\",\n",
       " '54ab4732': \"def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\\n    net.eval()\\n    state_h, state_c = net.zero_state(1)\\n    state_h = state_h.to(device)\\n    state_c = state_c.to(device)\\n    for w in words:\\n        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\\n        output, (state_h, state_c) = net(ix, (state_h, state_c))\\n    \\n    _, top_ix = torch.topk(output[0], k=top_k)\\n    choices = top_ix.tolist()\\n    choice = np.random.choice(choices[0])\\n\\n    words.append(int_to_vocab[choice])\\n    \\n    for _ in range(100):\\n        ix = torch.tensor([[choice]]).to(device)\\n        output, (state_h, state_c) = net(ix, (state_h, state_c))\\n\\n        _, top_ix = torch.topk(output[0], k=top_k)\\n        choices = top_ix.tolist()\\n        choice = np.random.choice(choices[0])\\n        words.append(int_to_vocab[choice])\\n\\n    print(''.join(words))\",\n",
       " '626e9aa7': 'main()',\n",
       " 'c1e2f0c4': '### Character Level Text Generation\\n### Please UPVOTE if you like this kernel',\n",
       " '5151923d': '%matplotlib inline\\nimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport os',\n",
       " '9c36a1b0': \"# params we will probably want to do some hyperparameter optimization later\\nBASE_MODEL= 'InceptionV3'\\nIMG_SIZE = (224, 224) # [(224, 224), (384, 384), (512, 512), (640, 640)]\\nBATCH_SIZE = 24 # [1, 8, 16, 24]\\nDENSE_COUNT = 128 # [32, 64, 128, 256]\\nDROPOUT = 0.5 # [0, 0.25, 0.5]\\nLEARN_RATE = 1e-4 # [1e-4, 1e-3, 4e-3]\\nTRAIN_SAMPLES = 6000 # [3000, 6000, 15000]\\nTEST_SAMPLES = 600\\nUSE_ATTN = False # [True, False]\",\n",
       " '5a3449b5': \"image_bbox_df = pd.read_csv('../input/lung-opacity-overview/image_bbox_full.csv')\\nimage_bbox_df['path'] = image_bbox_df['path'].map(lambda x: \\n                                                  x.replace('input', \\n                                                            'input/rsna-pneumonia-detection-challenge'))\\nprint(image_bbox_df.shape[0], 'images')\\nimage_bbox_df.sample(3)\",\n",
       " '9da3e24b': \"# get the labels in the right format\\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\\nclass_enc = LabelEncoder()\\nimage_bbox_df['class_idx'] = class_enc.fit_transform(image_bbox_df['class'])\\noh_enc = OneHotEncoder(sparse=False)\\nimage_bbox_df['class_vec'] = oh_enc.fit_transform(\\n    image_bbox_df['class_idx'].values.reshape(-1, 1)).tolist() \\nimage_bbox_df.sample(3)\",\n",
       " '62018c21': \"from sklearn.model_selection import train_test_split\\nimage_df = image_bbox_df.groupby('patientId').apply(lambda x: x.sample(1))\\nraw_train_df, valid_df = train_test_split(image_df, test_size=0.25, random_state=2018,\\n                                    stratify=image_df['class'])\\nprint(raw_train_df.shape, 'training data')\\nprint(valid_df.shape, 'validation data')\",\n",
       " '4038e62d': \"fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\\nraw_train_df.groupby('class').size().plot.bar(ax=ax1)\\ntrain_df = raw_train_df.groupby('class').\\\\\\n    apply(lambda x: x.sample(TRAIN_SAMPLES//3)).\\\\\\n    reset_index(drop=True)\\ntrain_df.groupby('class').size().plot.bar(ax=ax2) \\nprint(train_df.shape[0], 'new training size')\",\n",
       " 'b166e441': \"import keras.preprocessing.image as KPImage\\nfrom PIL import Image\\nimport pydicom\\ndef read_dicom_image(in_path):\\n    img_arr = pydicom.read_file(in_path).pixel_array\\n    return img_arr/img_arr.max()\\n    \\nclass medical_pil():\\n    @staticmethod\\n    def open(in_path):\\n        if '.dcm' in in_path:\\n            c_slice = read_dicom_image(in_path)\\n            int_slice =  (255*c_slice).clip(0, 255).astype(np.uint8) # 8bit images are more friendly\\n            return Image.fromarray(int_slice)\\n        else:\\n            return Image.open(in_path)\\n    fromarray = Image.fromarray\\nKPImage.pil_image = medical_pil\",\n",
       " '2886a196': \"try:\\n    # keras 2.2\\n    from keras_preprocessing.image import ImageDataGenerator\\nexcept:\\n    from keras.preprocessing.image import ImageDataGenerator\\nif BASE_MODEL=='VGG16':\\n    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\\nelif BASE_MODEL=='RESNET52':\\n    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\\nelif BASE_MODEL=='InceptionV3':\\n    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\\nelif BASE_MODEL=='Xception':\\n    from keras.applications.xception import Xception as PTModel, preprocess_input\\nelif BASE_MODEL=='DenseNet169': \\n    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\\nelif BASE_MODEL=='DenseNet121':\\n    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\\nelse:\\n    raise ValueError('Unknown model: {}'.format(BASE_MODEL))\",\n",
       " '290e7ade': \"img_gen_args = dict(samplewise_center=False, \\n                              samplewise_std_normalization=False, \\n                              horizontal_flip = True, \\n                              vertical_flip = False, \\n                              height_shift_range = 0.05, \\n                              width_shift_range = 0.02, \\n                              rotation_range = 3, \\n                              shear_range = 0.01,\\n                              fill_mode = 'nearest',\\n                              zoom_range = 0.05,\\n                               preprocessing_function=preprocess_input)\\nimg_gen = ImageDataGenerator(**img_gen_args)\",\n",
       " '342dbb27': \"def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, seed = None, **dflow_args):\\n    base_dir = os.path.dirname(in_df[path_col].values[0])\\n    print('## Ignore next message from keras, values are replaced anyways: seed: {}'.format(seed))\\n    df_gen = img_data_gen.flow_from_directory(base_dir, \\n                                     class_mode = 'sparse',\\n                                              seed = seed,\\n                                    **dflow_args)\\n    df_gen.filenames = in_df[path_col].values\\n    df_gen.classes = np.stack(in_df[y_col].values,0)\\n    df_gen.samples = in_df.shape[0]\\n    df_gen.n = in_df.shape[0]\\n    df_gen._set_index_array()\\n    df_gen.directory = '' # since we have the full path\\n    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\\n    return df_gen\",\n",
       " '0b45ac10': \"train_gen = flow_from_dataframe(img_gen, train_df, \\n                             path_col = 'path',\\n                            y_col = 'class_vec', \\n                            target_size = IMG_SIZE,\\n                             color_mode = 'rgb',\\n                            batch_size = BATCH_SIZE)\\n\\nvalid_gen = flow_from_dataframe(img_gen, valid_df, \\n                             path_col = 'path',\\n                            y_col = 'class_vec', \\n                            target_size = IMG_SIZE,\\n                             color_mode = 'rgb',\\n                            batch_size = 256) # we can use much larger batches for evaluation\\n# used a fixed dataset for evaluating the algorithm\\nvalid_X, valid_Y = next(flow_from_dataframe(img_gen, \\n                               valid_df, \\n                             path_col = 'path',\\n                            y_col = 'class_vec', \\n                            target_size = IMG_SIZE,\\n                             color_mode = 'rgb',\\n                            batch_size = TEST_SAMPLES)) # one big batch\",\n",
       " '74d8a833': 't_x, t_y = next(train_gen)',\n",
       " '4a17dd4b': \"base_pretrained_model = PTModel(input_shape =  t_x.shape[1:], \\n                              include_top = False, weights = 'imagenet')\\nbase_pretrained_model.trainable = False\",\n",
       " '257d8025': \"from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda, AvgPool2D\\nfrom keras.models import Model\\nfrom keras.optimizers import Adam\\npt_features = Input(base_pretrained_model.get_output_shape_at(0)[1:], name = 'feature_input')\\npt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\\nfrom keras.layers import BatchNormalization\\nbn_features = BatchNormalization()(pt_features)\\ngap = GlobalAveragePooling2D()(bn_features)\\n\\ngap_dr = Dropout(DROPOUT)(gap)\\ndr_steps = Dropout(DROPOUT)(Dense(DENSE_COUNT, activation = 'elu')(gap_dr))\\nout_layer = Dense(t_y.shape[1], activation = 'softmax')(dr_steps)\\n\\nattn_model = Model(inputs = [pt_features], \\n                   outputs = [out_layer], name = 'trained_model')\\n\\nattn_model.summary()\",\n",
       " '39d38743': \"from keras.models import Sequential\\nfrom keras.optimizers import Adam\\npneu_model = Sequential(name = 'combined_model')\\nbase_pretrained_model.trainable = False\\npneu_model.add(base_pretrained_model)\\npneu_model.add(attn_model)\\npneu_model.compile(optimizer = Adam(lr = LEARN_RATE), loss = 'categorical_crossentropy',\\n                           metrics = ['categorical_accuracy'])\\npneu_model.summary()\",\n",
       " '1c77fd52': 'from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\\nweight_path=\"{}_weights.best.hdf5\".format(\\'lung_opacity\\')\\n\\ncheckpoint = ModelCheckpoint(weight_path, monitor=\\'val_loss\\', verbose=1, \\n                             save_best_only=True, mode=\\'min\\', save_weights_only = True)\\n\\nreduceLROnPlat = ReduceLROnPlateau(monitor=\\'val_loss\\', factor=0.8, \\n                                   patience=10, verbose=1, mode=\\'auto\\', \\n                                   epsilon=0.0001, cooldown=5, min_lr=0.0001)\\nearly = EarlyStopping(monitor=\"val_loss\", \\n                      mode=\"min\", \\n                      patience=10) # probably needs to be more patient, but kaggle time is limited\\ncallbacks_list = [checkpoint, early, reduceLROnPlat]',\n",
       " '9b4a0a28': 'train_gen.batch_size = BATCH_SIZE\\npneu_model.fit_generator(train_gen, \\n                         validation_data = (valid_X, valid_Y), \\n                         epochs=20, \\n                         callbacks=callbacks_list,\\n                         workers=2)',\n",
       " '9ada0175': \"pneu_model.load_weights(weight_path)\\npneu_model.save('full_model.h5')\",\n",
       " '0ce1e568': 'pred_Y = pneu_model.predict(valid_X, \\n                          batch_size = BATCH_SIZE, \\n                          verbose = True)',\n",
       " 'fea21eb2': 'from sklearn.metrics import classification_report, confusion_matrix\\nplt.matshow(confusion_matrix(np.argmax(valid_Y, -1), np.argmax(pred_Y,-1)))\\nprint(classification_report(np.argmax(valid_Y, -1), \\n                            np.argmax(pred_Y,-1), target_names = class_enc.classes_))',\n",
       " '2d7ba0c7': \"from sklearn.metrics import roc_curve, roc_auc_score\\nfpr, tpr, _ = roc_curve(np.argmax(valid_Y,-1)==0, pred_Y[:,0])\\nfig, ax1 = plt.subplots(1,1, figsize = (5, 5), dpi = 250)\\nax1.plot(fpr, tpr, 'b.-', label = 'VGG-Model (AUC:%2.2f)' % roc_auc_score(np.argmax(valid_Y,-1)==0, pred_Y[:,0]))\\nax1.plot(fpr, fpr, 'k-', label = 'Random Guessing')\\nax1.legend(loc = 4)\\nax1.set_xlabel('False Positive Rate')\\nax1.set_ylabel('True Positive Rate');\\nax1.set_title('Lung Opacity ROC Curve')\\nfig.savefig('roc_valid.pdf')\",\n",
       " '54667867': \"from glob import glob\\nsub_img_df = pd.DataFrame({'path': \\n              glob('../input/rsna-pneumonia-detection-challenge/stage_1_test_images/*.dcm')})\\nsub_img_df['patientId'] = sub_img_df['path'].map(lambda x: os.path.splitext(os.path.basename(x))[0])\\nsub_img_df.sample(3)\",\n",
       " '0a4d562e': \"submission_gen = flow_from_dataframe(img_gen, \\n                                     sub_img_df, \\n                             path_col = 'path',\\n                            y_col = 'patientId', \\n                            target_size = IMG_SIZE,\\n                             color_mode = 'rgb',\\n                            batch_size = BATCH_SIZE,\\n                                    shuffle=False)\",\n",
       " '5150b310': 'from tqdm import tqdm\\nsub_steps = 2*sub_img_df.shape[0]//BATCH_SIZE\\nout_ids, out_vec = [], []\\nfor _, (t_x, t_y) in zip(tqdm(range(sub_steps)), submission_gen):\\n    out_vec += [pneu_model.predict(t_x)]\\n    out_ids += [t_y]\\nout_vec = np.concatenate(out_vec, 0)\\nout_ids = np.concatenate(out_ids, 0)',\n",
       " '3557048c': \"pred_df = pd.DataFrame(out_vec, columns=class_enc.classes_)\\npred_df['patientId'] = out_ids\\npred_avg_df = pred_df.groupby('patientId').agg('mean').reset_index()\\npred_avg_df['Lung Opacity'].hist()\\npred_avg_df.sample(2)\",\n",
       " 'c71b4495': \"pred_avg_df['PredictionString'] = pred_avg_df['Lung Opacity'].map(lambda x: '%2.2f 0 0 1024 1024' % x)\",\n",
       " 'd5a884e1': \"pred_avg_df[['patientId', 'PredictionString']].to_csv('submission.csv', index=False)\",\n",
       " 'd4e1a9cf': '## Balance Training Set\\nAnd reduce the total image count',\n",
       " '99da21f8': '### Simple Strategy\\nWe use the `Lung Opacity` as our confidence and predict the image image. It will hopefully do a little bit better than a trivial baseline, and can be massively improved.',\n",
       " '7437fe2d': '## Model Supplements\\nHere we add a few other layers to the model to make it better suited for the classification problem. ',\n",
       " '3c84f64e': '# Make a submission\\nWe load in the test images and make a submission using those images and a guess for $x, y$ and the width and height',\n",
       " '04705bb9': \"## Predict for each image twice and average the results\\nWe shouldn't get the same answer since the data are being augmented (here at so-called test-time)\",\n",
       " '14c8258e': '# Data Augmentation\\nHere we can perform simple augmentation (the `imgaug` and `Augmentation` packages offer much more flexiblity). In order to setup the augmentation we need to know which model we are using',\n",
       " 'dbeea067': 'Reference :\\n\\nhttps://www.kaggle.com/kmader/lung-opacity-classification-transfer-learning\\n\\n\\nI just used inceptionv3 instead of vgg16',\n",
       " '4ef2aa43': '## Keras Image Transplantation\\nSince Keras is design for color jpeg images we need to hack a bit to make it dicom friendly',\n",
       " 'fd2cb432': '# Split into Training and Validation\\nThis will give us some feedback on how well our model is doing and if we are overfitting',\n",
       " '03e11b5a': '# Build our pretrained model\\nHere we build the pretrained model and download the weights',\n",
       " '5572345c': \"import numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\nfrom sklearn.model_selection import train_test_split\\n\\n# Read the data\\nX = pd.read_csv('../input/udemy-courses/udemy_courses.csv', index_col='course_id')\",\n",
       " '749fe3f0': 'import time\\nimport datetime\\n\\nX = pd.read_csv(\\'../input/udemy-courses/udemy_courses.csv\\', index_col=\\'course_id\\')\\n\\n# helper functions\\ndef string_to_datetime(date_string):\\n    return datetime.datetime.strptime(date_string,\"%Y-%m-%dT%H:%M:%SZ\")\\n\\ndef elapsed_time(time_from, time_to):\\n    return string_to_datetime(time_from) - string_to_datetime(time_to)\\n\\n# to calculate time on the platform I use the dataset\\'s date of upload\\n# for simplicity, I rounded the time to years\\nfn = lambda row: elapsed_time(\"2020-05-10T00:00:00Z\", row.published_timestamp).days/365\\ncol = X.apply(fn, axis=1)\\nX = X.assign(time_on_platform=col.values)\\n\\nX[\"time_on_platform\"].describe()',\n",
       " 'e7589012': \"import matplotlib.pyplot as plt\\nimport matplotlib\\nimport seaborn as sns\\n\\npp = sns.pairplot(data=X,\\n                  y_vars=['num_subscribers'],\\n                  x_vars=['time_on_platform', 'price', 'num_lectures', 'content_duration', 'subject'])\\n\",\n",
       " '5470adae': \"X = X.sort_values(by=['num_subscribers'], ascending=False)\\nX\",\n",
       " 'd4000564': '# get top 10% of the set\\n# df = X.head(int(len(X)*(10/100)))\\n\\n# X = X.sort_values(by=[\\'time_on_platform\\'], ascending=False)\\n# df = X.head(int(len(X)*(15/100)))\\n\\ndf0 = X[X[\\'time_on_platform\\'].between(X[\\'time_on_platform\\'].min(), 4)]\\ndf1 = X[X[\\'time_on_platform\\'].between(4, 5)]\\ndf2 = X[X[\\'time_on_platform\\'].between(5, 6)]\\ndf3 = X[X[\\'time_on_platform\\'].between(6, 7)]\\ndf4 = X[X[\\'time_on_platform\\'].between(7,8)]\\ndf = X[X[\\'time_on_platform\\'] >= 8]\\n\\nplt.plot(df0[\"time_on_platform\"], df0[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\\nplt.plot(df1[\"time_on_platform\"], df1[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\\nplt.plot(df2[\"time_on_platform\"], df2[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\\nplt.plot(df3[\"time_on_platform\"], df3[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\\nplt.plot(df4[\"time_on_platform\"], df4[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\\nplt.plot(df[\"time_on_platform\"], df[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\\nplt.xlabel(\"time_on_platform\")\\nplt.ylabel(\"num_subscribers\")\\n',\n",
       " '677e5b80': 'df0 = X[X[\\'time_on_platform\\'].between(X[\\'time_on_platform\\'].min(), 4)]\\ndf1 = X[X[\\'time_on_platform\\'].between(4, 6)]\\ndf2 = X[X[\\'time_on_platform\\'].between(6, 7)]\\ndf = X[X[\\'time_on_platform\\'] >= 7]\\n\\n# draw all subsets side by side\\nfig, axs = plt.subplots(2, 3)\\nfig.set_size_inches(18.5, 10.5)\\n\\ndataframes = [df0, df1, df2, df]\\ndf_to_draw_index = 0;\\nfor r in range(2):\\n    for c in range(2):\\n        d = dataframes[df_to_draw_index]\\n        axs[r, c].plot(d[\"time_on_platform\"], d[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\\n        axs[r, c].set_title(\\'Df \\' + str(df_to_draw_index))\\n        df_to_draw_index += 1\\n        \\nfor ax in axs.flat:\\n    ax.set(xlabel=\\'years_on_platform\\', ylabel=\\'num_subscribers\\')',\n",
       " '6461987d': 'import seaborn as sns\\n\\nX[\"published_timestamp\"]=pd.to_datetime(X[\"published_timestamp\"])\\nX[\"Year\"]=X[\"published_timestamp\"].dt.year\\n\\nyear = X.groupby([\"Year\"])[\"course_title\"].count()\\nyear = pd.DataFrame(year)\\nyear.rename(columns = {\"course_title\":\"Total_Courses\"},inplace = True)\\nyear.reset_index(inplace=True)\\n\\nX[\"Month\"]=X[\"published_timestamp\"].dt.month\\nmonth = X.groupby([\"Month\"])[\"course_title\"].count()\\nmonth = pd.DataFrame(month)\\nmonth.rename(columns = {\"course_title\":\"Total_Courses\"},inplace = True)\\nmonth.reset_index(inplace=True)\\n\\nsns.barplot(x=\"Year\",y=\"Total_Courses\",data = year)\\nplt.show()\\nsns.barplot(x=\"Month\",y=\"Total_Courses\",data = month)\\nplt.show()\\nX.columns\\n\\ndf0 = X[X[\\'time_on_platform\\'].between(X[\\'time_on_platform\\'].min(), 4)]\\ndf1 = X[X[\\'time_on_platform\\'].between(4, 6)]\\ndf2 = X[X[\\'time_on_platform\\'].between(6, 7)]\\ndf = X[X[\\'time_on_platform\\'] >= 7]\\n',\n",
       " '421c60e2': '# fn = lambda row: string_to_datetime(row.published_timestamp).month\\n# col = X.apply(fn, axis=1)\\n# X = X.assign(published_month=col.values)\\n\\n# X[\"published_month\"].describe()',\n",
       " 'd89471db': \"categorical_columns = ['course_title', 'level', 'subject']\\nnumerical_columns = ['price', 'num_lectures', 'content_duration', 'time_on_platform', 'Month']\\nfeatures = categorical_columns + numerical_columns\\n\\n# Select target value\\ny = X.num_subscribers\\nX_full = X[features]\\n# Break off test set from training data\\nX_train, X_test, y_train, y_test = train_test_split(X_full, y, train_size=0.8, test_size=0.2, random_state=0)\\n\\ny.describe()\",\n",
       " '241327e9': 'from sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import mean_absolute_error\\n\\n# no missing values in any of the columns, so no need for imputing\\n\\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\\nnumerical_transformer = StandardScaler()\\n\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\"categorical\", categorical_transformer, categorical_columns),\\n        (\"numerical\", numerical_transformer, numerical_columns),\\n    ]\\n)',\n",
       " 'f2fe6a17': 'decission_tree = Pipeline(\\n    steps=[(\"preprocessor\", preprocessor), (\"regressor\", DecisionTreeRegressor())]\\n)\\n\\ndecission_tree.fit(X_train, y_train)\\n\\npredictions_tree = decission_tree.predict(X_test)\\nmae = mean_absolute_error(y_test, predictions_tree)\\n\\nprint(\"decission tree mae: \" + str(mae))',\n",
       " 'f6d7a421': 'from sklearn.linear_model import LinearRegression\\n\\nlinear = Pipeline(\\n    steps=[(\"preprocessor\", preprocessor), (\"linear\", LinearRegression())]\\n)\\n\\nlinear.fit(X_train, y_train)\\n\\nlinear_pred = linear.predict(X_test)\\nmae = mean_absolute_error(y_test, linear_pred)\\n\\nprint(\"Linear Regression mae: \" + str(mae))',\n",
       " 'e78c064c': 'from sklearn import svm\\n\\nsvr = Pipeline(\\n    steps=[(\"preprocessor\", preprocessor), (\"svr\", svm.SVR())]\\n)\\n\\nsvr.fit(X_train, y_train)\\n\\nsvr_pred = svr.predict(X_test)\\nmae = mean_absolute_error(y_test, svr_pred)\\n\\nprint(\"SVR mae: \" + str(mae))',\n",
       " '45777470': 'from sklearn.base import clone\\nfrom sklearn.base import BaseEstimator\\n\\n\\nclass OrdinalClassifier():\\n    \\n    def __init__(self, clf):\\n        self.clf = clf\\n        self.clfs = {}\\n    \\n    def fit(self, X, y):\\n        self.unique_class = np.sort(np.unique(y))\\n        if self.unique_class.shape[0] > 2:\\n            for i in range(self.unique_class.shape[0]-1):\\n                # for each k - 1 ordinal value we fit a binary classification problem\\n                binary_y = (y > self.unique_class[i]).astype(np.uint8)\\n                clf = clone(self.clf)\\n                clf.fit(X, binary_y)\\n                self.clfs[i] = clf\\n    \\n    def predict_proba(self, X):\\n        clfs_predict = {k:self.clfs[k].predict_proba(X) for k in self.clfs}\\n        predicted = []\\n        for i,y in enumerate(self.unique_class):\\n            if i == 0:\\n                # V1 = 1 - Pr(y > V1)\\n                predicted.append(1 - clfs_predict[y][:,1])\\n                \\n            elif y in clfs_predict:\\n                # Vi = Pr(y > Vi-1) - Pr(y > Vi)\\n                 predicted.append(clfs_predict[y-1][:,1] - clfs_predict[y][:,1])\\n            else:\\n                # Vk = Pr(y > Vk-1)\\n                predicted.append(clfs_predict[y-1][:,1])\\n        return np.vstack(predicted).T\\n    \\n    def predict(self, X):\\n        return np.argmax(self.predict_proba(X), axis=1)',\n",
       " '5e903465': \"categorical_columns = ['level', 'subject'] #'course_title'\\nnumerical_columns = ['price', 'num_lectures', 'content_duration', 'time_on_platform', 'Month']\\nfeatures = numerical_columns + categorical_columns\\n\\ndef classify(value):\\n    if 0 <= value <= 100: return 0\\n    elif 100 <= value <= 1000: return 1\\n    elif 1000 <= value <= 2500: return 2\\n#     elif 2000 <= value <= 5000: return 3\\n#     elif 5000 <= value <= 10000: return 4\\n    elif value > 2500: return 3\\n\\nfn = lambda row: classify(row.num_subscribers)\\ncol = X.apply(fn, axis=1)\\nX = X.assign(sub_class=col.values)\\n# Select target value\\ny = X.sub_class\\nX_full = X[features]\\n# Break off test set from training data\\nX_train, X_test, y_train, y_test = train_test_split(X_full, y, train_size=0.8, test_size=0.2, random_state=0)\\n\",\n",
       " '546b3a54': 'data = X.sub_class.value_counts().reset_index()\\nsns.barplot(x=\"index\", y=\"sub_class\", data = data)',\n",
       " 'f0d17f79': '# from sklearn.svm import SVC\\n# from sklearn.tree import DecisionTreeClassifier\\n\\n# clf = Pipeline(\\n#     steps=[(\"preprocessor\", preprocessor), (\"clf\", OrdinalClassifier(DecisionTreeClassifier(max_depth=3)))]\\n# )\\n\\n# clf.fit(X_train, y_train)\\n\\n# oc_pred = clf.predict(X_test)\\n\\n# print(oc_pred)\\n\\n# # print(df0.sub_class)\\n# # count = 0\\n# # for i in range(290):\\n# # #     print(\"true: \" + str(y_test[i]) + \", pred: \" + str(oc_pred[i]))\\n# #     if y_test[i] == oc_pred[i]: count+=1\\n        \\n# # print(count/290)',\n",
       " '53fe11f5': '# baseline model\\n\\nfrom sklearn.dummy import DummyClassifier\\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\\n\\ndummy_clf.fit(X, y)\\ndummy_clf.predict(X)\\ndummy_clf.score(X, y)',\n",
       " '761bd9c9': 'from sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import plot_confusion_matrix\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import OrdinalEncoder\\n\\n\\ncategorical_transformer = OrdinalEncoder()\\nnumerical_transformer = StandardScaler()\\n\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\"categorical\", categorical_transformer, categorical_columns),\\n        (\"numerical\", numerical_transformer, numerical_columns),\\n        \\n    ],remainder=\\'passthrough\\'\\n)\\n\\ndtree_model = Pipeline(\\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", DecisionTreeClassifier(max_depth = 300))]\\n)\\n\\n\\ndtree_model.fit(X_train, y_train)\\nfig, ax = plt.subplots(figsize=(10, 10))\\nplot_confusion_matrix(dtree_model, X_test, y_test, ax=ax)  \\nplt.show()',\n",
       " 'bb37fef7': 'X_train.head()\\n# , X_test, y_train, y_test',\n",
       " '4ece76c6': 'import optuna\\nfrom sklearn.metrics import accuracy_score\\n\\ndef objective(trial):\\n\\n    max_depth = trial.suggest_int(\"max_depth\", 2, 612)\\n    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 612)\\n    max_leaf_nodes = int(trial.suggest_int(\"max_leaf_nodes\", 2, 612))\\n\\n    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\\n\\n    DTC = DecisionTreeClassifier(min_samples_split = min_samples_split, \\n                                max_leaf_nodes = max_leaf_nodes,\\n                                criterion = criterion)\\n\\n    \\n    preprocessor = ColumnTransformer(\\n    transformers=[\\n        (\"categorical\", categorical_transformer, categorical_columns),\\n        (\"numerical\", numerical_transformer, numerical_columns),\\n        \\n    ],remainder=\\'passthrough\\'\\n    )\\n\\n    DTC = Pipeline(\\n        steps=[(\"preprocessor\", preprocessor), (\"classifier\", DecisionTreeClassifier(max_depth = 300))]\\n    )\\n    \\n    DTC.fit(X_train, y_train)\\n\\n    return 1.0 - accuracy_score(y_test, DTC.predict(X_test))\\n\\nstudy = optuna.create_study()\\nstudy.optimize(objective, n_trials = 500)',\n",
       " '0d1282df': 'print(study.best_params)\\nprint(1.0 - study.best_value)',\n",
       " '3861016f': 'from sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import plot_confusion_matrix\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import OrdinalEncoder\\n\\n# Read the data\\nX = pd.read_csv(\\'../input/udemy-courses/udemy_courses.csv\\', index_col=\\'course_id\\')\\n\\ncategorical_columns = [\\'level\\']\\nnumerical_columns = [\\'price\\', \\'num_lectures\\', \\'content_duration\\', \\'num_subscribers\\']\\nfeatures = categorical_columns+ numerical_columns\\n\\n# Select target value\\ny = X.subject\\n\\nX = X[features]\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\\n\\ncategorical_transformer = OrdinalEncoder()\\nnumerical_transformer = StandardScaler()\\n\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\"categorical\", categorical_transformer, categorical_columns),\\n        (\"numerical\", numerical_transformer, numerical_columns),\\n        \\n    ],remainder=\\'passthrough\\'\\n)\\n\\ndtree_model = Pipeline(\\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", DecisionTreeClassifier(max_depth = 300))]\\n)\\n\\n\\ndtree_model.fit(X_train, y_train)\\nfig, ax = plt.subplots(figsize=(10, 10))\\nplot_confusion_matrix(dtree_model, X_test, y_test, ax=ax)  \\nplt.show()\\n',\n",
       " '1851e4c5': 'from sklearn import tree\\n\\n# text_representation = tree.export_text(dtree_model[\"classifier\"])\\n# print(text_representation)\\nfig = plt.figure(figsize=(50,50))\\n_ = tree.plot_tree(dtree_model[\"classifier\"], \\n                   feature_names=numerical_columns + dtree_model[\"preprocessor\"].transformers_[0][2],  \\n                   class_names=[\"Web Development\", \"Business Finance\", \"Graphic Design\", \"Musical Instruments\"],\\n                   filled=True,\\n                   max_depth=3\\n                  )',\n",
       " '7f740633': 'The most popular courses tend to be either free or very expensive. Plots with the number of lectures and content duration show that most popular courses are shorter, and have fewer number of content hours. When it comes to subjects - the recipe for being popular is to make a course on Web Development.',\n",
       " '847b28dc': \"The cell below creates new column **time_on_platform** which is defined as the days elapsed since the upload of the course to info collection (last updated date in the dataset's description). I do so because all the other information were true about the course at that point in time. It is important when I relate courses' age to any other feature.\",\n",
       " '6352dc4e': '# # Predicting course popularity in time\\n\\nAt first, I made a predictive model that outputted the number of subscribers for a given course, but ... I started to think about what aim will a person have in mind when using the model. \\n\\nHaving a model say: \"Your course will have 2500 subscribers!\" won\\'t be very useful except boosting your ego. Often, numbers without context are of no use. If you wanted to get an answer to the question \"How many subscribers will my course have?\" you\\'ll probably also want a context, for example, timeframe, like 2500 subscribers in the first year from publishing your course. The information allows you to allocate less/more resources for updating the content, spend less/more on marketing, etc. In other words, gives you tangible benefits.\\n\\nFirstly, I will test if the number of subscribers depends on the time the course is on the platform.',\n",
       " 'd9691db9': 'To check if the course popularity depends on course age, I will check\\n\\n* The most popular courses (and what makes them popular)\\n\\n* The newest courses\\n\\n* The oldest courses\\n\\nAnd then investigate the number of subscribers for courses by the time on the platform, in those groups.',\n",
       " '62283e3c': \"For the classification algoithms, it is important to have balanced classes. Check class balance in year categories that I've created.\",\n",
       " '4433de42': 'First, I started off with a baseline model to compare my models to. I chose Dummy Classifier with the best performing strategy - in this case `most_frequent`.',\n",
       " '17b233cd': 'The most numerous dataframe is the one containging the newest courses.',\n",
       " '60228590': '# # Ordinal classification\\n\\nBased on: https://towardsdatascience.com/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c',\n",
       " '8046ddc4': 'At fisrt glance, I can tell that top most popular courses are Beginner Level/All levels courses on Web Development, and are on the platform form five to seven years, while the least popular are usually paid and on Graphic Design and Musical Instruments.',\n",
       " 'a6d91a9c': \"It looks like the courses based on time on the platform could be separated into four classes:\\n\\n- min to 4\\n\\n- 4 to 6\\n\\n- 6 to 7\\n\\n- 7 to 8\\n\\n- 8 +\\n\\nWhich are outliers and should they be excluded from the analysis? Is it possible to tell which range contains the most successful courses? To test that, I've plotted the four classes separately.\\n\\nNow, let's check the performance of models in the subsets defined by 'course age'.\",\n",
       " '1f0a14c4': \"The courses in the dataset are on the platform form ~ 2 to 8 years, 75% are 5 years or shorter. I've plotted a few pairplots to look for simple relations between chosen variables from the set.\",\n",
       " '617cd0e5': \"# # Time of publishing the course\\n\\nI've experimented the impact of the 'age' of a course on its popularity. Now, I'm interested whether the month of publishing the course also matters. For that I added the column denoting the month of publishing.\\n\\ntodo: month and number of subscribers, month trend in subsets\",\n",
       " 'f1752231': 'Below, I plotted the number of subscribers for courses by their age (in years).',\n",
       " '9ede5940': \"**Which columns to exclude and why?**\\n\\n* num_reviews - it is directly connected to the number of subscribers and this information is not availible at the time of publishing the course (data leakage)\\n* url - cannot be chosen by the course author, shouldn't be used in prediction\\n* is_paid - carries a less complete yet simmilar information in comparison to price column\\n* published_timestamp - is replaced by time_on_platform\\n* ? title - all unique\",\n",
       " '59a7ac64': '# Predicting udemy course popularity',\n",
       " 'f33acb49': 'Hyperparameter optimization using optuna.',\n",
       " '35b4ff8c': '# # Classification',\n",
       " '4012a3dc': \"[ ] How to include course title?\\n \\n - error: can't handle unknown\\n\\nWhy Ordinal Encoder? https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769\\n\\nOne hot encoder with categorical variables produces very sparse trees:\\n\\n![image.png](attachment:07d2bdeb-e3fb-4601-85c3-c4a6d149c975.png)!\\n\\n\",\n",
       " 'c9af2505': '# Scratch pad',\n",
       " 'fdf8cb2c': \"Hi! This notebook is still a work in progress. If you are interested in the process, and what I've accomplished so far... keep on reading ;)\\n\\nIn this notebook, I will analyze the dataset and check how to effectively predict the course popularity (defined as the number of subscribers).\",\n",
       " 'd4c69cec': '# # Regression',\n",
       " '8608de0d': '# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n# For example, here\\'s several helpful packages to load in \\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns \\n\\n# Input data files are available in the \"../input/\" directory.\\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\\n\\nimport os\\n\\n\\nfrom subprocess import check_output\\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\\n\\n# Any results you write to the current directory are saved as output.',\n",
       " 'c93bda0e': \"data = pd.read_csv('../input/Iris.csv')\",\n",
       " 'febf0ee9': 'data.columns',\n",
       " '9ea8c034': 'data.info()',\n",
       " '7e8aaaff': 'data.corr()',\n",
       " '1345d7ca': \"f,ax = plt.subplots(figsize=(18, 18))\\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\\nplt.show()\",\n",
       " '4f1dae94': 'data.PetalWidthCm.plot(kind = \"line\",color = \"r\",label=\"PWidth\",linestyle=\":\",linewidth=2,grid=True)\\ndata.SepalWidthCm.plot(color = \"b\",label=\"SWidth\",linewidth=2,linestyle=\"-.\",grid=True)\\nplt.legend()\\nplt.xlabel(\"X\")\\nplt.ylabel(\"y\")\\nplt.show()',\n",
       " '7147f794': '#plt.scatter(data.PetalLengthCm,data.SepalLengthCm,alpha=0.5,color = \"r\")\\n\\ndata.plot(kind = \"scatter\",x=\"PetalLengthCm\",alpha = 0.5 , color=\"r\",\\n          y=\"SepalLengthCm\")\\n\\nplt.show()',\n",
       " '664f674f': 'data.SepalLengthCm.plot(kind = \\'hist\\',bins = 50,figsize = (12,12))\\nplt.show()\\nprint(\"Mean of SepalLengthCm: \",(sum(data.SepalLengthCm)/len(data.SepalLengthCm)))',\n",
       " 'e26ac330': \"dictionary = {'Turkey' : 'Kayseri','Germany' : 'Münih'}\\nprint(dictionary.keys())\\nprint(dictionary.values())\\n\",\n",
       " '1eb451de': 'dictionary[\"Turkey\"]=\"Bursa\" # add a new value\\nprint(dictionary)\\nprint(\" \")\\ndictionary[\"Germany\"] = \"München\" # update existing value\\nprint(dictionary)\\nprint(\" \")\\nprint(\"New York\"in dictionary) # check \\nprint(\" \")\\ndel dictionary[\"Germany\"]# remove \"Germany\" from dictionary\\nprint(dictionary)\\nprint(\" \")\\ndictionary.clear()#clear all values\\nprint(dictionary)\\nprint(\" \")\\n',\n",
       " '9466b2c5': 'series = data[\"SepalLengthCm\"] \\nprint(type(series))\\ndata_frame = data[[\"SepalLengthCm\"]] # İf you dont use double [] you cant use dataframe classes!\\nprint(type(data_frame))',\n",
       " '86664b5d': 'x=data[\"PetalLengthCm\"]>6\\ndata[x]',\n",
       " '9919f572': \"data[(data['PetalLengthCm']>6) & (data['PetalWidthCm']>2.2)]\",\n",
       " '4b3390be': 'for i in dictionary:\\n   print(dictionary[i])',\n",
       " 'cbc054d4': 'for key, value in dictionary.items():\\n    print(key,\" : \",value)',\n",
       " '5b426fb2': 'data.info()',\n",
       " '0002fd8e': 'mean_of_sepallength = sum(data.SepalLengthCm) / len(data.SepalLengthCm)\\n#print(mean_of_sepallength)\\ndata[\"LeafLength\"] = [\"long\" if i>mean_of_sepallength else \"short\" for i in data.SepalLengthCm]\\n\\nprint(data[50:65])\\n',\n",
       " '7e73b091': \"print(data['SepalLengthCm'].value_counts(dropna =False))\",\n",
       " '09e8df3c': 'data.describe()',\n",
       " '8a854e05': \"data.boxplot(column='SepalLengthCm',by = 'SepalWidthCm')\\nplt.show()\",\n",
       " '4a34e604': 'newdata1 = data.head()\\nnewdata2 = data.tail()\\nconc_data = pd.concat([newdata1,newdata2],axis=0,ignore_index = True )\\n#ignore_index adds new id column\\n#if axix=1 it will be concatenate datas horizontly\\nconc_data',\n",
       " 'd14ba2f6': '# id_vars = what we do not wish to melt\\n# value_vars = what we want to melt\\nmelted = pd.melt(frame=conc_data , id_vars=\"Species\",value_vars=[\"SepalLengthCm\",\"PetalLengthCm\"])\\nmelted',\n",
       " 'c3b31640': 'data.dtypes',\n",
       " 'c2f1bc3f': \"data['SepalLengthCm'] = data['SepalLengthCm'].astype('str')\\n#We converted SepalLengthCm float to str\",\n",
       " 'a4ff8f11': 'data.dtypes',\n",
       " 'f4bec5e3': 'data1=data\\ndata1[\"Species\"].dropna(inplace = True)# we droped nan values in species',\n",
       " 'ec3ea6c5': 'data1[\"Species\"]# there is no missing data',\n",
       " '4f95c9f7': 'country = [\"Turkey\",\"Germany\"]\\npopulation = [\"80\",\"50\"]\\nlist_label = [\"country\",\"population\"]\\nlist_col = [country,population]\\nzipped = list(zip(list_label,list_col))\\ndata_dict = dict(zipped)\\ndf = pd.DataFrame(data_dict)\\ndf\\n',\n",
       " 'e3bfda37': 'df[\"capital\"] = [\"Ankara\",\"Münih\"]\\ndf',\n",
       " '3e289cca': 'data1 = data.loc[:,[\"SepalLengthCm\",\"PetalLengthCm\",\"SepalWidthCm\",\"SepalWidthCm\"]]\\ndata1.plot()\\n#hard to understand',\n",
       " 'eb84ec44': 'data1.plot(subplots = True)\\nplt.show()\\n#you can easily understand now',\n",
       " '56ffd650': 'data.describe()',\n",
       " '58631026': '#how to do str to datatime\\n\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")\\n# close warning\\n\\ndata2 = data.head()\\ndate_list = [\"1992-01-10\",\"1992-02-10\",\"1992-03-10\",\"1993-03-15\",\"1993-03-16\"]\\ndatetime_object = pd.to_datetime(date_list)\\ndata2[\"date\"] = datetime_object\\ndata2= data2.set_index(\"date\")\\ndata2 ',\n",
       " '1127ae4a': 'data2.resample(\"A\").mean()\\ndata2.resample(\"M\").mean()\\n#we havent got enough value for all months',\n",
       " '378a2cdd': 'data2.resample(\"M\").first().interpolate(\"linear\")\\n#fill NaNs linearly',\n",
       " 'd5d0b74b': 'data\\ndata = data.set_index(\"Id\")\\ndata',\n",
       " '0b0a8689': \"data['SepalLengthCm'] = data['SepalLengthCm'].astype('float')\\nfilter1 = data.SepalLengthCm> 6\\ndata[filter1]\\n\",\n",
       " 'dd6adea4': 'data2 =data.SepalWidthCm[filter1]\\ndata2',\n",
       " '414da35c': '**SECOND PART**',\n",
       " 'b43dc5ad': '*Describe function gives as median of the data and another informations.*',\n",
       " 'fcd9586c': '> ***Histogram***',\n",
       " '3a31c74e': '> ***Getting Values with Index in Dictionary with for loop***',\n",
       " 'c59ee953': \"**DİCTİONARY**\\n* *Sample of dictionary:*\\n* dictionary = {'spain' : 'madrid','usa' : 'vegas'}\\n\",\n",
       " '42143d30': '> ***Basic for loop***',\n",
       " 'bc685ff0': 'It is my first try to analyzing a dataset. And I choose Iris dataset because it is simple and useful . Thanks for your reading:)\\n\\nParts of my notebook:\\n\\n1. Visualization \\n1. List Comprehension\\n1. Cleaning Data',\n",
       " 'f2f61ed0': '**VISUALIZATION AND ANALYZING OF IRIS DATASET**\\n\\n',\n",
       " '2ec33fd4': '*Concatenating two datas with pd.concat*',\n",
       " 'e81152a0': '> ***Defining Dictionary***',\n",
       " '40f6947c': '*We can find our outlayers*',\n",
       " '158ad323': '> *** Scatterplot***',\n",
       " '8f4debfb': '> ***Lineplot***',\n",
       " '6f5db5d7': '> ***BUILDING DATA FRAMES FROM SCRATCH***',\n",
       " 'ca8e00bb': '> ***Getting Data***',\n",
       " '69e2fcf6': '*We can change dtype with astype function*',\n",
       " '7bc2c761': '> ***Some Tools For Dictionary***',\n",
       " 'a6146c9b': '**FOURTH PART**',\n",
       " '4fa21bf0': '> ***Filtering the Data with Two İnputs***',\n",
       " 'dd9919f9': '* parse_dates(boolean): Transform date to ISO 8601 (yyyy-mm-dd hh:mm:ss ) format',\n",
       " '67332f12': '***MANIPULATING DATA FRAMES WITH PANDAS***',\n",
       " 'ba52ceea': '***STATISTICAL EXPLORATORY DATA ANALYSIS***\\n',\n",
       " 'fa73ca84': '.\\n\\n\\n\\n**PANDAS**\\n\\n.\\n\\n\\n\\n',\n",
       " '69a3cae8': '*Number of the values in this label. *',\n",
       " '66073621': '> ***Filtering The Data***',\n",
       " '921060fb': '*We can get data types with data.dtypes\\n*',\n",
       " 'd130b6c6': '**THİRD PART**',\n",
       " 'd802b721': 'We tried: \\n\\n1. importing scv files\\n1. Plotting line,scatter and histogram\\n1. Dictionary\\n1. Basic pandas\\n1. Basic for loops\\n\\n\\nİt is my first try.\\n\\nSorry for my bad english.\\n \\nThanks for your reading...',\n",
       " 'ac15ffcb': '*Using list comprehension*',\n",
       " '7bc1b9a7': '***INDEXING PANDAS TIME SERIES***',\n",
       " '9578c44a': '> ***Some infos About Dataset of Iris***',\n",
       " '0779eb73': '*We melted our data*',\n",
       " '88fc381c': \"# import all needed libraries\\nimport numpy as np\\nimport pandas as pd\\nimport spacy\\nfrom spacy import displacy\\nimport matplotlib.pyplot as plt\\nimport warnings\\nimport os\\n\\nimport nltk\\nfrom nltk.corpus import words as english_words, stopwords\\nfrom nltk.stem import PorterStemmer\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.model_selection import train_test_split\\n\\nimport seaborn as sns\\n\\nfrom wordcloud import WordCloud, STOPWORDS \\n\\nfrom collections import Counter\\n\\nimport torch\\n\\nimport re\\n\\nwarnings.filterwarnings('ignore')\\n\\n%matplotlib inline\",\n",
       " 'b5131ff3': '# load data\\nspeechs = list()\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        file_path = os.path.join(dirname, filename)\\n        print(file_path)\\n        text = open(file_path,\\'r\\').read()\\n        speechs.append(text)\\n\\nprint(f\"Total Number of Documents : {len(speechs)}\")',\n",
       " '347676a2': \"def cleansing_text(text: str) -> str:\\n    ## replacing the newlines and extra spaces, and change all character to lower case\\n    corpus = text.replace('\\\\n', ' ').replace('\\\\r', '').replace('  ',' ').lower()\\n\\n    ## removing everything except alphabets\\n    corpus_sans_symbols = re.sub('[^a-zA-Z \\\\n]', '', corpus)\\n\\n    ## removing stopwords\\n    stop_words = set(w.lower() for w in stopwords.words())\\n\\n    corpus_sans_symbols_stopwords = ' '.join(filter(lambda x: x.lower() not in stop_words, corpus_sans_symbols.split()))\\n    return corpus_sans_symbols_stopwords\\n    \\npreprocessed_speechs = list(map(cleansing_text, speechs))\\n\\n# display an example of Tramp Speech\\nprint(preprocessed_speechs[0][:100] + '...')\\n\",\n",
       " 'a9ac0c00': \"stemmer = nltk.PorterStemmer()\\ndef stemmer_str(text: str)-> str:\\n    corpus_stemmed = ' ' .join (map(lambda str: stemmer.stem(str), text.split()))\\n    return corpus_stemmed\\n\\npreprocessed_speechs_stemmer = list(map(stemmer_str, preprocessed_speechs))\\n\\nprint(preprocessed_speechs_stemmer[0][:100] + '...')\\n\",\n",
       " '3ee8d131': 'all_speechs_str = \\' \\'.join(preprocessed_speechs)\\n\\nwordcloud = WordCloud(width = 800, height = 800,background_color =\\'grey\\', min_font_size = 10).generate(all_speechs_str)\\nplt.figure(figsize = (8, 8), facecolor = None) \\nplt.imshow(wordcloud) \\nplt.rcParams.update({\\'font.size\\': 25})\\nplt.axis(\"off\") \\nplt.title(\\'Word Cloud: DJT Rallies \\')\\nplt.tight_layout(pad = 0) \\n  \\nplt.show()',\n",
       " '8afbe973': 'word_freq_count = Counter(\\' \\'.join(preprocessed_speechs).split(\" \"))\\n\\ncommon_words = [word[0] for word in word_freq_count.most_common(20)]\\ncommon_counts = [word[1] for word in word_freq_count.most_common(20)]\\n\\nplt.figure(figsize=(15, 12))\\n\\nsns.set_style(\"whitegrid\")\\nsns_bar = sns.barplot(x=common_words, y=common_counts)\\nsns_bar.set_xticklabels(common_words, rotation=45)\\nplt.title(\\'Most Common Words in the document\\')\\nplt.show()',\n",
       " 'd09c76ed': 'all_speechs_str = \\' \\'.join(preprocessed_speechs_stemmer)\\n\\nwordcloud = WordCloud(width = 800, height = 800,background_color =\\'grey\\', min_font_size = 10).generate(all_speechs_str)\\nplt.figure(figsize = (8, 8), facecolor = None) \\nplt.imshow(wordcloud) \\nplt.rcParams.update({\\'font.size\\': 25})\\nplt.axis(\"off\") \\nplt.title(\\'Word Cloud: DJT Rallies \\')\\nplt.tight_layout(pad = 0) \\n  \\nplt.show()\\n ',\n",
       " 'd7fdf2a9': 'word_freq_count = Counter(\\' \\'.join(preprocessed_speechs_stemmer).split(\" \"))\\n\\ncommon_words = [word[0] for word in word_freq_count.most_common(20)]\\ncommon_counts = [word[1] for word in word_freq_count.most_common(20)]\\n\\nplt.figure(figsize=(15, 12))\\n\\nsns.set_style(\"whitegrid\")\\nsns_bar = sns.barplot(x=common_words, y=common_counts)\\nsns_bar.set_xticklabels(common_words, rotation=45)\\nplt.title(\\'Most Common Words in the document\\')\\nplt.show()',\n",
       " '23c1ff32': 'from itertools import islice\\n\\ntfidf_vec = TfidfVectorizer(stop_words=\"english\")\\ntransformed = tfidf_vec.fit_transform(raw_documents=preprocessed_speechs)\\nindex_value={i[1]:i[0] for i in tfidf_vec.vocabulary_.items()}\\nprint( {k: index_value[k] for k in list(index_value)[:50]})',\n",
       " '51410483': 'tfidf_vec = TfidfVectorizer(stop_words=\"english\")\\ntransformed = tfidf_vec.fit_transform(raw_documents=preprocessed_speechs_stemmer)\\nindex_value={i[1]:i[0] for i in tfidf_vec.vocabulary_.items()}\\nprint( {k: index_value[k] for k in list(index_value)[:50]})',\n",
       " '68593e37': \"# this function can get any n-grams from a string\\n# note that we can use also nltk.bigrams(eng_tokens)\\ndef get_n_grams(text: str, n:int):\\n    n_grams = list()\\n    text_tokens = text.split(' ')\\n    for index, token in enumerate(text_tokens):\\n        if index+n < len(text_tokens):\\n            n_grams.append(tuple(text_tokens[index:index+n]))\\n    return n_grams\",\n",
       " '15405cf3': '#get bi-grams from all speechs\\nbi_grams = list()\\nfor speech in preprocessed_speechs:\\n    bi_grams = bi_grams + get_n_grams(speech, 2)\\n\\nbi_grams_freq = nltk.FreqDist(bi_grams)\\nbi_grams_sorted = sorted(bi_grams_freq , key = bi_grams_freq.__getitem__, reverse = True)\\n\\n# keep only 20\\nbi_grams_sorted = bi_grams_sorted[:20]\\n[print(item, \\' : \\', bi_grams_freq[item]) for item in bi_grams_sorted]\\n\\nbi_grams_dict = dict()\\nfor item in bi_grams_sorted:\\n    bi_grams_dict[\\' \\'.join(item)] = bi_grams_freq[item]\\n \\nwordcloud = WordCloud(width = 800, height = 800,background_color =\\'grey\\', min_font_size = 10).generate_from_frequencies(bi_grams_dict)\\nplt.figure(figsize = (8, 8), facecolor = None) \\nplt.imshow(wordcloud) \\nplt.rcParams.update({\\'font.size\\': 25})\\nplt.axis(\"off\") \\nplt.title(\\'Word Cloud: DJT Rallies \\')\\nplt.tight_layout(pad = 0) \\n  \\nplt.show()',\n",
       " 'a14b2f2a': '#get 3-grams from all speechs\\nthree_grams = list()\\nfor speech in preprocessed_speechs:\\n    three_grams = three_grams + get_n_grams(speech, 3)\\n\\nthree_grams_freq = nltk.FreqDist(three_grams)\\nthree_grams_sorted = sorted(three_grams_freq , key = three_grams_freq.__getitem__, reverse = True)\\n\\n# keep only 20\\nthree_grams_sorted = three_grams_sorted[:20]\\n[print(item, \\' : \\', three_grams_freq[item]) for item in three_grams_sorted]\\n\\nthree_grams_dict = dict()\\nfor item in three_grams_sorted:\\n    three_grams_dict[\\' \\'.join(item)] = three_grams_freq[item]\\n     \\nwordcloud = WordCloud(width = 800, height = 800,background_color =\\'grey\\', min_font_size = 10).generate_from_frequencies(three_grams_dict)\\nplt.figure(figsize = (8, 8), facecolor = None) \\nplt.imshow(wordcloud) \\nplt.rcParams.update({\\'font.size\\': 25})\\nplt.axis(\"off\") \\nplt.title(\\'Word Cloud: DJT Rallies \\')\\nplt.tight_layout(pad = 0) \\n  \\nplt.show()',\n",
       " 'bdc2ad27': '## 1.3 Stemming and Lemmatizing',\n",
       " '605aaf4a': 'Without Stemming',\n",
       " 'cac2259d': '## 1.1 Load Data',\n",
       " 'ffff74a6': 'With stemming',\n",
       " '2a6e2cc2': '# 2. Frequency appearance words, n-grams, phrases:',\n",
       " '3fe591d8': '**With Stemmer**',\n",
       " 'da5b9e71': \"# Features of this analysis\\n\\nThe purpose of this exersise is to carry out an NLP investigation of Donald Trump'ss speeches at ellection rallies during 2019 and 2020. The following tasks is handled :\\n\\n* most used words, bigrams, 3-grams to display in word-cloud graph\\n\\n\\n\",\n",
       " 'a382a76b': '## 2.1 Words Frequencies',\n",
       " 'f6ed05fe': '## 1.2 Removing special characters and stopwords(using NLTK)',\n",
       " 'e0319b48': '## 2.3 Top 20 n-grams frequency',\n",
       " 'c0415191': '# 1. Pre-processing Speach',\n",
       " '8f2fbf1a': '## 2.2 TOP 50 terms using TF-IDF\\n\\nTF-IDF, short for **term frequency–inverse document frequency**, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\\n\\n**Without Stemmer**',\n",
       " '71320928': '**Word cloud of 3-grams**',\n",
       " 'cb90cc13': '**Word cloud of bi-grams**',\n",
       " '4cbc126b': \"import pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.neighbors import KNeighborsClassifier \\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn import svm\\nfrom sklearn.feature_selection import SelectFromModel\\nimport xgboost as xgb\\n\\n# To ignore unwanted warnings\\nimport warnings\\nwarnings.filterwarnings('ignore')\",\n",
       " '2555ce32': \"train = pd.read_csv('../input/titanic/train.csv')\\ntest = pd.read_csv('../input/titanic/test.csv')\\ngender_submission = pd.read_csv('../input/titanic/gender_submission.csv')\",\n",
       " '09e3e3d4': 'train.head()',\n",
       " '69bb2000': 'test.head()',\n",
       " 'db3d42e5': \"train.info()\\nprint('_'*40)\\ntest.info()\",\n",
       " '3d55fc1e': \"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\\n# Train data \\nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\\nax[0].set_title('Train data')\\n# Test data\\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\\nax[1].set_title('Test data');\",\n",
       " '29bd168d': \"#missing amount for train set\\nmissing= train.isnull().sum().sort_values(ascending=False)\\npercentage = (train.isnull().sum()/ train.isnull().count()).sort_values(ascending=False)\\nmissing_data = pd.concat([missing, percentage], axis=1, keys=['Missing', '%'])\\nmissing_data.head(3)\",\n",
       " 'ea0348cf': \"#missing amount for test set\\nmissing= test.isnull().sum().sort_values(ascending=False)\\npercentage = (test.isnull().sum()/ test.isnull().count()).sort_values(ascending=False)\\nmissing_data = pd.concat([missing, percentage], axis=1, keys=['Missing', '%'])\\nmissing_data.head(3)\",\n",
       " '063cc573': \"train['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)\",\n",
       " 'e53f6e1b': \"train['Embarked'].value_counts()\",\n",
       " '40c2ba2f': \"isn = pd.isnull(test['Fare'])\\ntest[isn]\",\n",
       " '28525e7b': \"average_of_fare = test.groupby('Pclass')['Fare'].mean()\\ntest['Fare'].fillna(value= average_of_fare[3], inplace=True)\",\n",
       " '60b987de': \"# combine train and test then find mean age grouped by Pclass\\nage_mean_by_pclass = pd.concat([train, test]).groupby('Pclass')['Age'].mean()\\nage_mean_by_pclass\",\n",
       " '785ecc04': \"train.loc[train['Age'].isnull(), 'Age'] = train['Pclass'].map(age_mean_by_pclass)\\ntest.loc[test['Age'].isnull(), 'Age'] = test['Pclass'].map(age_mean_by_pclass)\",\n",
       " '02225a68': \"# Sex & Age\\ng = sns.FacetGrid(train, hue = 'Survived', col = 'Sex', height = 3, aspect = 2)\\ng.map(plt.hist, 'Age', alpha = .5, bins = 20)\\ng.add_legend()\\nplt.show()\",\n",
       " '5434768b': \"train['Age'] = train['Age'].astype(int)\\ntest['Age'] = test['Age'].astype(int)\",\n",
       " '1557c42c': \"def age_range(df):\\n    df['Age'].loc[df['Age'] <= 16 ] = 0\\n    df['Age'].loc[(df['Age'] > 16) & (df['Age'] <= 32)] = 1\\n    df['Age'].loc[(df['Age'] > 32) & (df['Age'] <= 48)] = 2\\n    df['Age'].loc[(df['Age'] > 48) & (df['Age'] <= 64)] = 3\\n    df['Age'].loc[df['Age'] > 64] = 4   \\nage_range(train)\\nage_range(test)\",\n",
       " 'b56806e2': \"train['Cabin'].isnull().sum()\",\n",
       " '428e7c9c': \"train.groupby(train['Cabin'].isnull())['Survived'].mean()\",\n",
       " '4d29e30f': \"# train\\ntrain['Cabin'] = train['Cabin'].notnull().astype('int')\\n# test\\ntest['Cabin']=test['Cabin'].notnull().astype('int')\",\n",
       " '6e6ec51d': \"train_titles, test_titles = set(), set()  # empty sets to save titles\\n\\nfor train_name, test_name in zip(train['Name'], test['Name']):\\n    train_titles.add(train_name.split(',')[1].split('.')[0].strip())\\n    test_titles.add(test_name.split(',')[1].split('.')[0].strip())\\nprint(train_titles,'\\\\n', test_titles)\",\n",
       " 'd493a2cc': \"def title(df):\\n    # all the titles will be replaced to one of the following: 'Mr', 'Ms', Master, 'Officer', 'Royalty', 'Miss'\\n    df['Title'] = df['Name'].str.split(', ').str[1].str.split('.').str[0]\\n    \\n    df['Title'] = df['Title'].replace(['Capt','Col','Major','Dr','Rev'], 'Officer')\\n    df['Title'] = df['Title'].replace(['Mme','Ms'], 'Mrs')\\n    df['Title'] = df['Title'].replace(['Jonkheer','Don','Dona','Sir','the Countess','Lady'], 'Royalty')\\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\\n\\ntitle(train)\\ntitle(test)\",\n",
       " '2d835c8a': \"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\",\n",
       " '21af6791': \"train['FamilySize'].value_counts()\",\n",
       " 'f59d0d52': \"train['FamilySize'] = train['FamilySize'].astype(int)\\ntest['FamilySize'] = test['FamilySize'].astype(int)\\ndef family_range(df):\\n    df['FamilySize'].loc[df['FamilySize'] <= 1 ] = 0\\n    df['FamilySize'].loc[(df['FamilySize'] >= 2) & (df['FamilySize'] <= 4)] = 1\\n    df['FamilySize'].loc[df['FamilySize'] >= 5] = 2   \\nfamily_range(train)\\nfamily_range(test)\",\n",
       " '856e25a5': \"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\\n# Train data \\nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\\nax[0].set_title('Train data')\\n# Test data\\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\\nax[1].set_title('Test data');\",\n",
       " '7ce6cda0': \"# Train Data\\ntrain = pd.get_dummies(train, columns=['Sex','Embarked','Title'],drop_first=True)\\n# Test Data\\ntest= pd.get_dummies(test, columns=['Sex','Embarked','Title'],drop_first=True)\",\n",
       " '7db61e9d': \"fig=plt.figure(figsize=(18,10))\\nax = fig.gca()\\nsns.heatmap(train.corr(), annot=True,ax=ax, cmap=plt.cm.YlGnBu)\\nax.set_title('The correlations between all numeric features')\\npalette =sns.diverging_palette(80, 110, n=146)\\nplt.show()\",\n",
       " 'de2d36e7': 'corr_matrix = train.corr()\\ncorr_matrix[\"Survived\"].sort_values(ascending=False)',\n",
       " '0b0fe6ca': \"_ = sns.countplot(data=train, x='Survived', hue='Pclass')\\n_.set_title('Number of Survivals by Class')\\n_.set_xlabel('Survival')\\n_.set_xticklabels(['Dead', 'Survived'])\\n_.legend(['1st class', '2nd class', '3rd class']);\",\n",
       " '3b6fe1c3': \"print('Survival rate per class: ')\\ntrain.groupby('Pclass')['Survived'].sum() / train['Pclass'].value_counts() # Survive percent by class\",\n",
       " 'f7c482e4': \"_ = sns.countplot(data=train, x='Pclass', hue='Sex_male')\\n_.set_title('Number of Males and Females by Class')\\n_.set_xticklabels(['1st class', '2nd class', '3rd class'])\\n_.legend(['Female', 'Male']);\",\n",
       " '7eff1cb3': \"_ = sns.countplot(data=train, x='Survived', hue='FamilySize')\\n_.set_title('Number of Deaths by Family Size')\\n_.set_xlabel('Survival')\\n_.set_xticklabels(['Dead','Survived'])\\n_.legend(['Single', 'Small (2 to 4)', 'Large (5 and more)'], title='Family Size');\",\n",
       " '06f70dd4': \"print('Survival rate for people by there Family Size: ')\\n(train.groupby('FamilySize')['Survived'].sum() / train['FamilySize'].value_counts()).rename({0:'Single', 1:'Small', 2:'Large'})\",\n",
       " 'a9c441d5': \"_ = sns.countplot(data=train, x='Survived', hue='Sex_male')\\n_.set_title('Number of Survivals and Deaths Per Gender')\\n_.set_xlabel('Survival')\\n_.set_xticklabels(['Dead','Survived'])\\n_.legend(['Female', 'Male']);\",\n",
       " '0619eab1': \"print('Survival rate per gender: ')\\n(train.groupby('Sex_male')['Survived'].sum() / train['Sex_male'].value_counts()).rename(index={0:'female',1:'male'})\",\n",
       " '52b99c8f': \"_ = sns.countplot(data=train, x='Survived')\\n_.set_title('Number of Survivals and Deaths')\\n_.set_xlabel('Survival')\\n_.set_xticklabels(['Dead','Survived']);\",\n",
       " '021a0530': \"train['Survived'].value_counts()\",\n",
       " '8463c385': \"# majority_count, minority_count = train['Survived'].value_counts()\\n# majority = train[train['Survived'] == 0]\\n# minority = train[train['Survived'] == 1]\\n\\n# minority_overSamp = minority.sample(majority_count, replace=True, random_state=55)\\n# train_overSamp = pd.concat([majority, minority_overSamp])\\n\\n# train_overSamp['Survived'].value_counts()\",\n",
       " '2391e32a': \"# majority_count, minority_count = train['Survived'].value_counts()\\n# majority = train[train['Survived'] == 0]\\n# minority = train[train['Survived'] == 1]\\n\\n# majority_underSamp = majority.sample(minority_count)\\n# train_underSamp = pd.concat([majority_underSamp, minority], axis=0)\\n\\n# train_underSamp['Survived'].value_counts()\",\n",
       " 'e564a46a': '# from imblearn.over_sampling import SMOTE\\n# oversample = SMOTE()\\n# X, y = oversample.fit_resample(X, y)',\n",
       " 'f4f63f50': '# from sklearn.feature_selection import SelectKBest\\n# from sklearn.feature_selection import chi2\\n# ksel = SelectKBest(chi2, k=9) \\n# ksel.fit(X, y) \\n# new_X = ksel.transform(X)\\n# new_testing = ksel.transform(testing)\\n\\n# ix = ksel.get_support()\\n# pd.DataFrame(new_X, columns = X.columns[ix]).head(5)',\n",
       " 'aec6bb74': \"# Train data\\nfeatures_drop = ['PassengerId','Name', 'Ticket', 'Survived','SibSp','Parch']\",\n",
       " '24131b97': 'selected_features = [x for x in train.columns if x not in features_drop]',\n",
       " 'e3fd535c': \"# Test data\\nfeatures_drop_test = ['PassengerId','Name', 'Ticket','SibSp','Parch']\",\n",
       " '9e4f8d93': 'selected_features_test = [x for x in test.columns if x not in features_drop_test]',\n",
       " '107f27e3': \"# Train data\\nX = train[selected_features]\\ny = train['Survived']\",\n",
       " '8d64bce9': '# Test data\\ntesting = test[selected_features_test]',\n",
       " '06c514ff': 'ss = StandardScaler()\\nXs = ss.fit_transform(X)',\n",
       " '4fe7db3f': 'testing_s = ss.transform(testing)',\n",
       " '4703dc7c': 'X_train, X_test, y_train, y_test = train_test_split(\\n    Xs, y, test_size = .3, random_state= 42, stratify = y) ',\n",
       " 'd68094ed': 'def modeling(model, X_train, y_train, test_data, X_test=None, y_test=None, prefit=False):\\n    \\'\\'\\'Takes model and data then print model results with some metrics then return predictions\\'\\'\\'\\n    \\n    start = \"\\\\033[1m\" # to create BOLD print\\n    end = \"\\\\033[0;0m\" # to create BOLD print\\n    \\n    # Print bold model name \\n    model_name = str(model)#.split(\\'(\\')[0]\\n    print(\\'\\'.join([\\'\\\\n\\', start, model_name, end]))\\n    \\n    #Fit model\\n    if not prefit:\\n        model.fit(X_train, y_train)\\n    \\n    #Accuarcy score    \\n    print(\\'Train Score\\', model.score(X_train, y_train))\\n    try:\\n        print(\\'Test Score :\\', model.score(X_test, y_test))\\n    \\n        #confusion matrix\\n        X_test_pred = model.predict(X_test)\\n        print(\\'\\\\nconfusion matrix\\\\n\\', confusion_matrix(y_test, X_test_pred))  \\n    except: pass\\n    \\n    #cross val score\\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n    cv_res = cross_val_score(model, X_train, y_train, cv=cv, scoring=\\'accuracy\\')\\n    print(\\'\\\\nCV scores: \\', cv_res)\\n    print(\\'CV scores mean: \\', cv_res.mean())  \\n    \\n    #predictions\\n    y_pred = model.predict(test_data)\\n    print(\\'\\\\nFirst 10 Predictions: \\\\n\\', y_pred[:10])\\n\\n\\n    return y_pred',\n",
       " '66f95ee6': \"# Models\\nlogreg = LogisticRegression(max_iter=300)\\nknn = KNeighborsClassifier(n_neighbors=7)  \\nsvm_lin = svm.SVC(kernel='linear', C=33)\\nsvm_poly = svm.SVC(kernel='poly', C=3)\\nsvm_rbf = svm.SVC(kernel='rbf', C=33)\\n\\n# randomF = RandomForestClassifier(max_depth=350, n_estimators=9, max_features=11, random_state=14, min_samples_split=3)\\nrandomF = RandomForestClassifier(max_depth=350, random_state=42)\\ndtree= DecisionTreeClassifier(random_state=42)\\nextree = ExtraTreesClassifier(n_estimators=66, min_samples_split=7, random_state=42)\\nxgb_model = xgb.XGBClassifier(colsample_bytree= 0.8, gamma= 1, learning_rate= 0.002,\\n                              max_depth= 8, min_child_weight= 1,subsample= 0.8,)\\n\\n\\n\\nmodels = [(logreg,'logreg'), (knn,'knn'), (svm_lin,'svm_lin'), (svm_poly,'svm_poly'), (svm_rbf,'svm_rbf'),\\n          (randomF,'randomF'), (dtree,'dtree'), (extree,'extree'), (xgb_model,'xgb_model')]\\n\\npreds = {}    # empty dict to save all models predictions\\nfor model, name in models:\\n    preds[name] = modeling(model, X_train, y_train, testing_s, X_test, y_test)\",\n",
       " '4c880c8d': \"def g_search(model, param, X_train, y_train, test_data, X_test=None, y_test=None):\\n    '''Simple grid search with kfold'''\\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n    gs = GridSearchCV(model,\\n                  param,\\n                  scoring='accuracy',\\n                  cv=cv,\\n                  n_jobs=-1,\\n                  verbose=0)\\n    gs.fit(X_train, y_train)\\n    \\n    # Results\\n    y_pred = modeling(gs.best_estimator_, X_train, y_train, test_data, X_test, y_test, prefit=True) # print results and return predictions\\n    \\n    print('Best parameters: ', gs.best_params_)\\n    \\n    return y_pred\",\n",
       " '2cc2656f': \"# grid search using all the data (Xs, y)\\ngrid_logreg_pred = g_search(LogisticRegression(), {'C': np.arange(1, 40, 1)}, Xs, y, testing_s)\\ngrid_knn_pred = g_search(KNeighborsClassifier(), {'n_neighbors': np.arange(1, 100, 1)}, Xs, y, testing_s)\\ngrid_svm_lin_pred = g_search(svm.SVC(kernel='linear'), {'C': np.arange(1, 40, 1)}, Xs, y, testing_s)\\ngrid_svm_poly_pred = g_search(svm.SVC(kernel='poly'), {'C': np.arange(1, 40, 1)}, Xs, y, testing_s)\\ngrid_svm_rbf_pred = g_search(svm.SVC(kernel='rbf'), {'C': np.arange(1, 40, 1)}, Xs, y, testing_s)\",\n",
       " 'c0f8a1de': 'dt = DecisionTreeClassifier()\\ndt_en = BaggingClassifier(base_estimator=dt, n_estimators=500, max_features=4, random_state=55)\\n\\ndt_en_pred = modeling(dt_en, Xs, y, testing_s) ',\n",
       " 'fa6e5eac': 'adaboost = AdaBoostClassifier(n_estimators=67)\\nadaboost.fit(X_train, y_train)\\n\\nadaboost_pred = modeling(adaboost, Xs, y, testing_s) ',\n",
       " '543073cd': 'xgb_model = xgb.XGBClassifier(\\n\\n    colsample_bytree= 0.8,\\n    gamma= 1,\\n    learning_rate= 0.002,\\n    max_depth= 8,\\n    min_child_weight= 1,\\n    subsample= 0.8,\\n)\\n\\nxgb_pred = modeling(xgb_model, Xs, y, testing_s)',\n",
       " '924af5b4': \"param_grid = {\\n                   'n_estimators': np.arange(50,500,20),\\n                   'max_depth' : [i for i in range(1,15,1)],\\n#                    'gamma': [1,2,3,4],\\n#                    'reg_alpha': [0,1,2,3]\\n}\\nxgb_model = xgb.XGBClassifier(\\n\\n    colsample_bytree= 0.8,\\n    learning_rate= 0.001,\\n    min_child_weight= 1,\\n    subsample= 0.8,\\n)\\n\\ngrid_xgb_pred = g_search(xgb_model, param_grid, Xs, y, testing_s)\",\n",
       " '35830f5e': \"thesubmission = gender_submission.copy()\\nthesubmission['Survived'] = xgb_pred\\nthesubmission['Survived'].head(10)\",\n",
       " '66740633': \"thesubmission.to_csv('thesubmission.csv', index=False)\",\n",
       " '504670ef': \"# xgb_model = xgb.XGBClassifier(\\n\\n#     colsample_bytree= 0.8,\\n#     gamma= 1,\\n#     learning_rate= 0.002,\\n#     max_depth= 8,\\n#     min_child_weight= 1,\\n#     subsample= 0.8     \\n# )\\n\\n# Train Score 0.8709315375982043\\n\\n# CV scores:  [0.8547486  0.85393258 0.83707865 0.83707865 0.8258427 ]\\n# CV scores mean:  0.8417362375243236\\n\\n# First 10 Predictions: \\n#  [0 1 0 0 1 0 1 0 1 0]\\n\\n# features_drop = ['PassengerId','Name', 'Ticket', 'Survived','SibSp','Parch']\\n\\n# X_train, X_test, y_train, y_test = train_test_split(\\n#     Xs, y, test_size=.3,random_state=55, stratify=y) \\n\\n# Kaggle score: 0.79186\",\n",
       " '9da9fcb5': '### Embarked\\nFrom the data description wee see that \"Embarked\" (Port of Embarkation) contains 3 values: C, Q, S which means: C = Cherbourg, Q = Queenstown, S = Southampton<br>\\nAnd since there is only 2 null values in the train, We will fill them with the mode',\n",
       " 'c6d6b120': '## Features Selection',\n",
       " '48104aaf': \"Now let's take a look at the most important variables, which will have strong linear releationship with \\n<b>Survived</b> variable .<br><br>\",\n",
       " '6c761c48': '#### Family size to groups',\n",
       " 'f1424035': 'And from the above titles we can create a dictionary contains all titles meaning, Then we will create a new feature contains titles',\n",
       " '0b8a1552': '### Best model parameters',\n",
       " 'c26df6f2': 'We can see below that the data is imbalanced, And we will try different techniques to solve this problem',\n",
       " '0a0635b9': \"Most of the rows is null, Let's find out if we can gain some benefit\",\n",
       " 'd5c822d6': 'We can extract the title from \"Name\" feature',\n",
       " 'bc96d1bf': '#### Age groups',\n",
       " 'fdf633ee': '- The graph below shows that the number of deaths in males was a lot more than females',\n",
       " '39154dc0': '\\n\\n# &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Titanic\\n\\n\\n\\n',\n",
       " '8790249b': '### Dropping Some Features',\n",
       " '42064b08': '- ### These datasets include 11 explanatory variables:',\n",
       " 'b191086c': '**AdaBoost Classifier**',\n",
       " '746f63e4': '**The mean age by each Pclass.**',\n",
       " '4dac0d39': '# Exploring the Data',\n",
       " 'c48c99f5': '## Making several new features ',\n",
       " '248839e5': 'So we can put 0 for null values, and 1 for non null values ',\n",
       " 'bc2563c0': \"### Fare\\nThere is only one 'Fare' value missing in the test, And we will fill it with the mean grouped by Pclass\",\n",
       " '42c5e0c3': '### Submission',\n",
       " 'aa43ece3': '## Importing packages',\n",
       " 'f517ef22': '## One Hot Encoding\\nCreating Dummies For Categorical Features.',\n",
       " '3a5ee050': 'It says, 66% of passengers who have non-missing cabin values survived',\n",
       " 'bb890835': '**Number of Males and Females by Class**',\n",
       " 'f39be485': '## Check Missing Values',\n",
       " '965bd41d': '# Visual Analysis',\n",
       " '52b7d2fc': '# Feature Engineering',\n",
       " '6896fb40': '- The graph below shows that the death rate of males was higher than females\\n- The graph below shows that older passengers had less chance of survival.\\n<br><br>\\nTherefore we will create age groups',\n",
       " '00271fdd': '![kaggle_score.png](attachment:kaggle_score.png)',\n",
       " '812fe9f1': '## Smote sampling (worse results)',\n",
       " '87e95b4c': '## Oversampling (worse results)\\nTrying to balance the data by duplicating some rows for people who survived to be 549 survived and 549 dead',\n",
       " 'd93b71fa': '**Correlation with the target**',\n",
       " '48c9c272': '**Survived Correlation Matrix**',\n",
       " 'b23e5957': '## Apply modeling',\n",
       " 'c6f6611a': '**Number of Survivals and Deaths Per Gender**',\n",
       " '90fcad63': '- The graph below shows the number of deaths for people by there Family Size',\n",
       " '25c777c0': '## Making sure there is no null values',\n",
       " '39d5e315': '###  Title Feature',\n",
       " 'bb4a09e7': '- The graph below shows that most of men were in the 3rd class.  ',\n",
       " '5fb676e4': \"### Family Size Feature \\n(from adding 'SibSp' and  'Parch' togather)\",\n",
       " '04e876c3': ' ## Handle Missing Data',\n",
       " 'b30d0039': '**Number of Deaths by Family Size**',\n",
       " '58a60f01': '**Splitting and Standardizing Train Data to Obtain Test Scores**',\n",
       " '1c08c0d9': '**We fill them with the mean age with respect to each Pclass.**',\n",
       " '9cd5caaf': '- In this Kaggle competition, we aim to predict which passengers survived the Titanic shipwreck according to economic status (class), sex, age .\\n\\n- In this competition, we face a binary classification problem and we will try to solve it',\n",
       " 'a2dcbbd4': '## Undersampling (worse results)\\nTrying to balance the data by removing some rows for people who are dead to be 342 survived and 342 dead',\n",
       " 'd9af586a': '## Introduction',\n",
       " '14617469': '### Kaggle Score\\n(This is an old score because Titanic validation data on kaggle change every month)\\n<br>Old score: 0.80382\\n<br>Current: 0.79186',\n",
       " 'cdffba79': '### Age\\nBoth train and test data contains a lot of Age null values, And we will fill them with the mean age with respect to each Pclass, And then we will create age groups',\n",
       " 'beebcd1c': 'Train data have Survived (dependent variable) and other predictor variables.\\nTest data include the same variables that in train data, but without Survived (dependent variable) because this data will be submitted to kaggle.',\n",
       " '920104c6': '## Loading the Titanic',\n",
       " '03f2fa3e': '# Dealing with imbalanced data',\n",
       " '8dfc9296': '**Number of Survivals by Class**',\n",
       " 'ca20f5c9': '**BaggingClassifier with a decision tree base estimator**',\n",
       " 'a5c46c0a': '### Cabin',\n",
       " 'e7f074d3': '- The graph below shows that the number of survivals in the  3rd class was lower than 1st and 2nd class.',\n",
       " '2721e7db': '**Grid Search XGBOOST**',\n",
       " 'b8bc92eb': '-  #### Data Dictionary\\n\\n|Feature|Dataset|Description|\\n|-------|---|---|\\n|Survival|Train|The number of survived the Titanic shipwreck| \\n|Pclass|Train/Test|Economic status (class)| \\n|Sex|Train/Test|male or female.| \\n|Age|Train/Test|Age in years| \\n|Sibsp/Parch|Train/Test|The number of siblings, spouses, or children aboard the Titanic.| \\n|ticket|Train/Test|Ticket number.| \\n|Fare|Train/Test|Passenger fare| \\n|Cabin|Train/Test|Cabin number| \\n|Embarked|Train/Test|Port of Embarkation| \\n\\n',\n",
       " 'fb7db714': '![g.jpg](attachment:g.jpg)',\n",
       " 'b2c12d4c': '## XGBoost (best result)',\n",
       " 'cf5f068a': '# Modeling ',\n",
       " 'c28f0218': \"# Adding imports that will be used\\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# The following code takes all files in the given directory and print the paths of the files\\n\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\",\n",
       " '6630e7d4': 'from IPython.core.debugger import set_trace',\n",
       " '93a0e603': '# Definition of the tuples that will be read from the input files. These tuples of three positions represent the item index, its value and its weight\\nimport sys\\nimport csv\\ncsv.field_size_limit(sys.maxsize)\\nfrom collections import namedtuple\\nItem = namedtuple(\"Item\", [\\'index\\', \\'value\\', \\'weight\\'])',\n",
       " '4bd9d5dd': '# This method checks that the solution is a valid solution. It is that the selected list of items does not weight more than the knapsack capacity\\ndef check_solution(capacity, items, taken):\\n    weight = 0\\n    value = 0\\n    for item in items:\\n        if taken[item.index]== 1:\\n            weight += item.weight\\n            value += item.value\\n    if weight> capacity:\\n        print(\"solución incorrecta, se supera la capacidad de la mochila (capacity, weight):\", capacity, weight)\\n        return 0\\n    return value',\n",
       " '27ee8d3a': \"def from_data_to_items(input_data):\\n    lines = input_data.split('\\\\n')\\n\\n    firstLine = lines[0].split()\\n    item_count = int(firstLine[0])\\n    capacity = int(firstLine[1])\\n\\n    items = []\\n\\n    for i in range(1, item_count+1):\\n        line = lines[i]\\n        parts = line.split()\\n        items.append(Item(i-1, int(parts[0]), int(parts[1])))\\n    \\n    return items, capacity\",\n",
       " 'caf72eda': 'class_example =\"\"\"3 10\\n        45 5\\n        48 8\\n        35 3\\n        \"\"\"',\n",
       " '9325d3ba': 'items, capacity = from_data_to_items(class_example)',\n",
       " '90bdfbb7': 'items',\n",
       " 'de9229ef': 'from pprint import pformat\\n\\nclass Node:\\n    def __init__(self, index, path, value, room):\\n        self.index = index\\n        self.path = path\\n        self.value = value\\n        self.room = room\\n        \\n    def __repr__(self):\\n        return pformat(vars(self))\\n        \\n    def estimate(self, items):\\n        return self.value + sum(item.value for item in items[self.index:])',\n",
       " '06775aa9': 'def solve_branch_and_bound(items, capacity):\\n    root_node = Node(0, [], 0, capacity)\\n    alive = []\\n    alive.append(root_node)\\n    \\n    best_value = 0\\n    \\n    while (len(alive)>0):\\n        current = alive.pop()\\n            \\n        current_estimate = current.estimate(items)\\n        \\n        if current_estimate <= best_value:\\n            continue\\n        \\n        if current.value > best_value:\\n            best_value = current.value\\n            best = current\\n        \\n        if current.index >= len(items):\\n            continue\\n            \\n        right_node = Node(current.index+1, current.path.copy(), current.value, current.room)\\n        alive.append(right_node)\\n        \\n        enough_room = current.room-items[current.index].weight\\n        if enough_room>0:\\n            left_path = current.path.copy()\\n            left_path.append(current.index) #xi =0\\n            left_node = Node(current.index+1, \\n                         left_path, \\n                         current.value + items[current.index].value,\\n                         enough_room)\\n            alive.append(left_node)\\n        \\n        \\n    taken = [0]*len(items)\\n        \\n    for i in best.path:\\n        taken[items[i].index]= 1\\n            \\n        \\n    return best_value, taken',\n",
       " '600cb994': 'def solve_naive(items, capacity):\\n    # a trivial greedy algorithm for filling the knapsack\\n    # it takes items in-order until the knapsack is full\\n    value = 0\\n    weight = 0\\n    taken = [0]*len(items)\\n\\n    for item in items:\\n        if weight + item.weight <= capacity:\\n            taken[item.index] = 1\\n            value += item.value\\n            weight += item.weight\\n            \\n    return value, taken\\n    ',\n",
       " 'd7687ee6': 'from nose.tools import *',\n",
       " '1b7d3038': 'def test_solution_taken():\\n    capacity = 10\\n    items = [Item(index=0, value=45, weight=5),\\n                                 Item(index=1, value=48, weight=8),\\n                                 Item(index=2, value=35, weight=3)]\\n\\n    value, taken = solve_branch_and_bound(items, capacity)\\n    assert_equal(taken, [1, 0, 1])',\n",
       " '32a13b76': 'test_solution_taken()',\n",
       " 'd9bdda47': \"# This function takes input data that describes a specific problem of TSP and solve it\\ndef solve_it(input_data):\\n    # Modify this code to run your optimization algorithm\\n    # parse the input\\n    \\n    items, capacity = from_data_to_items(input_data)\\n    \\n    #value, taken = solve_naive(items, capacity)\\n    \\n    value, taken = solve_branch_and_bound(items, capacity)\\n            \\n    # prepare the solution in the specified output format\\n    output_data = str(value) + ' ' + str(0) + '\\\\n'\\n    output_data += ' '.join(map(str, taken))\\n    return output_data, check_solution(capacity, items, taken)\",\n",
       " '43b4968e': '# For each input file, solve_it is called and the result serialized in the ouputs for kaggle and moodle\\nstr_output_kaggle = [[\"Filename\",\"Max_value\"]]\\nstr_output_moodle = [[\"Filename\",\"Max_value\", \"Solution\"]]\\n\\nfilter_filenames = [\\'ks_19_0\\']\\n\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        if filename in filter_filenames:\\n            full_name = dirname+\\'/\\'+filename\\n            with open(full_name, \\'r\\') as input_data_file:\\n                input_data = input_data_file.read()\\n                output, value = solve_it(input_data)\\n                print(full_name, value)\\n                str_output_kaggle.append([filename,str(value)])\\n                str_output_moodle.append([filename,str(value), output.split(\\'\\\\n\\')[1]])',\n",
       " 'd8831af8': 'output',\n",
       " 'db5f48b9': \"from IPython.display import FileLink\\ndef submission_generation(filename, str_output):\\n    os.chdir(r'/kaggle/working')\\n    with open(filename, 'w', newline='') as file:\\n        writer = csv.writer(file)\\n        for item in str_output:\\n            writer.writerow(item)\\n    return  FileLink(filename)\",\n",
       " '4c958951': \"submission_generation('NAME_starter_kaggle.csv', str_output_kaggle)\",\n",
       " '5ec18380': '# The file generated by this method must be uploaded in the task of the \"campus virtual\". The file to upload in the \"campus virtual\" must be the one related to one submitted to Kaggle. That is, both submitted files must be generated in the same run\\nsubmission_generation(\\'NAME_starter_moodle.csv\\', str_output_moodle)',\n",
       " '3e047252': '![imagen.png](attachment:imagen.png)',\n",
       " 'b97767fe': \"ks_19_0: '12248 0\\\\n0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0'**\",\n",
       " '394b2536': '![imagen.png](attachment:imagen.png)',\n",
       " 'b31c37db': \"Ejemplo de definición de test de unidad en Jupyter, a diferencia de pyCharm, donde usábamos 'import unittest', en Jupyter vamos a usar nose.\\n\\nEn Jupyter, nótese la diferencia entre *self.assertEqual(...)* en pyCharm y '*assert_equal(...)*'\",\n",
       " '9e358158': '![imagen.png](attachment:imagen.png)',\n",
       " '82218899': '![imagen.png](attachment:imagen.png)',\n",
       " 'ad1a68d4': 'import pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n%matplotlib inline',\n",
       " '2bc5fb09': \"df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\",\n",
       " '159d1c08': 'df.info()',\n",
       " 'ea78869c': 'df.head()',\n",
       " '0bfaf060': \"# Use seaborn on the dataframe to create a pairplot with the hue indicated by the Outcome column.\\n# It is very large plot\\nsns.pairplot(df,hue='Outcome',palette='coolwarm');\",\n",
       " '15643fe8': 'sns.pairplot(df)',\n",
       " 'c0858ec9': '# import the main KNN ilibrary\\nfrom sklearn.preprocessing import StandardScaler',\n",
       " '8c513e58': '# StandardScaler() object called scaler\\nscaler = StandardScaler()',\n",
       " '0dbe3f29': \"# Fit the scaler to the features\\nscaler.fit(df.drop('Outcome',axis=1))\",\n",
       " '7fc28ddc': \"# Transform the features to a scaled version \\nscaled_features = scaler.transform(df.drop('Outcome',axis=1))\",\n",
       " '65cc98ed': '# Convert the scaled features to a dataframe \\ndf_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\\ndf_feat.head()',\n",
       " 'a3ec418d': \"# Train and test split\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(scaled_features,df['Outcome'],\\n                                                    test_size=0.30)\",\n",
       " 'a5750bc8': '# Create a KNN model instance with n_neighbors=1# \\n\\nfrom sklearn.neighbors import KNeighborsClassifier\\nknn = KNeighborsClassifier(n_neighbors=1)\\nknn.fit(X_train,y_train)',\n",
       " '0e67a114': '# Fit this KNN model to the training data\\n\\npred = knn.predict(X_test)',\n",
       " 'e87b8508': 'from sklearn.metrics import classification_report,confusion_matrix',\n",
       " '70cd3bbd': 'print(confusion_matrix(y_test,pred))',\n",
       " 'dca56978': 'print(classification_report(y_test,pred))',\n",
       " '728c2b2c': 'error_rate = []\\n\\nfor i in range(1,40):\\n    \\n    knn = KNeighborsClassifier(n_neighbors=i)\\n    knn.fit(X_train,y_train)\\n    pred_i = knn.predict(X_test)\\n    error_rate.append(np.mean(pred_i != y_test))',\n",
       " '40018fec': \"plt.figure(figsize=(10,6))\\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\\n         markerfacecolor='red', markersize=10)\\nplt.title('Error Rate vs. K Value')\\nplt.xlabel('K')\\nplt.ylabel('Error Rate')\",\n",
       " 'fe8d465d': \"#After that we choose some K Value for available algorihmas value\\n# Retrain with new K Value\\nknn = KNeighborsClassifier(n_neighbors=1)\\n\\nknn.fit(X_train,y_train)\\npred = knn.predict(X_test)\\n\\nprint('WITH K=1')\\nprint('\\\\n')\\nprint(confusion_matrix(y_test,pred))\\nprint('\\\\n')\\nprint(classification_report(y_test,pred))\",\n",
       " 'e5c6105a': \"knn = KNeighborsClassifier(n_neighbors=23)\\n\\nknn.fit(X_train,y_train)\\npred = knn.predict(X_test)\\n\\nprint('WITH K=23')\\nprint('\\\\n')\\nprint(confusion_matrix(y_test,pred))\\nprint('\\\\n')\\nprint(classification_report(y_test,pred))\",\n",
       " '84762508': '# Predictions',\n",
       " 'bb270083': '# How to choosing a K Value',\n",
       " '473e430f': '# Using KNN',\n",
       " '71181d6d': '# EDA',\n",
       " 'd442a546': '# Standardize the Variables',\n",
       " 'e2c26661': \"import matplotlib.pyplot as plt\\nimport numpy as np\\n\\ndef read():\\n    ''' reads a field of numbers from a textfile.'''\\n    with open('/kaggle/input/samples/03_samples.txt') as file:\\n        lines = file.readlines()\\n    # convert each line from string to sequence of floats. \\n    X = list(map(lambda x: [float(t) for t in x.split()], lines))\\n    X = np.array(X) \\n    return X\\n\\ndef plot(X):\\n    _ = plt.scatter(X[:,0],X[:,1],s=5)\\n    plt.xlim([0,10]); plt.ylim([-10,30])\\n    plt.grid(); plt.show()\\n\\nX = read()\\nprint('X is a dataset with %s rows and %d columns.' %X.shape)\\nplot(X)\",\n",
       " '9e8b9d5f': 'def cov(X):\\n    pass # FIXME\\n\\nprint(cov(X))',\n",
       " '8a587ca9': 'def corr(X):\\n    pass # FIXME\\n\\nprint(corr(X))',\n",
       " 'e537c02e': \"import pandas\\ncars = pandas.read_csv('/kaggle/input/carscsv/cars.csv')\\n\\n# columns = Spalten-Namen\\ncolumns = cars.columns\\n\\n# data = die eigentlichen Daten\\ndata = cars.values.astype('float')\\n# Wir filtern alle Autos die teurer als 100K EUR sind.\\n# (man kann's zum Spaß mal ohne diesen Filter probieren)\\ndata = data[data[:,0]<100000,:]\\n\\n# Korrelationen berechnen\\ncorrs = corr(data)\\n\\n# Merkmale und Korrelationen ausgeben.\\n# FIXME\",\n",
       " '2bab8ecd': '## 1. Vorbereitung\\nDie folgende Zelle liest eine bivariate Stichprobe aus einer Textdatei ein und plottet die Daten. Führen Sie die Zelle aus.\\n**Sie müssen den Code in der Zelle nicht ändern!**',\n",
       " '9e7b02d3': '## 3. Korrelation\\nSchreiben Sie eine weitere Methode *corr()*, die eine ähnliche Matrix wie die Kovarianzmatrix zurückliefert. Die Matrix sollte aber statt den Kovarianzen die **Korrelationen** zwischen den Merkmalen enthalten.\\n\\n*Hinweis: Verwenden Sie das Ergebnis von cov() als Zwischenergebnis und normalisieren Sie die Einträge zu Korrelationen, indem Sie durch die Standardabweichungen teilen (die zugehörigen Varianzen finden Sie auf der Diagonalen der Kovarianzmatrix).*',\n",
       " '3346d9de': '## 2. Kovarianzmatrix schätzen\\nNotieren Sie auf einem Zettel eine Schätzung für die Einträge der Kovarianzmatrix $\\\\Sigma$ der Stichprobe, basierend auf dem obigen Plot.\\n\\n<img src=\"https://c8.alamy.com/comp/2B5DC6H/handwritten-math-calculations-on-a-paper-note-pad-2B5DC6H.jpg\" width=\"300\">',\n",
       " '72e43ed0': ' # Kovarianz und Korrelation\\nIn diesem Notebook werden wir Kovarianzen und Korrelationen berechnen und praktisch anwenden. Machen Sie sich hierzu - soweit noch nicht geschehen - noch einmal mit den Definitionen aus der Vorlesung vertraut, insbesondere der **Kovarianzmatrix**.\\n',\n",
       " 'e6c08099': '## 3. Kovarianzmatrix berechnen\\n \\nSchreiben Sie nun eine ***eigene numpy-Methode*** cov(), die die Kovarianzmatrix $\\\\Sigma$ einer Stichprobe $X$ als Numpy-Array zurückliefert. \\n\\nDeckt sich das Ergebnis Ihrer Methode mit Ihrer Schätzung oben?',\n",
       " 'c58a331a': '## 4. Korrelationen auf Ebay-Autos\\nDer folgende Code liest unsere Ebay-Auto-Daten ein, und filtert sämtliche Autos mit einem Preis von mehr als 100,000 Autos *(diese sind als Ausreißer zu betrachten)*.\\n\\nBerechnen Sie nun mit Ihrer Methode *corr()* die Korrelationsmatrix. Geben Sie für sämtliche Merkmale die **Korrelation zum Merkmal Preis** (Spalte 0) aus. Welche Merkmale beeinflussen den Preis scheinbar besonders positiv/negativ?',\n",
       " '18542313': '# Import PyStackNet Package\\n# Source: https://www.kaggle.com/kirankunapuli/pystacknet\\nimport os\\nimport sys\\nsys.path.append(\"../input/pystacknet/repository/h2oai-pystacknet-af571e0\")\\nimport pystacknet',\n",
       " '6ffc5e38': \"# Standard Dependencies\\nimport gc\\nimport sys\\nimport time\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import roc_auc_score, roc_curve\\n\\n# Machine Learning\\nfrom lightgbm import LGBMRegressor\\nfrom catboost import CatBoostRegressor\\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\\nfrom sklearn.linear_model import Ridge\\n\\n# Specify paths and key features\\nKAGGLE_DIR = '../input/ieee-fraud-detection/'\\nTARGET = 'isFraud'\\n\\n# Seed for reproducability\\nseed = 1234\\nnp.random.seed(seed)\\n\\n# For keeping time. Limit for Kaggle kernels is set to approx. 9 hours.\\nt_start = time.time()\",\n",
       " 'b6ef8fc1': \"# File sizes and specifications\\nprint('\\\\n# Files and file sizes:')\\nfor file in os.listdir(KAGGLE_DIR):\\n    print('{}| {} MB'.format(file.ljust(30), \\n                             str(round(os.path.getsize(KAGGLE_DIR + file) / 1000000, 2))))\",\n",
       " '780f868b': 'def auc_score(y_true, y_pred):\\n    \"\"\"\\n    Calculates the Area Under ROC Curve (AUC)\\n    \"\"\"\\n    return roc_auc_score(y_true, y_pred)',\n",
       " '6749f71f': 'def plot_curve(y_true_train, y_pred_train, y_true_val, y_pred_val, model_name):\\n    \"\"\"\\n    Plots the ROC Curve given predictions and labels\\n    \"\"\"\\n    fpr_train, tpr_train, _ = roc_curve(y_true_train, y_pred_train, pos_label=1)\\n    fpr_val, tpr_val, _ = roc_curve(y_true_val, y_pred_val, pos_label=1)\\n    plt.figure(figsize=(8, 8))\\n    plt.plot(fpr_train, tpr_train, color=\\'black\\',\\n             lw=2, label=f\"ROC train curve (AUC = {round(roc_auc_score(y_true_train, y_pred_train), 4)})\")\\n    plt.plot(fpr_val, tpr_val, color=\\'darkorange\\',\\n             lw=2, label=f\"ROC validation curve (AUC = {round(roc_auc_score(y_true_val, y_pred_val), 4)})\")\\n    plt.plot([0, 1], [0, 1], color=\\'navy\\', lw=2, linestyle=\\'--\\')\\n    plt.xlim([-0.05, 1.05])\\n    plt.ylim([-0.05, 1.05])\\n    plt.xlabel(\\'False Positive Rate\\', fontsize=16)\\n    plt.ylabel(\\'True Positive Rate\\', fontsize=16)\\n    plt.xticks(fontsize=14)\\n    plt.yticks(fontsize=14)\\n    plt.title(f\\'ROC Plot for {model_name}\\', weight=\"bold\", fontsize=20)\\n    plt.legend(loc=\"lower right\", fontsize=16)',\n",
       " '9f721e5a': '# Load in datasets\\ntrain_transaction = pd.read_csv(f\"{KAGGLE_DIR}train_transaction.csv\", index_col=\\'TransactionID\\')\\ntest_transaction = pd.read_csv(f\"{KAGGLE_DIR}test_transaction.csv\", index_col=\\'TransactionID\\')\\ntrain_identity = pd.read_csv(f\"{KAGGLE_DIR}train_identity.csv\", index_col=\\'TransactionID\\')\\ntest_identity = pd.read_csv(f\"{KAGGLE_DIR}test_identity.csv\", index_col=\\'TransactionID\\')\\n\\n# Merge datasets into full training and test dataframe\\ntrain_df = train_transaction.merge(train_identity, how=\\'left\\', left_index=True, right_index=True).reset_index()\\ntest_df = test_transaction.merge(test_identity, how=\\'left\\', left_index=True, right_index=True).reset_index()\\ndel train_identity, train_transaction, test_identity, test_transaction\\ngc.collect()',\n",
       " 'd749b564': '# Select only first 52 features\\n# The other columns are quite noisy\\ntrain_df = train_df.iloc[:, :53]\\ntest_df = test_df.iloc[:, :52]',\n",
       " 'de12ab5e': 'def reduce_mem_usage(df):\\n    \"\"\"\\n    Reduces memory usage for all columns in a Pandas DataFrame\\n    \"\"\"\\n    start_mem_usg = df.memory_usage().sum() / 1024**2 \\n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\\n    NAlist = [] # Keeps track of columns that have missing values filled in. \\n    for col in df.columns:\\n        if df[col].dtype != object:  # Exclude strings                       \\n            # make variables for Int, max and min\\n            IsInt = False\\n            mx = df[col].max()\\n            mn = df[col].min()\\n            # Integer does not support NA, therefore, NA needs to be filled\\n            if not np.isfinite(df[col]).all(): \\n                NAlist.append(col)\\n                df[col].fillna(mn-1,inplace=True)  \\n                   \\n            # test if column can be converted to an integer\\n            asint = df[col].fillna(0).astype(np.int32)\\n            result = (df[col] - asint)\\n            result = result.sum()\\n            if result > -0.01 and result < 0.01:\\n                IsInt = True            \\n            # Make Integer/unsigned Integer datatypes\\n            if IsInt:\\n                if mn >= 0:\\n                    if mx < 255:\\n                        df[col] = df[col].astype(np.uint8)\\n                    elif mx < 65535:\\n                        df[col] = df[col].astype(np.uint16)\\n                    else:\\n                        df[col] = df[col].astype(np.uint32)\\n                else:\\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\\n                        df[col] = df[col].astype(np.int8)\\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\\n                        df[col] = df[col].astype(np.int16)\\n                    else:\\n                        df[col] = df[col].astype(np.int32)\\n            # Make float datatypes 32 bit\\n            else:\\n                df[col] = df[col].astype(np.float32)\\n\\n    # Print final result\\n    mem_usg = df.memory_usage().sum() / 1024**2 \\n    print(\"Memory usage of properties dataframe is after reduction is:\",mem_usg,\" MB\")\\n    return df, NAlist',\n",
       " 'd8581cea': '# Reduce memory\\ntrain_df, _ = reduce_mem_usage(train_df)\\ntest_df, _ = reduce_mem_usage(test_df)',\n",
       " '05052bf5': \"# Drop nuisance columns and specify target variable\\nX_train = train_df.drop([TARGET, 'TransactionID', 'TransactionDT'], axis=1)\\nX_test = test_df.drop(['TransactionID', 'TransactionDT'], axis=1)\\ntarget = train_df[TARGET]\\n\\n# Label Encoding\\nlbl = LabelEncoder()\\nfor f in X_train.columns:\\n    if X_train[f].dtype == 'object': \\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\\n        X_train[f] = lbl.transform(list(X_train[f].values))\\n        X_test[f] = lbl.transform(list(X_test[f].values))   \",\n",
       " '84fee3ec': '# Split Train and Validation\\nX_train, X_val, y_train, y_val = train_test_split(X_train,\\n                                                  target,\\n                                                  test_size=0.15, \\n                                                  random_state=seed, \\n                                                  stratify=target)',\n",
       " 'f4480a33': '# Level 1 are the base models that take the training dataset as input\\nl1_clf1 = GradientBoostingRegressor(n_estimators=400,\\n                                    learning_rate=0.006,\\n                                    min_samples_leaf=10,\\n                                    max_depth=20, \\n                                    max_features=\\'sqrt\\', \\n                                    subsample=0.85,\\n                                    random_state=seed)\\n\\nl1_clf2 = LGBMRegressor(boosting_type=\\'gbdt\\',\\n                        objective=\"binary\",\\n                        metric=\"AUC\",\\n                        boost_from_average=\"false\",\\n                        learning_rate=0.007,\\n                        num_leaves=491,\\n                        max_depth=25,\\n                        min_child_weight=0.035,\\n                        feature_fraction=0.38,\\n                        bagging_fraction=0.42,\\n                        min_data_in_leaf=100,\\n                        max_bin=255,\\n                        importance_type=\\'split\\',\\n                        reg_alpha=0.4,\\n                        reg_lambda=0.65,\\n                        bagging_seed=seed,\\n                        random_state=seed,\\n                        verbosity=-1,\\n                        subsample=0.85,\\n                        colsample_bytree=0.8,\\n                        min_child_samples=79)\\n\\nl1_clf3 = CatBoostRegressor(learning_rate=0.2,\\n                            bagging_temperature=0.1, \\n                            l2_leaf_reg=30,\\n                            depth=12, \\n                            max_bin=255,\\n                            iterations=100,\\n                            loss_function=\\'Logloss\\',\\n                            objective=\\'RMSE\\',\\n                            eval_metric=\"AUC\",\\n                            bootstrap_type=\\'Bayesian\\',\\n                            random_seed=seed,\\n                            early_stopping_rounds=10)\\n\\n# Level 2 models will take predictions from level 1 models as input\\n# Remember to keep level 2 models smaller\\n# Basic models like Ridge Regression with large regularization or small random forests work well\\nl2_clf1 = Ridge(alpha=0.001, normalize=True, random_state=seed)',\n",
       " 'f5b5126b': '# Specify model tree for StackNet\\nmodels = [[l1_clf1, l1_clf2, l1_clf3], # Level 1\\n          [l2_clf1]] # Level 2',\n",
       " 'dc3dd984': 'from pystacknet.pystacknet import StackNetClassifier\\n\\n# Specify parameters for stacked model and begin training\\nmodel = StackNetClassifier(models, \\n                           metric=\"auc\", \\n                           folds=3,\\n                           restacking=False,\\n                           use_retraining=True,\\n                           use_proba=True, # To use predict_proba after training\\n                           random_state=seed,\\n                           n_jobs=-1, \\n                           verbose=1)\\n\\n# Fit the entire model tree\\nmodel.fit(X_train, y_train)',\n",
       " 'a0fc4bc4': '# Get score on training set and validation set for our StackNetClassifier\\ntrain_preds = model.predict_proba(X_train)[:, 1]\\nval_preds = model.predict_proba(X_val)[:, 1]\\ntrain_score = auc_score(y_train, train_preds)\\nval_score = auc_score(y_val, val_preds)',\n",
       " 'ad30b48f': 'print(f\"StackNet AUC on training set: {round(train_score, 4)}\")\\nprint(f\"StackNet AUC on validation set: {round(val_score, 4)}\")',\n",
       " 'ab602931': '# Plot ROC curve\\nplot_curve(y_train, train_preds, y_val, val_preds, \"StackNet Baseline\")',\n",
       " '17d8d0b1': '# Write predictions to csv\\nsub = pd.read_csv(f\"{KAGGLE_DIR}sample_submission.csv\")\\npreds = model.predict_proba(X_test)[:, 1]\\nsub[TARGET] = preds\\nsub.to_csv(f\"submission.csv\", index=False)',\n",
       " '6dc66f70': '# Check Submission format\\nprint(\"Final Submission Format:\")\\nsub.head()',\n",
       " 'b78753c0': 'plt.figure(figsize=(12,4))\\nplt.hist(train_df[TARGET], bins=100)\\nplt.title(\"Distribution for train set\", weight=\\'bold\\', fontsize=18)\\nplt.xlabel(\"Predictions\", fontsize=15)\\nplt.ylabel(\"Frequency\", fontsize=15)\\nplt.xlim(0, 1);',\n",
       " 'd0422737': 'plt.figure(figsize=(12,4))\\nplt.hist(sub[TARGET], bins=100)\\nplt.title(\"Prediction Distribution for test set\", weight=\\'bold\\', fontsize=18)\\nplt.xlabel(\"Predictions\", fontsize=15)\\nplt.ylabel(\"Frequency\", fontsize=15)\\nplt.xlim(0, 1);',\n",
       " '77bd06dd': \"# Check kernels run-time. Limit for Kaggle Kernels is set to approx. 9 hours.\\nt_finish = time.time()\\ntotal_time = round((t_finish-t_start) / 3600, 4)\\nprint('Kernel runtime = {} hours ({} minutes)'.format(total_time, \\n                                                      int(total_time*60)))\",\n",
       " '0c879d37': 'To check if the predictions are sound, we check the format of our submission and compare our prediction distribution with that of the target distribution in the training set.',\n",
       " 'a75252bb': '## Modeling <a id=\"4\"></a>',\n",
       " '82c1ebfd': \"In this kernel we will take a look on how to use StackNet to stack multiple levels of models in order to efficiently blend models. StackNet is a powerful package that works really well for competitions! We are going to stack a random forest on top of 3 GBM models as an example. We will use data from the [IEEE Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection) competition to explain StackNet.\\n\\nStackNet was created by Kaggle Grandmaster Marios Michailidis ([kazanova](https://www.kaggle.com/kazanova)) as part of his PhD. Thanks to [Kiran Kunapuli](https://www.kaggle.com/kirankunapuli) for uploading the package as [a Kaggle dataset](https://www.kaggle.com/kirankunapuli/pystacknet) so it can conveniently be used with Kaggle kernels.\\n\\nLet's dive in!\",\n",
       " '880d5855': 'To plot the ROC curve we will use a function using sklearn and matplotlib. An example of this visualization is shown in the evaluation section of this kernel.',\n",
       " 'e73f0f7b': '## Table Of Contents',\n",
       " 'b1bf1871': '![](https://github.com/kaz-Anova/StackNet/raw/master/images/StackNet_Logo.png?raw=true)',\n",
       " 'bd2865ea': 'Image: An example of an ROC curve. AOC is a typo and should be AUC.\\n\\n![](https://miro.medium.com/max/722/1*pk05QGzoWhCgRiiFbz-oKQ.png)',\n",
       " '582bc06e': '## Evaluation <a id=\"5\"></a>',\n",
       " '6668d065': '## Data Preparation <a id=\"3\"></a>',\n",
       " 'decc8d63': 'Try to experiment with [StackNet](https://github.com/h2oai/pystacknet) yourself. The possibilities are almost endless!\\n\\nIf you want to check out another solution using PyStackNet, check out [this Kaggle kernel on the Titanic dataset by Yann Berthelot](https://www.kaggle.com/yannberthelot/pystacknet-working-implementation).\\n\\n**If you like this Kaggle kernel, feel free to give an upvote and leave a comment! I will try to implement your suggestions in this kernel!**',\n",
       " '3acea5fe': 'The blue line signifies the baseline AUC which is 0.5. The final validation score is the area under the orange curve, which is mentioned in the plot.',\n",
       " '3ba63860': '## Metric (AUC) <a id=\"2\"></a>',\n",
       " 'bfd6738c': \"StackNet allows you to define all kinds of models. For example, Sklearn models, LightGBM, XGBoost, CatBoost and Keras models can all be used with StackNet.\\n\\nFor the individual models, you are responsible for not overfitting. Therefore, it is advisable to first experiment with individual models and make sure they are sound, before combining them into StackNet. For this example we will use [sklearn's Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), a [LightGBM Regressor](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor) and a [CatBoost Regressor](https://catboost.ai/docs/concepts/python-reference_catboostregressor.html) in the 1st level. Then we will train a [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) in level 2, which takes the predictions of the models in the 1st level as input. StackNet takes care of the stacking and cross validation.\",\n",
       " '51510569': '## Dependencies <a id=\"1\"></a>',\n",
       " '5d0fbb9b': '## Ensembling With StackNet',\n",
       " '405f2644': '## Submission <a id=\"6\"></a>',\n",
       " 'e89ad790': 'The model is compiled and fitted through the a familiar sklearn-like API. The StackNetClassifier will perform cross-validation (CV) and will output the CV scores for each model. To make sure we can output a probability of fraud we specify \"use_proba=True\".\\n\\nThe \"folds\" argument in StackNetClassifier can also accept an iterable of train/test splits. Since the target distibution is imbalanced you can probably improve on the CV strategy by first yielding stratified train/test split with for example [sklearn\\'s StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html).',\n",
       " 'dd4ad524': 'The Metric used in this competition is \"[Area Under ROC Curve (AUC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\". We create this curve by plotting the true positive rate (TPR) against the false positive rate (FPR) at different threshold settings. \\nThis is very convenient since with binary classification problems like fraud detection the accuracy score is not that informative. For example, if we predict only 0 (not fraud) on this dataset, then we will get an accuracy score of 0.965. The AUC score will be 0.5 (no better than random). All naive baselines will get an AUC score of approximately 0.5.\\n\\nTo calculate the AUC score we can use [sklearn\\'s roc_auc_score function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) straight out of the box.',\n",
       " 'db7b3eb2': \"StackNet does not accept missing values (NaN's), Infinity values (inf) or values higher than 32 bytes (for example float64 or int64). Therefore, we have to fill in missing values and compress certain columns as the Pandas standard is 64 bytes. Big thanks to [Arjan Groen](https://www.kaggle.com/arjanso) for creating this convenient function. The function is taken from [this Kaggle kernel](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65).\",\n",
       " '86d73c60': '- [Dependencies](#1)\\n- [Metric (AUC)](#2)\\n- [Data Preparation](#3)\\n- [Modeling](#4)\\n- [Evaluation](#5)\\n- [Submission](#6)',\n",
       " 'ab5d9249': 'The model tree that StackNet takes as input is a list of lists. The 1st list defines the 1st level, the 2nd one the 2nd level, etc. You can build a model tree of arbitrary depth and width.',\n",
       " '144e4d2a': 'Since this kernel is meant to explain StackNet and establish a baseline we will not go into advanced feature engineering and EDA here. However, your performance will greatly benefit from feature engineering so I encourage you to explore it. A good kernel which does that for this competition can be found [here](https://www.kaggle.com/kyakovlev/ieee-gb-2-make-amount-useful-again?scriptVersionId=18874747).',\n",
       " 'f8e500d8': 'import numpy as np # linear algebra\\nimport pandas as pd # data processing\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n# Input data files are available in the \"../input/\" directory.\\nimport os\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))',\n",
       " '79373c2f': \"netflix=pd.read_csv('../input/netflix-shows/netflix_titles.csv')\",\n",
       " 'f09b627f': 'netflix.head(10)',\n",
       " '22a2a575': 'netflix.shape',\n",
       " 'e59b010f': 'netflix.columns',\n",
       " '949b542c': 'netflix.isnull().sum()',\n",
       " '61dc05a5': 'netflix.nunique()',\n",
       " '81f49b0c': 'netflix.duplicated().sum()',\n",
       " '15cc6fae': 'df = netflix.copy()',\n",
       " 'd92c070f': 'df.shape',\n",
       " '42744944': 'df=df.dropna()\\ndf.shape',\n",
       " '6ee13fa4': 'df.head(10)',\n",
       " '97ed4563': 'df[\"date_added\"] = pd.to_datetime(df[\\'date_added\\'])\\ndf[\\'day_added\\'] = df[\\'date_added\\'].dt.day\\ndf[\\'year_added\\'] = df[\\'date_added\\'].dt.year\\ndf[\\'month_added\\']=df[\\'date_added\\'].dt.month\\ndf[\\'year_added\\'].astype(int);\\ndf[\\'day_added\\'].astype(int);',\n",
       " 'ea1b74c2': 'df.head(10)',\n",
       " 'ab156d12': \"sns.countplot(netflix['type'])\\nfig = plt.gcf()\\nfig.set_size_inches(10,10)\\nplt.title('Type')\",\n",
       " '5056d1c0': 'sns.countplot(netflix[\\'rating\\'])\\nsns.countplot(netflix[\\'rating\\']).set_xticklabels(sns.countplot(netflix[\\'rating\\']).get_xticklabels(), rotation=90, ha=\"right\")\\nfig = plt.gcf()\\nfig.set_size_inches(13,13)\\nplt.title(\\'Rating\\')',\n",
       " '35e03783': \"plt.figure(figsize=(10,8))\\nsns.countplot(x='rating',hue='type',data=netflix)\\nplt.title('Relation between Type and Rating')\\nplt.show()\",\n",
       " 'ac6edf9d': \"labels = ['Movie', 'TV show']\\nsize = netflix['type'].value_counts()\\ncolors = plt.cm.Wistia(np.linspace(0, 1, 2))\\nexplode = [0, 0.1]\\nplt.rcParams['figure.figsize'] = (9, 9)\\nplt.pie(size,labels=labels, colors = colors, explode = explode, shadow = True, startangle = 90)\\nplt.title('Distribution of Type', fontsize = 25)\\nplt.legend()\\nplt.show()\",\n",
       " '6e73a8d7': \"netflix['rating'].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,figsize=(10,8))\\nplt.show()\",\n",
       " '65ab5124': 'from wordcloud import WordCloud',\n",
       " '5a94e016': 'plt.subplots(figsize=(25,15))\\nwordcloud = WordCloud(\\n                          background_color=\\'white\\',\\n                          width=1920,\\n                          height=1080\\n                         ).generate(\" \".join(df.country))\\nplt.imshow(wordcloud)\\nplt.axis(\\'off\\')\\nplt.savefig(\\'country.png\\')\\nplt.show()',\n",
       " '697a1dfa': 'plt.subplots(figsize=(25,15))\\nwordcloud = WordCloud(\\n                          background_color=\\'white\\',\\n                          width=1920,\\n                          height=1080\\n                         ).generate(\" \".join(df.cast))\\nplt.imshow(wordcloud)\\nplt.axis(\\'off\\')\\nplt.savefig(\\'cast.png\\')\\nplt.show()',\n",
       " 'd9465387': 'plt.subplots(figsize=(25,15))\\nwordcloud = WordCloud(\\n                          background_color=\\'white\\',\\n                          width=1920,\\n                          height=1080\\n                         ).generate(\" \".join(df.director))\\nplt.imshow(wordcloud)\\nplt.axis(\\'off\\')\\nplt.savefig(\\'director.png\\')\\nplt.show()',\n",
       " 'ac32dbd5': 'plt.subplots(figsize=(25,15))\\nwordcloud = WordCloud(\\n                          background_color=\\'white\\',\\n                          width=1920,\\n                          height=1080\\n                         ).generate(\" \".join(df.listed_in))\\nplt.imshow(wordcloud)\\nplt.axis(\\'off\\')\\nplt.savefig(\\'category.png\\')\\nplt.show()',\n",
       " 'e1ce9a0b': \"<h1 align='center'> Netflix Data Analysis <h1>\\n![](https://www.gizmozones.com/wp-content/uploads/2019/07/netflix-icon-wallpaper-66504-68771-hd-wallpapers.jpg)\",\n",
       " '3f0ba8c5': '# Data Visualization',\n",
       " '1bf3d1eb': '#### Print first 10 values',\n",
       " '22b4a7d4': '### Type: Movie and TV Shows',\n",
       " '8168f6af': '#### Make a copy of the dataset',\n",
       " '1a626af2': '# WordCloud',\n",
       " 'fda3072b': '#### Check for NULL Values',\n",
       " 'd3e211a8': '### Relation between Type and Rating',\n",
       " '190b30e6': '## Cast in the Shows',\n",
       " 'ac49ee5a': '#### Check for Duplicate values',\n",
       " '532517df': '#### Print the name of columns',\n",
       " 'c43d6657': '#### Check unique values',\n",
       " '07a7ec36': '## Directors',\n",
       " '3281a186': 'No duplicate values present.',\n",
       " '5e1a2ce9': '## Country',\n",
       " '74b34672': '### Loading the Dataset',\n",
       " '0eb98823': '# Breakdown of this notebook:\\n1. **Importing Libraries**\\n2. **Loading the dataset**\\n3. **Data Cleaning:** \\n - Deleting redundant columns.\\n - Dropping duplicates.\\n - Cleaning individual columns.\\n - Remove the NaN values from the dataset\\n - Some Transformations\\n4. **Data Visualization:** Using plots to find relations between the features.\\n    - Type: Movie and TV Shows\\n    - Rating\\n    - Relation between Type and Rating\\n5. **Word Cloud**\\n    - Country\\n    - Cast\\n    - Director\\n    - Category',\n",
       " 'bc5642f8': '#### Drop NULL values',\n",
       " '517bc0c2': '### Pie-chart for Rating',\n",
       " '4fcf34cd': '## I will try to update this notebook frequently in the GitHub: https://github.com/chiragsamal/Netflix-Movies-and-TV-Shows',\n",
       " '844ab936': '### Importing Libraries',\n",
       " '4c843f70': '### Rating of shows and movies',\n",
       " '3294d8d6': '#### First 10 values',\n",
       " '08958924': '#### Convert Date Time format',\n",
       " '6fe6445b': '#### Shape of the dataset',\n",
       " '75d3084d': '### Pie-chart for the Type: Movie and TV Shows',\n",
       " '3bfcf5ac': \"<h1><font color='orange'>Please Upvote if you found these helpful</font></h1>\",\n",
       " '248958cd': '## Categories',\n",
       " '52216b08': '# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\n# For example, here\\'s several helpful packages to load in \\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\n\\n# Input data files are available in the \"../input/\" directory.\\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\n\\nimport os\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n# Any results you write to the current directory are saved as output.',\n",
       " '833dfb45': \"import random as rnd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nimport eli5\\n\\n#statistic\\nfrom scipy import stats\\nfrom pandas import DataFrame\\nfrom scipy.stats import chisquare\\nfrom scipy.stats import chi2_contingency\\nfrom statsmodels.graphics.mosaicplot import mosaic\\n\\n# machine learning structures\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.tree import export_graphviz\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import classification_report\\nfrom sklearn import metrics\\nimport statsmodels.api as sm\\nfrom IPython import display\\nimport graphviz\\nfrom sklearn.metrics import roc_curve, auc\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import accuracy_score\\nfrom eli5.sklearn import PermutationImportance\\n\\npd.options.mode.chained_assignment = None  #hide any pandas warnings\\n\\n# get the data\\nhd_data_path = '../input/heart-disease-uci/heart.csv'\\nhd_data = pd.read_csv(hd_data_path)\\n\",\n",
       " '40d7fdd4': 'hd_data = hd_data.apply(pd.to_numeric)\\n\\n# preview the data\\nhd_data.head()',\n",
       " '662042e1': '# check whether data has null value\\nhd_data.info()',\n",
       " 'dba421c8': '# missing values\\nhd_data.isnull().sum()',\n",
       " '67ee71bd': '# see basic values about the data\\nhd_data.describe()',\n",
       " '10b5635b': \"# check correlation between columns\\nplt.figure(figsize=(16, 12))\\ncorr = hd_data.corr()\\nax = sns.heatmap(\\n    corr, square=True, annot=True, fmt='.2f'\\n)\\nplt.show()\",\n",
       " 'd81b2024': \"hd_data.groupby('target').mean()\",\n",
       " '9f72244d': 'colors=[\\'cadetblue\\', \\'gold\\']\\nsns.countplot(x=\"target\", data=hd_data, palette=colors, alpha=0.5, edgecolor=\\'black\\', linewidth=2)\\nplt.show()',\n",
       " 'ab2d0ee0': 'hd_data_target_1=hd_data[hd_data.target==1]\\nhd_data_target_0=hd_data[hd_data.target==0]',\n",
       " '2bc8b55e': 'f,ax=plt.subplots(3,2,figsize=(16,12))\\nf.delaxes(ax[2,1])\\n\\nfor i,feature in enumerate([\\'age\\',\\'thalach\\',\\'chol\\',\\'trestbps\\',\\'oldpeak\\',\\'age\\']):\\n    colors = [\\'cadetblue\\', \\'gold\\']\\n    sns.countplot(x=feature,data=hd_data,hue=\\'target\\',ax=ax[i//2,i%2], palette = colors, alpha=0.7, linewidth=2)\\n    \\n    #sns.despine(ax[i//2,i%2]=ax[i//2,i%2], left=True)\\n    ax[i//2,i%2].set_ylabel(\"frequency\", fontsize=12)\\n    ax[i//2,i%2].set_xlabel(str(feature), fontsize=12)\\n\\nplt.tight_layout()\\nplt.show()',\n",
       " '107a7893': '# the code from https://www.kaggle.com/vincentlugat/heart-disease-analysis-and-prediction\\n\\nf,ax=plt.subplots(3,2,figsize=(12,12))\\nf.delaxes(ax[2,1])\\n\\nfor i,feature in enumerate([\\'age\\',\\'thalach\\',\\'chol\\',\\'trestbps\\',\\'oldpeak\\',\\'age\\']):\\n    sns.distplot(hd_data[hd_data[\\'target\\']==0][(feature)], ax=ax[i//2,i%2], kde_kws={\"color\":\"cadetblue\"}, hist=False )\\n    sns.distplot(hd_data[hd_data[\\'target\\']==1][(feature)], ax=ax[i//2,i%2], kde_kws={\"color\":\"gold\"}, hist=False )\\n\\n    # Get the two lines from the ax[i//2,i%2]es to generate shading\\n    l1 = ax[i//2,i%2].lines[0]\\n    l2 = ax[i//2,i%2].lines[1]\\n\\n    # Get the xy data from the lines so that we can shade\\n    x1 = l1.get_xydata()[:,0]\\n    y1 = l1.get_xydata()[:,1]\\n    x2 = l2.get_xydata()[:,0]\\n    y2 = l2.get_xydata()[:,1]\\n    ax[i//2,i%2].fill_between(x2,y2, color=\"gold\", alpha=0.6)\\n    ax[i//2,i%2].fill_between(x1,y1, color=\"cadetblue\", alpha=0.6)\\n\\n     #grid\\n    ax[i//2,i%2].grid(b=True, which=\\'major\\', color=\\'grey\\', linewidth=0.3)\\n    \\n    ax[i//2,i%2].set_title(\\'{} - target\\'.format(feature), fontsize=18)\\n    ax[i//2,i%2].set_ylabel(\\'count\\', fontsize=12)\\n    ax[i//2,i%2].set_xlabel(\\'Modality\\', fontsize=12)\\n\\n    #sns.despine(ax[i//2,i%2]=ax[i//2,i%2], left=True)\\n    ax[i//2,i%2].set_ylabel(\"frequency\", fontsize=12)\\n    ax[i//2,i%2].set_xlabel(str(feature), fontsize=12)\\n\\nplt.tight_layout()\\nplt.show()',\n",
       " 'e5face03': 'f,ax=plt.subplots(3,2,figsize=(12,12))\\nf.delaxes(ax[2,1])\\n\\nfor i,feature in enumerate([\\'age\\',\\'thalach\\',\\'chol\\',\\'trestbps\\',\\'oldpeak\\',\\'age\\']):\\n    colors = [\\'cadetblue\\', \\'gold\\']\\n    sns.boxplot(x=\"target\", y=feature , data=hd_data, ax=ax[i//2,i%2], palette=colors, boxprops=dict(alpha=0.8))\\n\\n    # Get the two lines from the ax[i//2,i%2]es to generate shading\\n    l1 = ax[i//2,i%2].lines[0]\\n    l2 = ax[i//2,i%2].lines[1]\\n\\n    # Get the xy data from the lines so that we can shade\\n    x1 = l1.get_xydata()[:,0]\\n    y1 = l1.get_xydata()[:,1]\\n    x2 = l2.get_xydata()[:,0]\\n    y2 = l2.get_xydata()[:,1]\\n    ax[i//2,i%2].fill_between(x2,y2, color=\"gold\", alpha=0.6)\\n    ax[i//2,i%2].fill_between(x1,y1, color=\"cadetblue\", alpha=0.6)\\n\\n     #grid\\n    ax[i//2,i%2].grid(b=True, which=\\'major\\', color=\\'grey\\', linewidth=0.3)\\n    \\n    ax[i//2,i%2].set_title(\\'{} - target\\'.format(feature), fontsize=18)\\n    ax[i//2,i%2].set_ylabel(\\'count\\', fontsize=12)\\n    ax[i//2,i%2].set_xlabel(\\'Modality\\', fontsize=12)\\n\\n    #sns.despine(ax[i//2,i%2]=ax[i//2,i%2], left=True)\\n    ax[i//2,i%2].set_ylabel(\"frequency\", fontsize=12)\\n    ax[i//2,i%2].set_xlabel(str(feature), fontsize=12)\\n\\nplt.tight_layout()\\nplt.show()',\n",
       " '8251f99d': '# t-test\\n\\n# age and target\\ntTestResult = stats.ttest_ind(hd_data_target_1[\\'age\\'], hd_data_target_0[\\'age\\'])\\ntTestResultDiffVar = stats.ttest_ind(hd_data_target_1[\\'age\\'], hd_data_target_0[\\'age\\'], equal_var=False)\\nprint(\\'* age & target\\')\\nprint(tTestResultDiffVar)\\nprint(\"\")\\n\\n# trestbps and target\\ntTestResult = stats.ttest_ind(hd_data_target_1[\\'trestbps\\'], hd_data_target_0[\\'trestbps\\'])\\ntTestResultDiffVar = stats.ttest_ind(hd_data_target_1[\\'trestbps\\'], hd_data_target_0[\\'trestbps\\'], equal_var=False)\\nprint(\\'* trestbps & target\\')\\nprint(tTestResultDiffVar)\\nprint(\"\")\\n\\n# chol and target\\ntTestResult = stats.ttest_ind(hd_data_target_1[\\'chol\\'], hd_data_target_0[\\'chol\\'])\\ntTestResultDiffVar = stats.ttest_ind(hd_data_target_1[\\'chol\\'], hd_data_target_0[\\'chol\\'], equal_var=False)\\nprint(\\'* chol & target\\')\\nprint(tTestResultDiffVar)\\nprint(\"\")\\n\\n# thalach and target\\ntTestResult = stats.ttest_ind(hd_data_target_1[\\'thalach\\'], hd_data_target_0[\\'thalach\\'])\\ntTestResultDiffVar = stats.ttest_ind(hd_data_target_1[\\'thalach\\'], hd_data_target_0[\\'thalach\\'], equal_var=False)\\nprint(\\'* thalach & target\\')\\nprint(tTestResultDiffVar)\\nprint(\"\")\\n\\n# oldpeak and target\\ntTestResult = stats.ttest_ind(hd_data_target_1[\\'oldpeak\\'], hd_data_target_0[\\'oldpeak\\'])\\ntTestResultDiffVar = stats.ttest_ind(hd_data_target_1[\\'oldpeak\\'], hd_data_target_0[\\'oldpeak\\'], equal_var=False)\\nprint(\\'* oldpeak & target\\')\\nprint(tTestResultDiffVar)\\nprint(\"\")',\n",
       " '06f97f99': \"# scatter plot between numeric variables\\nvar=['age', 'thalach', 'oldpeak', 'target']\\nsns.pairplot(hd_data[var], kind='scatter', diag_kind='hist')\\nplt.show()\",\n",
       " 'a4d1eb30': '# check regression plot\\n\\n# age and thalach\\nf, ax = plt.subplots(figsize=(8, 6))\\nax = sns.regplot(x=\"age\", y=\"thalach\", data=hd_data)\\nplt.show()\\n\\n# age and oldpeak\\nf, ax = plt.subplots(figsize=(8, 6))\\nax = sns.regplot(x=\"age\", y=\"oldpeak\", data=hd_data)\\nplt.show()\\n\\n# thalach and oldpeak\\nf, ax = plt.subplots(figsize=(8, 6))\\nax = sns.regplot(x=\"thalach\", y=\"oldpeak\", data=hd_data)\\nplt.show()',\n",
       " '9b7098f1': \"# check correlation between columns\\nplt.figure(figsize=(16, 12))\\ncorr = hd_data[var].corr()\\nax = sns.heatmap(\\n    corr, square=True, annot=True, fmt='.2f')\\nplt.show()\",\n",
       " '4187ee00': \"# code from https://www.kaggle.com/vincentlugat/heart-disease-analysis-and-prediction\\n\\nf,ax=plt.subplots(4,2,figsize=(12,12))\\n\\nfor i,feature in enumerate(['sex','cp','fbs','restecg','exang','slope','ca','thal']):\\n    colors = ['cadetblue', 'gold']\\n    sns.countplot(x=feature,data=hd_data,hue='target',ax=ax[i//2,i%2], palette = colors, alpha=0.7, edgecolor=('black'), linewidth=2)\\n    ax[i//2,i%2].grid(b=True, which='major', color='grey', linewidth=0.4)\\n    ax[i//2,i%2].set_title('Count of {} vs target'.format(feature), fontsize=18)\\n    ax[i//2,i%2].legend(loc='best')\\n    ax[i//2,i%2].set_ylabel('count', fontsize=12)\\n    ax[i//2,i%2].set_xlabel('modality', fontsize=12)\\n\\nplt.tight_layout()\\nplt.show()\",\n",
       " 'e8fcb5bd': \"mosaic(hd_data.sort_values('sex'), ['target', 'sex'],\\n      title='Mosaic chart of sex and target')\\nplt.show()\\n\\nmosaic(hd_data.sort_values('cp'), ['target', 'cp'],\\n      title='Mosaic chart of cp and target')\\nplt.show()\\n\\nmosaic(hd_data.sort_values('fbs'), ['target', 'fbs'],\\n      title='Mosaic chart of fbs and target')\\nplt.show()\\n\\nmosaic(hd_data.sort_values('restecg'), ['target', 'restecg'],\\n      title='Mosaic chart of restecg and target')\\nplt.show()\\n\\nmosaic(hd_data.sort_values('exang'), ['target', 'exang'],\\n      title='Mosaic chart of exang and target')\\nplt.show()\\n\\nmosaic(hd_data.sort_values('slope'), ['target', 'slope'],\\n      title='Mosaic chart of slope and target')\\nplt.show()\\n\\nmosaic(hd_data.sort_values('ca'), ['target', 'ca'],\\n      title='Mosaic chart of ca and target')\\nplt.show()\\n\\nmosaic(hd_data.sort_values('thal'), ['target', 'thal'],\\n      title='Mosaic chart of thal and target')\\nplt.show()\",\n",
       " 'c7374eff': \"print(hd_data.groupby('sex')['target'].value_counts())\\nprint(hd_data.groupby('cp')['target'].value_counts())\\nprint(hd_data.groupby('fbs')['target'].value_counts())\\nprint(hd_data.groupby('restecg')['target'].value_counts())\\nprint(hd_data.groupby('slope')['target'].value_counts())\\nprint(hd_data.groupby('exang')['target'].value_counts())\\nprint(hd_data.groupby('ca')['target'].value_counts())\\nprint(hd_data.groupby('thal')['target'].value_counts())\",\n",
       " '2bbe41d0': \"sex_target0, sex_target1 = [24, 114], [72, 93]\\nsex_target = DataFrame([sex_target0, sex_target1], columns=['sex=0(F)', 'sex=1(M)'], index=['target=0(No Disease)', 'target=1(Have Disease)'])\\n\\ncp_target0, cp_target1 = [104, 9, 18, 7], [39, 41, 69, 16]\\ncp_target = DataFrame([cp_target0, cp_target1], columns=['cp=0', 'cp=1', 'cp=2', 'cp=3'], index=['target=0(No Disease)', 'target=1(Have Disease)'])\\n\\nfbs_target0, fbs_target1 = [116, 22], [142, 23]\\nfbs_target = DataFrame([fbs_target0, fbs_target1], columns=['fbs=0(<=120mg/dl)', 'fbs=1(>120mg/dl)'], index=['target=0(No Disease)', 'target=1(Have Disease)'])\\n\\nrest_target0, rest_target1 = [79, 56, 3], [68, 96, 1]\\nrest_target = DataFrame([rest_target0, rest_target1], columns=['restecg=0(normal)', 'restecg=1(ST-T)', 'restecg=2(Probable)'], index=['target=0(No Disease)', 'target=1(Have Disease)'])\\n\\nslp_target0, slp_target1 = [12, 91, 35], [9, 49, 107]\\nslp_target = DataFrame([slp_target0, slp_target1], columns=['slp=0', 'slp=1', 'slp=2'], index=['target=0(No Disease)', 'target=1(Have Disease)'])\\n\\nex_target0, ex_target1 = [62, 76], [142, 23]\\nex_target = DataFrame([ex_target0, ex_target1], columns=['ex=0(not induced)', 'ex=1(induced)'], index=['target=0(No Disease)', 'target=1(Have Disease)'])\\n\\nca_target0, ca_target1 = [45, 44, 31, 17, 1], [130, 21, 7, 3, 4]\\nca_target = DataFrame([ca_target0,ca_target1], columns=['ca=0', 'ca=1', 'ca=2', 'ca=3', 'ca=4'], index=['target=0(No Disease)', 'target=1(Have Disease)'])\\n\\nthal_target0, thal_target1 = [1, 12, 36, 89], [1, 6, 130, 28]\\nthal_target = DataFrame([thal_target0,thal_target1], columns=['thal=0', 'thal=1', 'thal=2', 'thal=3'], index=['target=0(No Disease)', 'target=1(Have Disease)'])\\n\",\n",
       " 'faac602b': 'sex_target',\n",
       " 'ca182d8e': 'cp_target',\n",
       " 'df71e263': 'fbs_target',\n",
       " 'd5691449': 'rest_target',\n",
       " '00cc2dde': \"rest_target0, rest_target1 = [79, 59], [68, 97]\\nrest_target = DataFrame([rest_target0, rest_target1], columns=['restecg=0(normal)', 'restecg=1(ST-T)'], index=['target=0(No Disease)', 'target=1(Have Disease)'])\\nrest_target\",\n",
       " '23c6c9df': 'slp_target',\n",
       " '912d4447': 'ex_target',\n",
       " '573d2485': 'ca_target',\n",
       " '898b4374': \"ca_target0, ca_target1 = [45, 44, 31, 18], [130, 21, 7, 7]\\nca_target = DataFrame([ca_target0,ca_target1], columns=['ca=0', 'ca=1', 'ca=2', 'ca=3'], index=['target=0(No Disease)', 'target=1(Have Disease)'])\\nca_target\",\n",
       " 'bf7c0813': 'thal_target',\n",
       " '2870f8fa': \"thal_target0, thal_target1 = [13, 36, 89], [7, 130, 28]\\nthal_target = DataFrame([thal_target0,thal_target1], columns=['thal=1', 'thal=2', 'thal=3'], index=['target=0(No Disease)', 'target=1(Have Disease)'])\\nthal_target\",\n",
       " '5bf574a7': '# code from https://m.blog.naver.com/PostView.nhn?blogId=parksehoon1971&logNo=221589203965&proxyReferer=https%3A%2F%2Fwww.google.com%2F\\n\\nchi2, p, dof, expected = chi2_contingency([sex_target0, sex_target1])\\nprint(\" * sex - target expected frequency\")\\nprint(expected)\\nprint(\"\")\\n\\nchi2, p, dof, expected = chi2_contingency([cp_target0, cp_target1])\\nprint(\" * cp - target expected frequency\")\\nprint(expected)\\nprint(\"\")\\n\\nchi2, p, dof, expected = chi2_contingency([fbs_target0, fbs_target1])\\nprint(\" * fbs - target expected frequency\")\\nprint(expected)\\nprint(\"\")\\n\\nchi2, p, dof, expected = chi2_contingency([rest_target0, rest_target1])\\nprint(\" * rest - target expected frequency\")\\nprint(expected)\\nprint(\"\")\\n\\nchi2, p, dof, expected = chi2_contingency([slp_target0, slp_target1])\\nprint(\" * slp - target expected frequency\")\\nprint(expected)\\nprint(\"\")\\n\\nchi2, p, dof, expected = chi2_contingency([ex_target0, ex_target1])\\nprint(\" * ex - target expected frequency\")\\nprint(expected)\\nprint(\"\")\\n\\nchi2, p, dof, expected = chi2_contingency([ca_target0, ca_target1])\\nprint(\" * ca - target expected frequency\")\\nprint(expected)\\nprint(\"\")\\n\\nchi2, p, dof, expected = chi2_contingency([thal_target0, thal_target1])\\nprint(\" * thal - target expected frequency\")\\nprint(expected)\\nprint(\"\")',\n",
       " '9fffa676': 'sex_target_result = chisquare(sex_target0, f_exp=sex_target1)\\ncp_target_result = chisquare(cp_target0, f_exp=cp_target1)\\nfbs_target_result = chisquare(fbs_target0, f_exp=fbs_target1)\\nrest_target_result = chisquare(rest_target0, f_exp=rest_target1)\\nslp_target_result = chisquare(slp_target0, f_exp=slp_target1)\\nex_target_result = chisquare(ex_target0, f_exp=ex_target1)\\nca_target_result = chisquare(ca_target0, f_exp=ca_target1)\\nthal_target_result = chisquare(thal_target0, f_exp=thal_target1)',\n",
       " '49b3e1a4': 'print(\" * sex-target\")\\nprint(sex_target_result)\\nprint(\"\")\\nprint(\" * cp-target\")\\nprint(cp_target_result)\\nprint(\"\")\\nprint(\" * fbs-target\")\\nprint(fbs_target_result)\\nprint(\"\")\\nprint(\" * rest-target\")\\nprint(rest_target_result)\\nprint(\"\")\\nprint(\" * slp-target\")\\nprint(slp_target_result)\\nprint(\"\")\\nprint(\" * ex-target\")\\nprint(ex_target_result)\\nprint(\"\")\\nprint(\" * ca-target\")\\nprint(ca_target_result)\\nprint(\"\")\\nprint(\" * thal-target\")\\nprint(thal_target_result)\\nprint(\"\")',\n",
       " '4e46cb6b': 'y = hd_data.target\\nX = hd_data',\n",
       " 'ada5b52a': \"# choose the cols(which has p value smaller than 0.01)\\nfiltered_col = ['age', 'sex', 'cp', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\\ny1 = y\\nX1 = X[filtered_col]\\n\\n# split into train set and test set\\ntrain_X1, test_X1, train_y1, test_y1 = train_test_split(X1, y1, random_state=0)\",\n",
       " 'd9e4f62c': '# Random Forest\\nrandom_forest = RandomForestClassifier(n_estimators=100, max_depth = 7)\\nrandom_forest.fit(train_X1, train_y1)\\ny_pred = random_forest.predict(test_X1)\\n\\nacc_RandomForest = accuracy_score(test_y1, y_pred)*100\\nprint(acc_RandomForest)',\n",
       " '2f35d143': '# code from https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\\n\\nestimator = random_forest.estimators_[1]\\n\\nexport_graphviz(estimator, out_file=\\'tree.dot\\', \\n                feature_names = train_X1.columns,\\n                class_names = [\"0(No disease)\",\"1(Have disease)\"],\\n                rounded = True, proportion = True, \\n                label=\\'root\\', filled = True,\\n                precision = 2)\\n\\nfrom subprocess import call\\ncall([\\'dot\\', \\'-Tpng\\', \\'tree.dot\\', \\'-o\\', \\'tree.png\\', \\'-Gdpi=600\\'])\\n\\nfrom IPython.display import Image\\nImage(filename = \\'tree.png\\')',\n",
       " 'b3f3bef3': '# code fromhttps://www.kaggle.com/cdabakoglu/heart-disease-classifications-machine-learning\\n\\n# KNN\\n# try ro find best k value\\nscoreList = []\\nfor i in range(1,20):\\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\\n    knn2.fit(train_X1, train_y1)\\n    y_pred = knn2.predict(test_X1)\\n    scoreList.append(accuracy_score(test_y1, y_pred))\\n    \\nplt.plot(range(1,20), scoreList)\\nplt.xticks(np.arange(1,20,1))\\nplt.xlabel(\"K value\")\\nplt.ylabel(\"Score\")\\nplt.show()\\n\\naccuracies = {}\\nacc = max(scoreList)*100\\naccuracies[\\'KNN\\'] = acc\\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))\\n\\n',\n",
       " '521b3303': '# KNN\\nknn = KNeighborsClassifier(n_neighbors = 13)\\nknn.fit(train_X1, train_y1)\\ny_pred = knn.predict(test_X1)\\nacc_knn = accuracy_score(test_y1, y_pred)*100\\nprint(acc_knn)',\n",
       " 'c734525a': '# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(train_X1, train_y1)\\ny_pred = gaussian.predict(test_X1)\\nacc_gaussian_naive_bayes = accuracy_score(test_y1, y_pred)*100\\nprint(acc_gaussian_naive_bayes)\\n',\n",
       " 'c9789afa': '# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(train_X1, train_y1)\\ny_pred = perceptron.predict(test_X1)\\nacc_perceptron = accuracy_score(test_y1, y_pred)*100\\nprint(acc_perceptron)\\n',\n",
       " 'd966905c': '# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(train_X1, train_y1)\\ny_pred = linear_svc.predict(test_X1)\\nacc_linear_svc = accuracy_score(test_y1, y_pred)*100\\nprint(acc_linear_svc)\\n',\n",
       " 'c9ed62a6': '# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(train_X1, train_y1)\\ny_pred = sgd.predict(test_X1)\\nacc_stochastic_gradient_descent = accuracy_score(test_y1, y_pred)*100\\nprint(acc_stochastic_gradient_descent)\\n',\n",
       " '265696a3': '# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier(max_depth=7)\\ndecision_tree.fit(train_X1, train_y1)\\ny_pred = decision_tree.predict(test_X1)\\nacc_decision_tree = accuracy_score(test_y1, y_pred)*100\\nprint(acc_decision_tree)\\n',\n",
       " 'bc3198e6': 'export_graphviz(decision_tree, out_file=\"tree.dot\",\\n                feature_names=train_X1.columns, \\n                class_names=[\"0(No disease)\",\"1(Have disease)\"], \\n                rounded = True, proportion = True, \\n                label=\\'root\\', precision = 2,\\n                filled=True, impurity=True)\\nfrom subprocess import call\\ncall([\\'dot\\', \\'-Tpng\\', \\'tree.dot\\', \\'-o\\', \\'tree.png\\', \\'-Gdpi=600\\'])\\n\\nfrom IPython.display import Image\\nImage(filename = \\'tree.png\\')',\n",
       " 'bcce5e26': '# Logistic Regression\\n\\nlogistic_regression_model = LogisticRegression()\\nlogistic_regression_model.fit(train_X1, train_y1)\\nX1=sm.add_constant(X)\\nmodel=sm.OLS(y, X1)\\nres=model.fit()\\ny_pred = logistic_regression_model.predict(test_X1)\\nacc_logistic_regression=accuracy_score(test_y1, y_pred)*100\\nprint(acc_logistic_regression)\\n',\n",
       " 'd2d024fc': \"models = pd.DataFrame({\\n    'Model': ['KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_knn, acc_logistic_regression, \\n              acc_RandomForest, acc_gaussian_naive_bayes, acc_perceptron, \\n              acc_stochastic_gradient_descent, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\n",
       " 'a915ad95': 'plt.rcParams[\\'figure.figsize\\']=(15,8)\\n\\ncolors = [\\'palegoldenrod\\',\\'lightgreen\\',\\'cadetblue\\',\\'gold\\',\\'greenyellow\\',\\'aquamarine\\',\\'steelblue\\',\\'khaki\\']\\n\\nax = plt.bar(x=\\'Model\\', data=models, height=\"Score\", alpha=0.7, color=colors, edgecolor=(\\'black\\'), linewidth=2)\\nplt.ylabel(\\'accuracy score\\', fontsize=12)\\nplt.xlabel(\\'model\\', fontsize=12)\\nplt.title(\\'Accuracy score of Models\\', fontsize=18)\\nplt.show()\\n',\n",
       " '8f750a86': '# code from https://www.kaggle.com/tentotheminus9/what-causes-heart-disease-explaining-the-model\\n\\ny_pred = random_forest.predict(test_X1)\\ny_pred_quant = random_forest.predict_proba(test_X1)[:, 1]\\ny_pred_bin = random_forest.predict(test_X1)',\n",
       " 'bea58e2d': 'confusion_matrix = confusion_matrix(test_y1, y_pred_bin)\\nconfusion_matrix',\n",
       " 'a5abd876': \"total=sum(sum(confusion_matrix))\\n\\nsensitivity = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[1,0])\\nprint('Sensitivity : ', sensitivity )\\n\\nspecificity = confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[0,1])\\nprint('Specificity : ', specificity)\",\n",
       " '90b29a23': 'fpr, tpr, thresholds = roc_curve(test_y1, y_pred_quant)\\n\\nfig, ax = plt.subplots()\\nax.plot(fpr, tpr)\\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\\nplt.xlim([0.0, 1.0])\\nplt.ylim([0.0, 1.0])\\nplt.rcParams[\\'font.size\\'] = 12\\nplt.title(\\'ROC curve\\')\\nplt.xlabel(\\'False Positive Rate (1 - Specificity)\\')\\nplt.ylabel(\\'True Positive Rate (Sensitivity)\\')\\nplt.grid(True)',\n",
       " '3981f239': 'auc(fpr, tpr)',\n",
       " 'deaab2af': 'perm = PermutationImportance(random_forest, random_state=1).fit(test_X1, test_y1)\\neli5.show_weights(perm, feature_names = test_X1.columns.tolist())',\n",
       " 'a52c33d7': \"The count of 'restecg=2' is so small that it will be combined into 'restecg=1'.\",\n",
       " 'fa6f9b4a': '# 4. Make machine learning model',\n",
       " 'f0834d98': '# Heart Disease Prediction\\n\\nFinding the best machine learning model with Heart Disease UCI data.\\n> 1. About the columns\\n> 2. Get data, Import Libraries\\n> 3. Explore data\\n    * 3.1 target\\n    * 3.2 numerical attributes & target\\n    * 3.3 correlation between numerical attributes\\n    * 3.4 categorical attributes & target\\n> 4. Make machine learning model\\n    * 4.1 Choose the attributes & Make train/test set\\n    * 4.2 Make models\\n    * 4.3 Model evaluation',\n",
       " '2e3378c8': '# AUC\\n\\n- 0.90 - 1.00 = excellent\\n- 0.80 - 0.90 = good\\n- 0.70 - 0.80 = fair\\n- 0.60 - 0.70 = poor\\n- 0.50 - 0.60 = fail',\n",
       " 'ace717b8': '# 4.1 Choose the attributes & Make train/test set',\n",
       " '42e378b7': \"It's excellent!\",\n",
       " 'b319e08c': '# 3.3 Correlation between numerical attributes\\n- scatter plot - age, thalach, oldpeak\\n- age & thalach\\n- age & oldpeak\\n- thalach & oldpeak',\n",
       " '85e566a3': 'The best model is Random Forest!!',\n",
       " 'ef2ea54f': '# 4.3 Model Evaluation',\n",
       " 'b72a1aa1': 'Thanks!!',\n",
       " '1a60ee8e': 'fbs -> p-value>0.01 -> remove fbs from modeling attributes.',\n",
       " 'e841c0b2': '# 1. About the columns\\n',\n",
       " 'e44f06f2': \"The count of 'ca=4' is so small that it will be combined into 'ca=3'.\",\n",
       " '5406093f': 'check the expected value',\n",
       " '6b04338d': \"The count of 'thal=0' is so small that it will be combined into 'thal=1'.\",\n",
       " '23b7013d': '# 3.2 Numerical attributes & target\\n- crosstab\\n- kde\\n- boxplot\\n- t-test (critical value = 0.01)',\n",
       " '13649982': '# 3. Explore data',\n",
       " '9c585043': '# 2. Get data, Import libraries',\n",
       " 'fbba8028': '# 4.2 Make various machine learning models\\n- Random Forest\\n- KNN\\n- Gausian Naive Bayes\\n- Perceptron\\n- Linear SVC\\n- Stochastic Gradient Descent\\n- Decision Tree\\n- Logistic Regression',\n",
       " '98441ad0': 'trestbps & chol -> p-value > 0.01\\nRemove trestbps, chol from the modeling attributes.\\n\\nage & thalach & oldpeak -> p-value < 0.01\\nUse them to modeling.',\n",
       " '1726b399': '# 3.1 target',\n",
       " '4d4159a8': '# 3.4 Categorical attributes & target\\n- crosstab\\n- mosaic plot\\n- chi-sqare test',\n",
       " 'ed4a0c36': \"- **age**\\n- **sex** 1 = male, 0 = female\\n- **cp:** The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: had pain but not angina, Value 4: had no pain)\\n- **trestbps:** Resting Blood Pressure\\n- **chol:** Cholesterol level\\n- **fbs:** Fasting Blood Sugar (Blood Sugar Level on empty stomach, over 126mg/dl => Diabetes area) (> 120 mg/dl, 1 = true; 0 = false)\\n- **restecg:** Resting electrocardiogram(ECG) (0 = normal, 1 = having ST-T change, 2 = left ventricular hypertrophy(serious))\\n- **thalach:** Maximum Heart Rate\\n- **exang:** Stable angina (angina caused by exercise) (1 = yes; 0 = no)\\n- **oldpeak:** ST depression (how low EGC's ST segments point is from the base line)\\n- **slope:** the slope of the peak exercise ST segment (Value 0: upsloping(ST-elevation), Value 1: flat(Normal), Value 2: downsloping(ST-depression))\\n- **ca:** The number of major blood vessels (0-4) colored by fluoroscopy(X-ray)\\n- **thal:** Thalassemia (Anemia caused by defective globine genes. Fail to produce normal hemoglobin.)\\n- **target:** Have Heart Disease (0 = no, 1 = yes)\",\n",
       " 'c4eeda51': \"# to handle datasets\\nimport pandas as pd\\nimport numpy as np\\n\\n# for plotting\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# to divide train and test set\\nfrom sklearn.model_selection import train_test_split\\n\\n# feature scaling\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\n# to visualise al the columns in the dataframe\\npd.pandas.set_option('display.max_columns', None)\\n\\nimport warnings\\nwarnings.simplefilter(action='ignore')\",\n",
       " 'e2b8469c': \"data = pd.read_csv('../input/houserentpredictiondataset/houseRent/housing_train.csv')\\ndata.shape\",\n",
       " 'f707ddc3': 'data.info()',\n",
       " '9c6cda58': \"data['baths'] = np.ceil(data['baths'])\\ndata['baths'] = data['baths'].astype(np.int)\",\n",
       " '220f6646': \"# make a list of the categorical variables that contain missing values\\ncat_var_na = ['laundry_options', 'parking_options']\",\n",
       " 'a8c511ca': 'def impute_missing_cat(data, var, modeof):\\n    return data.groupby(modeof)[var].transform(\\n        lambda x: x.fillna(x.mode()[0]))',\n",
       " '45bb02b9': 'data[\"laundry_options\"] = impute_missing_cat(data, \"laundry_options\", \"type\")\\ndata[\"parking_options\"] = impute_missing_cat(data, \"parking_options\", \"type\")\\ndata = data.dropna(subset=[\"state\", \"description\"],axis=0)',\n",
       " '206d9105': \"# make a list with the numerical variables that contain missing values\\nnum_var_na = ['lat', 'long']\",\n",
       " '8d597b9d': 'def impute_missing_num(data, var, meanof):\\n    return data.groupby(meanof)[var].transform(\\n        lambda x: x.fillna(x.mode()[0]))',\n",
       " '0b225573': 'data[\"lat\"] = impute_missing_num(data, \"lat\", \"region\")\\ndata[\"long\"] = impute_missing_num(data, \"long\", \"region\")',\n",
       " '8605f203': \"#Check remaining missing values if any \\nall_data_na = (data.isnull().sum() / len(data)) * 100\\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\\nmissing_data.head()\",\n",
       " 'b67fa470': 'data.shape',\n",
       " '17c0d84d': \"price_upper, price_lower = 2400, 1\\nsqfeet_upper, sqfeet_lower = 1762, 1\\nbeds_upper, beds_lower = 3, 1\\nbaths_upper, baths_lower = 3, 1\\n\\ndata = data[(data['price'] <= price_upper) & (data['price'] >= price_lower)]\\ndata = data[(data['sqfeet'] <= sqfeet_upper) & (data['sqfeet'] >= sqfeet_lower)]\\ndata = data[(data['beds'] <= beds_upper) & (data['beds'] >= beds_lower)]\\ndata = data[(data['baths'] <= baths_upper) & (data['baths'] >= baths_lower)]\",\n",
       " '003d155b': 'data.shape',\n",
       " '22c68343': \"data['premium_house'] = np.where((data['baths']>=data['beds'])&(data['beds']>1),1,0)\\ndata['pets_allowed'] = np.where((data['cats_allowed']==1)&data['dogs_allowed']==1,1,0)\\ndata['beds_per_sqfeet'] = data['beds'] / data['sqfeet']\\ndata['baths_per_beds'] = data['baths'] / data['beds']\",\n",
       " '8c48bda3': 'data.description[82226].lower()',\n",
       " '7ecf2803': \"[x in data.description[82226].lower() for x in ['pool', 'swimming','wi-fi','fireplace','grilling','gym','fence', 'court']]\",\n",
       " 'dcf290ed': \"data['has_pool'] = data['description'].apply(lambda x: 1 if 'pool' in x.lower() or 'swimming' in x.lower() else 0)\\ndata['has_grill'] = data['description'].apply(lambda x: 1 if 'grill' in x.lower() or 'grilling' in x.lower() else 0)\\ndata['has_fireplace'] = data['description'].apply(lambda x: 1 if 'fireplace' in x.lower() or 'fire pits' in x.lower() else 0)\\ndata['gym_nearby'] = data['description'].apply(lambda x: 1 if 'gym' in x.lower() or 'fitness' in x.lower() else 0)\\ndata['school/clg_nearby'] = data['description'].apply(lambda x: 1 if 'school' in x.lower() or 'college' in x.lower() else 0)\\ndata['wifi_facilities'] = data['description'].apply(lambda x: 1 if 'wifi' in x.lower() or 'wi-fi' in x.lower() else 0)\\ndata['valet_service'] = data['description'].apply(lambda x: 1 if 'valet' in x.lower() else 0)\\ndata['shopping_nearby'] = data['description'].apply(lambda x: 1 if 'shopping' in x.lower() else 0)\\ndata['sports_playground'] = data['description'].apply(lambda x: 1 if 'sport' in x.lower()  or 'sports' in x.lower() \\n                                                      or 'tennis' in x.lower() or 'soccer' in x.lower() \\n                                                      or 'soccers' in x.lower() or 'court' in x.lower() else 0)\\ndata['dining_nearby'] = data['description'].apply(lambda x: 1 if 'dining' in x.lower() else 0)\\n\",\n",
       " '6887708c': 'data.columns',\n",
       " '73d31a45': \"for var in ['has_pool', 'has_grill', 'has_fireplace', 'gym_nearby',\\n       'school/clg_nearby', 'wifi_facilities', 'valet_service',\\n       'shopping_nearby', 'sports_playground', 'dining_nearby']:\\n    print(data[var].value_counts())\",\n",
       " 'fb4e504d': \"for var in ['price','sqfeet','baths_per_beds','beds_per_sqfeet']:\\n    data[var] = np.log(data[var])\",\n",
       " '1c2c57d4': \"# check that data set does not contain null values in the engineered variables\\n[var for var in ['price','sqfeet','baths_per_beds','beds_per_sqfeet'] if data[var].isnull().sum() > 0]\",\n",
       " '3bada2d9': \"# let's capture the categorical variables in a list\\n\\ncat_vars = ['region', 'type', 'laundry_options', 'parking_options', 'state']\",\n",
       " 'c718bb13': \"frequent_ls = {\\n    'region': \\n        ['denver', 'fayetteville', 'jacksonville', 'omaha / council bluffs', 'rochester'],\\n     'type': \\n        ['apartment', 'condo', 'duplex', 'house', 'manufactured', 'townhouse'],\\n     'laundry_options': \\n        ['laundry in bldg', 'laundry on site', 'w/d hookups', 'w/d in unit'],  \\n     'parking_options': \\n        ['attached garage', 'carport', 'detached garage', 'off-street parking', 'street parking'],\\n     'state': \\n        ['al', 'ar', 'az', 'ca', 'co', 'ct', 'fl', 'ga', 'ia', 'id', 'il', 'in', 'ks', 'ky', 'la', \\n        'ma', 'md', 'mi', 'mn', 'ms', 'nc', 'nd', 'ne', 'nj', 'nm', 'nv', 'ny', 'oh']\\n}\\n\\n\\nfor var in cat_vars:\\n    data[var] = np.where(data[var].isin(\\n        frequent_ls[var]), data[var], 'Rare')\",\n",
       " '074a06e1': '# this function will assign discrete values to the strings of the variables,\\n# so that the smaller value corresponds to the category that shows the smaller\\n# mean house sale price\\n\\n\\ndef replace_categories(data, var, target):\\n\\n    # order the categories in a variable from that with the lowest\\n    # house sale price, to that with the highest\\n    ordered_labels = data.groupby([var])[target].mean().sort_values().index\\n\\n    # create a dictionary of ordered categories to integer values\\n    ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}\\n\\n    # use the dictionary to replace the categorical strings by integers\\n    data[var] = data[var].map(ordinal_label)',\n",
       " '0ed4190d': \"for var in cat_vars:\\n    replace_categories(data, var, 'price')\",\n",
       " 'db290ccc': '# check absence of na in the train set\\n[var for var in data.columns if data[var].isnull().sum() > 0]',\n",
       " '3ed439a6': 'data.head()',\n",
       " 'e33ad797': 'data.columns',\n",
       " '6ffc0e33': \"features = ['region', 'price', 'type', 'sqfeet', 'smoking_allowed', 'wheelchair_access', \\n            'electric_vehicle_charge', 'comes_furnished', 'laundry_options', 'parking_options','lat', 'long', \\n            'premium_house', 'pets_allowed', 'beds_per_sqfeet', 'baths_per_beds', 'has_pool', 'has_grill', \\n            'has_fireplace', 'gym_nearby', 'school/clg_nearby', 'wifi_facilities', 'valet_service', \\n            'shopping_nearby', 'sports_playground', 'dining_nearby']\\n\\ndata_final = data[features].copy()\\ndata_final.head()\",\n",
       " '04724f0c': 'for feature in features:\\n    data_final[feature] = data_final[feature].astype(np.float64)',\n",
       " 'f42554db': \"corr_matrix = data.corr()\\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\\nmask[np.triu_indices_from(mask)]= True\\n\\nfig, ax = plt.subplots(figsize=(25,25)) \\n\\nsns.heatmap(corr_matrix, \\n            annot=True, \\n            square=True,\\n            fmt='.2g',\\n            mask=mask,\\n            ax=ax).set(\\n    title = 'Feature Correlation', xlabel = 'Columns', ylabel = 'Columns')\\n\\nax.set_yticklabels(corr_matrix.columns, rotation = 0)\\nax.set_xticklabels(corr_matrix.columns)\\nsns.set_style({'xtick.bottom': True}, {'ytick.left': True})\",\n",
       " '2dfb8bd7': \"data_final.to_csv('data_cleaned.csv', index=False)\",\n",
       " '66509dde': '## Outliers',\n",
       " 'f219c4ac': \"## Exploring *'description'* column\",\n",
       " '6684312e': '### Encoding of categorical variables\\n\\nNext, we need to transform the strings of the categorical variables into numbers. We will do it so that we capture the monotonic relationship between the label and the target.\\n\\nTo learn more about how to encode categorical variables visit our course [Feature Engineering for Machine Learning](https://www.udemy.com/feature-engineering-for-machine-learning/?couponCode=UDEMY2018) in Udemy.',\n",
       " '6f370824': '## Machine Learning Model Building Pipeline: Data Analysis\\n\\nIn this following notebook, we will go through the Data Analysis step in the Machine Learning model building pipeline. There will be a notebook for each one of the Machine Learning Pipeline steps:\\n\\n1. [Data Analysis](https://www.kaggle.com/rkb0023/exploratory-data-analysis-house-rent-prediction)\\n2. Feature Engineering\\n3. [Model Building](https://www.kaggle.com/rkb0023/model-building-house-rent-prediction)\\n\\n**This is the notebook for step 2: Feature Engineering**\\n\\nThe dataset can be found in [iNeuron](https://challenge-ineuron.in/mlchallenge.php#) ML Challenge 2.\\n\\n<hr>\\n\\n## Predicting Rent Price of Houses\\n\\nThe aim of the project is to build a machine learning model to predict the rent price of homes based on different explanatory variables describing aspects of residential houses. \\n\\n\\n<hr>',\n",
       " 'f1c97f48': '## Missing values\\n\\n### Categorical variables',\n",
       " '9715d21e': '## Numerical variable transformation\\n\\nWe will log transform the positive numerical variables in order to get a more Gaussian-like distribution. This tends to help Linear machine learning models. ',\n",
       " '2f033cbe': 'Is there any remaining missing value ? ',\n",
       " '70c36d2a': '### Correlation Heatmap',\n",
       " '7511df4c': \"Let's get some intriguing new features\",\n",
       " 'b8feaeb4': 'In EDA, we decided to remove outliers according to the upper and lower bound of its interquartile range.',\n",
       " '77bf1762': 'Getting appropriate data types. For example baths have a dtype of float, and it contains some decimal values. But it is a quantitative variable. So transforming it to remove decimal.',\n",
       " 'e900d301': '## Getting More Features',\n",
       " 'cf3f611c': '## Categorical variables\\n\\n### Removing rare labels\\n\\nFirst, we will group those categories within variables that are present in less than 1% of the observations. That is, all values of categorical variables that are shared by less than 1% of houses, well be replaced by the string \"Rare\".\\n\\nTo learn more about how to handle categorical variables visit our course [Feature Engineering for Machine Learning](https://www.udemy.com/feature-engineering-for-machine-learning/?couponCode=UDEMY2018) in Udemy.',\n",
       " '26b707c8': '### Numerical variables\\n',\n",
       " 'd743aa74': \"## House Prices dataset: Feature Engineering\\n\\nIn the following cells, we will engineer / pre-process the variables of the House Rent Dataset from iNeuron. We will engineer the variables so that we tackle:\\n\\n1. Missing values\\n2. Temporal variables\\n3. Non-Gaussian distributed variables\\n4. Categorical variables: remove rare labels\\n5. Categorical variables: convert strings to numbers\\n5. Standarise the values of the variables to the same range\\n\\nLet's go ahead and load the dataset.\",\n",
       " '7f2e9c9b': '## Data Cleaning',\n",
       " 'dfc29eee': 'import numpy as np\\nimport pandas as pd\\nimport os, math, sys\\nimport time, datetime\\nimport glob, itertools\\nimport argparse, random\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torchvision import datasets\\nfrom torch.autograd import Variable\\nfrom torchvision.models import vgg19\\nimport torchvision.transforms as transforms\\nfrom torch.utils.data import DataLoader, Dataset\\nfrom torchvision.utils import save_image, make_grid\\n\\nimport plotly\\nfrom scipy import signal\\nimport plotly.express as px\\nimport plotly.graph_objects as go\\nimport matplotlib.pyplot as plt\\n\\nfrom PIL import Image\\nfrom tqdm import tqdm_notebook as tqdm\\nfrom sklearn.model_selection import train_test_split\\n\\nrandom.seed(42)\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")',\n",
       " '76b36b19': '# path to pre-trained models\\npretrained_model_path = \"../input/cyclegan-facades-segmentation-translation-pytorch/saved_models\"\\n# epoch to start training from\\nepoch_start = 36\\n# number of epochs of training\\nn_epochs = 80\\n# name of the dataset\\ndataset_path = \"../input/facades-dataset\"\\n# size of the batches\"\\nbatch_size = 4\\n# adam: learning rate\\nlr = 0.00012\\n# adam: decay of first order momentum of gradient\\nb1 = 0.5\\n# adam: decay of first order momentum of gradient\\nb2 = 0.999\\n# epoch from which to start lr decay\\ndecay_epoch = 1\\n# number of cpu threads to use during batch generation\\nn_workers = 8\\n# size of image height\\nimg_height = 256\\n# size of image width\\nimg_width = 256\\n# number of image channels\\nchannels = 3\\n# interval between saving generator outputs\\nsample_interval = 100\\n# interval between saving model checkpoints\\ncheckpoint_interval = -1\\n# number of residual blocks in generator\\nn_residual_blocks = 9\\n# cycle loss weight\\nlambda_cyc = 10.0\\n# identity loss weight\\nlambda_id = 5.0\\n# Development / Debug Mode\\ndebug_mode = False\\n\\n# Create images and checkpoint directories\\nos.makedirs(\"images\", exist_ok=True)\\nos.makedirs(\"saved_models\", exist_ok=True)',\n",
       " '1f476dee': 'def to_rgb(image):\\n    rgb_image = Image.new(\"RGB\", image.size)\\n    rgb_image.paste(image)\\n    return rgb_image\\n\\n\\nclass ReplayBuffer:\\n    def __init__(self, max_size=50):\\n        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\\n        self.max_size = max_size\\n        self.data = []\\n\\n    def push_and_pop(self, data):\\n        to_return = []\\n        for element in data.data:\\n            element = torch.unsqueeze(element, 0)\\n            if len(self.data) < self.max_size:\\n                self.data.append(element)\\n                to_return.append(element)\\n            else:\\n                if random.uniform(0, 1) > 0.5:\\n                    i = random.randint(0, self.max_size - 1)\\n                    to_return.append(self.data[i].clone())\\n                    self.data[i] = element\\n                else:\\n                    to_return.append(element)\\n        return Variable(torch.cat(to_return))\\n',\n",
       " '23746e82': 'class ImageDataset(Dataset):\\n    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\\n        self.transform = transforms.Compose(transforms_)\\n        self.unaligned = unaligned\\n\\n        self.files_A = sorted(glob.glob(os.path.join(root, f\"{mode}A\") + \"/*.*\"))\\n        self.files_B = sorted(glob.glob(os.path.join(root, f\"{mode}B\") + \"/*.*\"))\\n        if debug_mode:\\n            self.files_A = self.files_A[:100]\\n            self.files_B = self.files_B[:100]\\n\\n    def __getitem__(self, index):\\n        image_A = Image.open(self.files_A[index % len(self.files_A)])\\n\\n        if self.unaligned:\\n            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\\n        else:\\n            image_B = Image.open(self.files_B[index % len(self.files_B)])\\n\\n#         # Convert grayscale images to rgb\\n#         if image_A.mode != \"RGB\":\\n#             image_A = to_rgb(image_A)\\n#         if image_B.mode != \"RGB\":\\n#             image_B = to_rgb(image_B)\\n\\n        if np.random.random() < 0.5:\\n            image_A = Image.fromarray(np.array(image_A)[:, ::-1, :], \"RGB\")\\n            image_B = Image.fromarray(np.array(image_B)[:, ::-1, :], \"RGB\")\\n            \\n        item_A = self.transform(image_A)\\n        item_B = self.transform(image_B)\\n        return {\"A\": item_A, \"B\": item_B}\\n\\n    def __len__(self):\\n        return max(len(self.files_A), len(self.files_B))',\n",
       " 'cd31a689': '# Image transformations\\ntransforms_ = [\\n    transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\\n    transforms.RandomCrop((img_height, img_width)),\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\\n]\\n\\n# Training data loader\\ntrain_dataloader = DataLoader(\\n    ImageDataset(f\"{dataset_path}\", transforms_=transforms_, unaligned=False),\\n    batch_size=batch_size,\\n    shuffle=True,\\n    num_workers=n_workers,\\n)\\n# Test data loader\\ntest_dataloader = DataLoader(\\n    ImageDataset(f\"{dataset_path}\", transforms_=transforms_, unaligned=False, mode=\"test\"),\\n    batch_size=1,\\n    shuffle=True,\\n    num_workers=1,\\n)',\n",
       " 'f64a23b1': 'def weights_init_normal(m):\\n    classname = m.__class__.__name__\\n    if classname.find(\"Conv\") != -1:\\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\\n        if hasattr(m, \"bias\") and m.bias is not None:\\n            torch.nn.init.constant_(m.bias.data, 0.0)\\n    elif classname.find(\"BatchNorm2d\") != -1:\\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\\n        torch.nn.init.constant_(m.bias.data, 0.0)\\n\\n\\nclass ResidualBlock(nn.Module):\\n    def __init__(self, in_features):\\n        super(ResidualBlock, self).__init__()\\n\\n        self.block = nn.Sequential(\\n            nn.ReflectionPad2d(1),\\n            nn.Conv2d(in_features, in_features, 3),\\n            nn.InstanceNorm2d(in_features),\\n            nn.ReLU(inplace=True),\\n            nn.ReflectionPad2d(1),\\n            nn.Conv2d(in_features, in_features, 3),\\n            nn.InstanceNorm2d(in_features),\\n        )\\n\\n    def forward(self, x):\\n        return x + self.block(x)\\n\\n\\nclass GeneratorResNet(nn.Module):\\n    def __init__(self, input_shape, num_residual_blocks):\\n        super(GeneratorResNet, self).__init__()\\n\\n        channels = input_shape[0]\\n\\n        # Initial convolution block\\n        out_features = 64\\n        model = [\\n            nn.ReflectionPad2d(channels),\\n            nn.Conv2d(channels, out_features, 7),\\n            nn.InstanceNorm2d(out_features),\\n            nn.ReLU(inplace=True),\\n        ]\\n        in_features = out_features\\n\\n        # Downsampling\\n        for _ in range(2):\\n            out_features *= 2\\n            model += [\\n                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\\n                nn.InstanceNorm2d(out_features),\\n                nn.ReLU(inplace=True),\\n            ]\\n            in_features = out_features\\n\\n        # Residual blocks\\n        for _ in range(num_residual_blocks):\\n            model += [ResidualBlock(out_features)]\\n\\n        # Upsampling\\n        for _ in range(2):\\n            out_features //= 2\\n            model += [\\n                nn.Upsample(scale_factor=2),\\n                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\\n                nn.InstanceNorm2d(out_features),\\n                nn.ReLU(inplace=True),\\n            ]\\n            in_features = out_features\\n\\n        # Output layer\\n        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\\n\\n        self.model = nn.Sequential(*model)\\n\\n    def forward(self, x):\\n        return self.model(x)\\n\\n\\nclass Discriminator(nn.Module):\\n    def __init__(self, input_shape):\\n        super(Discriminator, self).__init__()\\n\\n        channels, height, width = input_shape\\n\\n        # Calculate output shape of image discriminator (PatchGAN)\\n        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\\n\\n        def discriminator_block(in_filters, out_filters, normalize=True):\\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\\n            if normalize:\\n                layers.append(nn.InstanceNorm2d(out_filters))\\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\\n            return layers\\n\\n        self.model = nn.Sequential(\\n            *discriminator_block(channels, 64, normalize=False),\\n            *discriminator_block(64, 128),\\n            *discriminator_block(128, 256),\\n            *discriminator_block(256, 512),\\n            nn.ZeroPad2d((1, 0, 1, 0)),\\n            nn.Conv2d(512, 1, 4, padding=1)\\n        )\\n\\n    def forward(self, img):\\n        return self.model(img)',\n",
       " 'f5a438d8': '# Losses\\ncriterion_GAN = torch.nn.MSELoss()\\ncriterion_cycle = torch.nn.L1Loss()\\ncriterion_identity = torch.nn.L1Loss()\\n\\ncuda = torch.cuda.is_available()\\n\\ninput_shape = (channels, img_height, img_width)\\n\\n# Initialize generator and discriminator\\nG_AB = GeneratorResNet(input_shape, n_residual_blocks)\\nG_BA = GeneratorResNet(input_shape, n_residual_blocks)\\nD_A = Discriminator(input_shape)\\nD_B = Discriminator(input_shape)\\n\\nif cuda:\\n    G_AB = G_AB.cuda()\\n    G_BA = G_BA.cuda()\\n    D_A = D_A.cuda()\\n    D_B = D_B.cuda()\\n    criterion_GAN.cuda()\\n    criterion_cycle.cuda()\\n    criterion_identity.cuda()\\n\\nif epoch_start != 0:\\n    # Load pretrained models\\n    G_AB.load_state_dict(torch.load(f\"{pretrained_model_path}/G_AB.pth\"))\\n    G_BA.load_state_dict(torch.load(f\"{pretrained_model_path}/G_BA.pth\"))\\n    D_A.load_state_dict(torch.load(f\"{pretrained_model_path}/D_A.pth\"))\\n    D_B.load_state_dict(torch.load(f\"{pretrained_model_path}/D_B.pth\"))\\nelse:\\n    # Initialize weights\\n    G_AB.apply(weights_init_normal)\\n    G_BA.apply(weights_init_normal)\\n    D_A.apply(weights_init_normal)\\n    D_B.apply(weights_init_normal)\\n\\n# Optimizers\\noptimizer_G = torch.optim.Adam(\\n    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\\n)\\noptimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\\noptimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\\n\\nTensor = torch.cuda.FloatTensor if cuda else torch.Tensor\\n\\n# Buffers of previously generated samples\\nfake_A_buffer = ReplayBuffer()\\nfake_B_buffer = ReplayBuffer()\\n\\ntrain_counter = []\\ntrain_losses_gen, train_losses_id, train_losses_gan, train_losses_cyc = [], [], [], []\\ntrain_losses_disc, train_losses_disc_a, train_losses_disc_b = [], [], []\\n\\ntest_counter = [2*idx*len(train_dataloader.dataset) for idx in range(epoch_start+1, n_epochs+1)]\\ntest_losses_gen, test_losses_disc = [], []\\n',\n",
       " 'd2895b93': 'for epoch in range(epoch_start, n_epochs):\\n    \\n    #### Training\\n    loss_gen = loss_id = loss_gan = loss_cyc = 0.0\\n    loss_disc = loss_disc_a = loss_disc_b = 0.0\\n    tqdm_bar = tqdm(train_dataloader, desc=f\\'Training Epoch {epoch} \\', total=int(len(train_dataloader)))\\n    for batch_idx, batch in enumerate(tqdm_bar):\\n\\n        # Set model input\\n        real_A = Variable(batch[\"A\"].type(Tensor))\\n        real_B = Variable(batch[\"B\"].type(Tensor))\\n        # Adversarial ground truths\\n        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\\n        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\\n\\n        ### Train Generators\\n        G_AB.train()\\n        G_BA.train()\\n        optimizer_G.zero_grad()\\n        # Identity loss\\n        loss_id_A = criterion_identity(G_BA(real_A), real_A)\\n        loss_id_B = criterion_identity(G_AB(real_B), real_B)\\n        loss_identity = (loss_id_A + loss_id_B) / 2\\n        # GAN loss\\n        fake_B = G_AB(real_A)\\n        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\\n        fake_A = G_BA(real_B)\\n        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\\n        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\\n        # Cycle loss\\n        recov_A = G_BA(fake_B)\\n        loss_cycle_A = criterion_cycle(recov_A, real_A)\\n        recov_B = G_AB(fake_A)\\n        loss_cycle_B = criterion_cycle(recov_B, real_B)\\n        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\\n        # Total loss\\n        loss_G = lambda_id * loss_identity + loss_GAN + lambda_cyc * loss_cycle\\n        loss_G.backward()\\n        optimizer_G.step()\\n\\n        ### Train Discriminator-A\\n        D_A.train()\\n        optimizer_D_A.zero_grad()\\n        # Real loss\\n        loss_real = criterion_GAN(D_A(real_A), valid)\\n        # Fake loss (on batch of previously generated samples)\\n        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\\n        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\\n        # Total loss\\n        loss_D_A = (loss_real + loss_fake) / 2\\n        loss_D_A.backward()\\n        optimizer_D_A.step()\\n\\n        ### Train Discriminator-B\\n        D_B.train()\\n        optimizer_D_B.zero_grad()\\n        # Real loss\\n        loss_real = criterion_GAN(D_B(real_B), valid)\\n        # Fake loss (on batch of previously generated samples)\\n        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\\n        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\\n        # Total loss\\n        loss_D_B = (loss_real + loss_fake) / 2\\n        loss_D_B.backward()\\n        optimizer_D_B.step()\\n        loss_D = (loss_D_A + loss_D_B) / 2\\n\\n        ### Log Progress\\n        loss_gen += loss_G.item(); loss_id += loss_identity.item(); loss_gan += loss_GAN.item(); loss_cyc += loss_cycle.item()\\n        loss_disc += loss_D.item(); loss_disc_a += loss_D_A.item(); loss_disc_b += loss_D_B.item()\\n        train_counter.append(2*(batch_idx*batch_size + real_A.size(0) + epoch*len(train_dataloader.dataset)))\\n        train_losses_gen.append(loss_G.item()); train_losses_id.append(loss_identity.item()); train_losses_gan.append(loss_GAN.item()); train_losses_cyc.append(loss_cycle.item())\\n        train_losses_disc.append(loss_D.item()); train_losses_disc_a.append(loss_D_A.item()); train_losses_disc_b.append(loss_D_B.item())\\n        tqdm_bar.set_postfix(Gen_loss=loss_gen/(batch_idx+1), identity=loss_id/(batch_idx+1), adv=loss_gan/(batch_idx+1), cycle=loss_cyc/(batch_idx+1),\\n                            Disc_loss=loss_disc/(batch_idx+1), disc_a=loss_disc_a/(batch_idx+1), disc_b=loss_disc_b/(batch_idx+1))\\n\\n    #### Testing\\n    loss_gen = loss_id = loss_gan = loss_cyc = 0.0\\n    loss_disc = loss_disc_a = loss_disc_b = 0.0\\n    tqdm_bar = tqdm(test_dataloader, desc=f\\'Testing Epoch {epoch} \\', total=int(len(test_dataloader)))\\n    for batch_idx, batch in enumerate(tqdm_bar):\\n\\n        # Set model input\\n        real_A = Variable(batch[\"A\"].type(Tensor))\\n        real_B = Variable(batch[\"B\"].type(Tensor))\\n        # Adversarial ground truths\\n        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\\n        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\\n\\n        ### Test Generators\\n        G_AB.eval()\\n        G_BA.eval()\\n        # Identity loss\\n        loss_id_A = criterion_identity(G_BA(real_A), real_A)\\n        loss_id_B = criterion_identity(G_AB(real_B), real_B)\\n        loss_identity = (loss_id_A + loss_id_B) / 2\\n        # GAN loss\\n        fake_B = G_AB(real_A)\\n        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\\n        fake_A = G_BA(real_B)\\n        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\\n        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\\n        # Cycle loss\\n        recov_A = G_BA(fake_B)\\n        loss_cycle_A = criterion_cycle(recov_A, real_A)\\n        recov_B = G_AB(fake_A)\\n        loss_cycle_B = criterion_cycle(recov_B, real_B)\\n        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\\n        # Total loss\\n        loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\\n\\n        ### Test Discriminator-A\\n        D_A.eval()\\n        # Real loss\\n        loss_real = criterion_GAN(D_A(real_A), valid)\\n        # Fake loss (on batch of previously generated samples)\\n        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\\n        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\\n        # Total loss\\n        loss_D_A = (loss_real + loss_fake) / 2\\n\\n        ### Test Discriminator-B\\n        D_B.eval()\\n        # Real loss\\n        loss_real = criterion_GAN(D_B(real_B), valid)\\n        # Fake loss (on batch of previously generated samples)\\n        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\\n        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\\n        # Total loss\\n        loss_D_B = (loss_real + loss_fake) / 2\\n        loss_D = (loss_D_A + loss_D_B) / 2\\n        \\n        ### Log Progress\\n        loss_gen += loss_G.item(); loss_id += loss_identity.item(); loss_gan += loss_GAN.item(); loss_cyc += loss_cycle.item()\\n        loss_disc += loss_D.item(); loss_disc_a += loss_D_A.item(); loss_disc_b += loss_D_B.item()\\n        tqdm_bar.set_postfix(Gen_loss=loss_gen/(batch_idx+1), identity=loss_id/(batch_idx+1), adv=loss_gan/(batch_idx+1), cycle=loss_cyc/(batch_idx+1),\\n                            Disc_loss=loss_disc/(batch_idx+1), disc_a=loss_disc_a/(batch_idx+1), disc_b=loss_disc_b/(batch_idx+1))\\n        \\n        # If at sample interval save image\\n        if random.uniform(0,1)<0.4:\\n            # Arrange images along x-axis\\n            real_A = make_grid(real_A, nrow=1, normalize=True)\\n            real_B = make_grid(real_B, nrow=1, normalize=True)\\n            fake_A = make_grid(fake_A, nrow=1, normalize=True)\\n            fake_B = make_grid(fake_B, nrow=1, normalize=True)\\n            # Arange images along y-axis\\n            image_grid = torch.cat((real_B, fake_A, real_A), -1)\\n            save_image(image_grid, f\"images/{batch_idx}.png\", normalize=False)\\n\\n    test_losses_gen.append(loss_gen/len(test_dataloader))\\n    test_losses_disc.append(loss_disc/len(test_dataloader))\\n\\n    # Save model checkpoints\\n    if np.argmin(test_losses_gen) == len(test_losses_gen)-1:\\n        # Save model checkpoints\\n        torch.save(G_AB.state_dict(), \"saved_models/G_AB.pth\")\\n        torch.save(G_BA.state_dict(), \"saved_models/G_BA.pth\")\\n        torch.save(D_A.state_dict(), \"saved_models/D_A.pth\")\\n        torch.save(D_B.state_dict(), \"saved_models/D_B.pth\")\\n        ',\n",
       " 'bec09551': 'fig = go.Figure()\\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_gen, mode=\\'lines\\', name=\\'Train Gen Loss (Loss_G)\\'))\\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_id, mode=\\'lines\\', name=\\'Train Gen Identity Loss\\'))\\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_gan, mode=\\'lines\\', name=\\'Train Gen GAN Loss\\'))\\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_cyc, mode=\\'lines\\', name=\\'Train Gen Cyclic Loss\\'))\\nfig.add_trace(go.Scatter(x=test_counter, y=test_losses_gen, marker_symbol=\\'star-diamond\\', \\n                         marker_color=\\'orange\\', marker_line_width=1, marker_size=9, mode=\\'markers\\', name=\\'Test Gen Loss (Loss_G)\\'))\\nfig.update_layout(\\n    width=1000,\\n    height=500,\\n    title=\"Train vs. Test Generator Loss\",\\n    xaxis_title=\"Number of training examples seen (A+B)\",\\n    yaxis_title=\"Generator Losses\"),\\nplotly.offline.plot(fig, filename = \\'plotly_gen_losses.html\\')\\nfig.show()',\n",
       " 'c4b4bd07': 'fig = go.Figure()\\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_disc, mode=\\'lines\\', name=\\'Train Disc Loss (Loss_D)\\'))\\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_disc_a, mode=\\'lines\\', name=\\'Train Disc-A Loss\\'))\\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_disc_b, mode=\\'lines\\', name=\\'Train Disc-B Loss\\'))\\nfig.add_trace(go.Scatter(x=test_counter, y=test_losses_disc, marker_symbol=\\'star-diamond\\', \\n                         marker_color=\\'orange\\', marker_line_width=1, marker_size=9, mode=\\'markers\\', name=\\'Test Disc Loss (Loss_G)\\'))\\nfig.update_layout(\\n    width=1000,\\n    height=500,\\n    title=\"Train vs. Test Discriminator Loss\",\\n    xaxis_title=\"Number of training examples seen (A+B)\",\\n    yaxis_title=\"Discriminator Losses\"),\\nplotly.offline.plot(fig, filename = \\'plotly_disc_losses.html\\')\\nfig.show()',\n",
       " '65309eb9': '!ls images',\n",
       " 'eedbe0bf': \"from IPython.display import Image\\nImage(filename='images/3.png') \",\n",
       " '23e348c1': '<h3><center>Model Architecture</center></h3>\\n<img src=\"https://miro.medium.com/max/700/1*_KxtJIVtZjVaxxl-Yl1vJg.png\" width=\"900\" height=\"900\"/>\\n<h4></h4>\\n<h4><center>Image Source:  <a href=\"https://arxiv.org/pdf/1703.10593.pdf\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks [C. Jun-Yan Zhu et al.]</a></center></h4>',\n",
       " 'c2b07542': '### Define Model Classes',\n",
       " 'cbf59bb6': '<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Pix2Pix-GAN-Translation-of-Semantic-Images-to-Photographs-of-Building-Facades.png\" width=\"500\" height=\"500\"/>\\n<h4></h4>\\n<h4><center>Image Source:  <a href=\"https://arxiv.org/pdf/1611.07004.pdf\">Image-to-Image Translation with Conditional Adversarial Networks [Phillip Isola et al.]</a></center></h4>',\n",
       " '854fe1a7': '### Get Train/Test Dataloaders',\n",
       " '766f80f7': '### Define Dataset Class',\n",
       " '4a6cf167': '### Train CycleGAN',\n",
       " 'e894e2b4': '### Define Utilities',\n",
       " '63ae167e': '## Introduction\\n### In this notebook we use [CycleGAN](https://arxiv.org/abs/1703.10593) to convert Building Facades to their Segmentations using [Facades Dataset](https://www.kaggle.com/balraj98/facades-dataset).',\n",
       " '7a40a5fb': '### Libraries 📚⬇',\n",
       " 'c7b5daac': '### Work in Progress ...\\n\\n### Note: The order of the below saved images are (from left-to-right): \\n\\n* Real Facade\\n* Fake Building Segmentation (from Gen-AB)\\n* Real Building Segmentation\\n* Fake Facade (from Gen-BA)',\n",
       " '283bc152': '### Settings ⚙️',\n",
       " '61fb4fd0': '!pip install chart_studio',\n",
       " '4e2113da': ' !pip install --upgrade pip',\n",
       " '0031ef67': '# This Python 3 environment comes with many helpful analytics libraries installed\\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\\n# For example, here\\'s several helpful packages to load\\n\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\nimport plotly.graph_objs as go\\nimport matplotlib.pyplot as plt\\nimport chart_studio.plotly as py\\nimport seaborn as sns\\n# Input data files are available in the read-only \"../input/\" directory\\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\n\\nimport os\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \\n# You can also write temporary files to /kaggle/temp/, but they won\\'t be saved outside of the current session',\n",
       " '93a78904': '\\nfrom plotly.offline import iplot, init_notebook_mode\\nimport cufflinks\\ncufflinks.go_offline(connected=True)\\ninit_notebook_mode(connected=True)',\n",
       " '49a05c4a': \"\\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\\ntest=pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\",\n",
       " 'a22cd819': 'train.head()',\n",
       " '7ec4ecab': 'test.head()',\n",
       " '95e6abda': 'train.columns',\n",
       " '92849b36': 'train.describe().T',\n",
       " '2fd2b67c': 'train.info()',\n",
       " '3a98031d': 'f, ax =plt.subplots(figsize=(50,30))\\nsns.heatmap(train.corr(),annot=True, fmt=\"1f\", ax=ax)\\nplt.show()',\n",
       " 'd5cc1e3d': 'train_data=train[[\"Id\",\"OverallQual\" , \"YearBuilt\" , \"YearRemodAdd\", \"TotalBsmtSF\", \"1stFlrSF\", \\n                  \"GrLivArea\", \"FullBath\" , \"TotRmsAbvGrd\", \"GarageCars\", \"GarageArea\",\"MasVnrArea\",\"Fireplaces\",\"SalePrice\"]]',\n",
       " '499af2e6': 'train_data.head()',\n",
       " '0051d330': 'train_data.isnull().sum()',\n",
       " '97e872c1': 'train_data.describe().T',\n",
       " '5274923e': 'train_data=train_data.rename(columns={\"1stFlrSF\":\"FstFlrSF\"})',\n",
       " '7fbf59f2': 'train_data.columns',\n",
       " 'ed3d1b53': 'train_data.info()',\n",
       " '6a550680': 'train_data= train_data.astype(float)\\ntrain_data.info()',\n",
       " '807d6f3a': 'convert={\"Id\": int,\\n            \"SalePrice\": int}\\ntrain_data=train_data.astype(convert)\\ntrain_data.info()\\n',\n",
       " '4493082f': 'train_data.MasVnrArea.describe()    ',\n",
       " '0969d0e4': 'def plot_hist(variable):\\n    plt.figure(figsize=(7,3))\\n    plt.hist(train_data[variable],bins=30)\\n    plt.xlabel(variable)\\n    plt.title(\"{} abcde \".format(variable))\\n    plt.show()\\n    \\n',\n",
       " '11db2075': \"numeric=[ 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF',\\n       'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea',\\n       'MasVnrArea', 'Fireplaces', 'SalePrice', 'FstFlrSF'   ]\\nfor n in numeric:\\n    plot_hist(n)\",\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_cellid_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1c34f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/innoacad05/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/innoacad05/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "# import fasttext\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "        #return document\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        return preprocessed_text\n",
    "\n",
    "    \n",
    "def preprocess_df(df):\n",
    "    \"\"\"\n",
    "    This function is for processing sorce of notebook\n",
    "    returns preprocessed dataframe\n",
    "    \"\"\"\n",
    "    return [preprocess_text(message) for message in df.source]\n",
    "\n",
    "df.source = df.source.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70e0aec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>rank</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>pct_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>386d31f0</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy import panda import tensorflow import tf2_0_baseline_w_bert tf2baseline script import tf2_0_baseline_w_...</td>\n",
       "      <td>2</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>16435878</td>\n",
       "      <td>code</td>\n",
       "      <td>del_all_flags flag flags_dict flag _flags keys_list flags_dict keys_list flag __delattr__ del_all_flags absl flag fl...</td>\n",
       "      <td>4</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "      <td>0.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>f4bb282f</td>\n",
       "      <td>code</td>\n",
       "      <td>bert_config modeling bertconfig from_json_file flag bert_config_file tf2baseline validate_flags_or_throw bert_config...</td>\n",
       "      <td>6</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>4cc5ee5a</td>\n",
       "      <td>code</td>\n",
       "      <td>test_answers_df read_json kaggle working prediction json</td>\n",
       "      <td>8</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>215ec8c0</td>\n",
       "      <td>code</td>\n",
       "      <td>create_short_answer entry entry short_answers_score return answer short_answer entry short_answers short_answer star...</td>\n",
       "      <td>10</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "      <td>0.47619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id   cell_id cell_type                                                                                                                   source rank ancestor_id  \\\n",
       "0  003f36ab2c577d  386d31f0      code  import numpy import panda import tensorflow import tf2_0_baseline_w_bert tf2baseline script import tf2_0_baseline_w_...    2    8508be37   \n",
       "1  003f36ab2c577d  16435878      code  del_all_flags flag flags_dict flag _flags keys_list flags_dict keys_list flag __delattr__ del_all_flags absl flag fl...    4    8508be37   \n",
       "2  003f36ab2c577d  f4bb282f      code  bert_config modeling bertconfig from_json_file flag bert_config_file tf2baseline validate_flags_or_throw bert_config...    6    8508be37   \n",
       "3  003f36ab2c577d  4cc5ee5a      code                                                                 test_answers_df read_json kaggle working prediction json    8    8508be37   \n",
       "4  003f36ab2c577d  215ec8c0      code  create_short_answer entry entry short_answers_score return answer short_answer entry short_answers short_answer star...   10    8508be37   \n",
       "\n",
       "        parent_id  pct_rank  \n",
       "0  3bde8d65a3508b  0.095238  \n",
       "1  3bde8d65a3508b  0.190476  \n",
       "2  3bde8d65a3508b  0.285714  \n",
       "3  3bde8d65a3508b  0.380952  \n",
       "4  3bde8d65a3508b   0.47619  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f47897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "try:\n",
    "    from transformers import DistilBertModel, DistilBertTokenizer\n",
    "except:\n",
    "  !pip install transformers\n",
    "  from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5da592b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 743.32it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_triplet(df, mode='train'):\n",
    "    triplets = []\n",
    "    ids = df.id.unique() #notebook id\n",
    "    random_drop = np.random.random(size=10000)>0.9\n",
    "    count = 0\n",
    "\n",
    "    #df의 모든 노트북 반복\n",
    "    for id, df_tmp in tqdm(df.groupby('id')): #같은 노트북\n",
    "        df_tmp_markdown = df_tmp[df_tmp['cell_type']=='markdown'] #마크다운만\n",
    "\n",
    "        df_tmp_code = df_tmp[df_tmp['cell_type']=='code'] #코드셀\n",
    "        df_tmp_code_rank = df_tmp_code['rank'].values #코드셀 rank(순서)\n",
    "        df_tmp_code_cell_id = df_tmp_code['cell_id'].values #코드셀 셀id\n",
    "\n",
    "        for cell_id, rank in df_tmp_markdown[['cell_id', 'rank']].values: #마크다운 셀id, 랭크만 가져와서\n",
    "            labels = np.array([(r==(rank+1)) for r in df_tmp_code_rank]).astype('int') #다음 순서의 코드셀 저장\n",
    "\n",
    "            for cid, label in zip(df_tmp_code_cell_id, labels): #코드셀 id랑, 라벨 가져와서\n",
    "                count += 1\n",
    "                if label==1:    #다음 코드셀인거 일단 짝짝쿵은 다 가져옴\n",
    "                    triplets.append( [cell_id, cid, label] ) #마크다운셀id, 코드셀id, 라벨 저장\n",
    "                  # triplets.append( [cid, cell_id, label] )\n",
    "                elif mode == 'test': #테스트 모드면 전부 다 넣는듯\n",
    "                    triplets.append( [cell_id, cid, label] )\n",
    "                  # triplets.append( [cid, cell_id, label] )\n",
    "                elif random_drop[count%10000]: #0~1만 중 걸리면(0.1확률) - 왜????? 아 1은 상관있는거 0은 상관없는걸로 학습??\n",
    "                    triplets.append( [cell_id, cid, label] )\n",
    "                  # triplets.append( [cid, cell_id, label] )\n",
    "\n",
    "    return triplets\n",
    "\n",
    "triplets = generate_triplet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "55f16711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 771.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['3e1430c4', '4cc5ee5a', 1],\n",
       " ['6f70d84e', '16435878', 0],\n",
       " ['6f70d84e', '4cc5ee5a', 0],\n",
       " ['6f70d84e', '215ec8c0', 0],\n",
       " ['6f70d84e', '1d664ca8', 1],\n",
       " ['da99f684', '386d31f0', 1],\n",
       " ['da99f684', '4cc5ee5a', 0],\n",
       " ['db8c69de', '215ec8c0', 1],\n",
       " ['5948bc1a', '386d31f0', 0],\n",
       " ['5948bc1a', 'a3f4e6bc', 1],\n",
       " ['00070116', '05bc949b', 1],\n",
       " ['82dfbe9a', '16435878', 1],\n",
       " ['6d31400d', '16435878', 0],\n",
       " ['6d31400d', 'f4bb282f', 1],\n",
       " ['6d31400d', '1d664ca8', 0],\n",
       " ['6d31400d', '5c545af1', 0],\n",
       " ['c47a4a8e', '5c545af1', 1],\n",
       " ['c47a4a8e', 'a3f4e6bc', 0],\n",
       " ['42435be9', '3f201013', 0],\n",
       " ['42435be9', 'eb645be7', 0],\n",
       " ['8f4c4034', '7b78796e', 0],\n",
       " ['8f4c4034', '5d3d89e1', 0],\n",
       " ['8f4c4034', '41f19673', 1],\n",
       " ['0fa88287', '4afe7f61', 1],\n",
       " ['0fa88287', 'c878e995', 0],\n",
       " ['ed9b90a1', 'd416ce6e', 0],\n",
       " ['ed9b90a1', 'b4c1e48e', 0],\n",
       " ['ed9b90a1', '41f19673', 0],\n",
       " ['ed9b90a1', 'c878e995', 1],\n",
       " ['63466cff', '19a6b2db', 0],\n",
       " ['63466cff', '6583d7cf', 1],\n",
       " ['8f191e71', '41f19673', 0],\n",
       " ['8f191e71', '8bf7dfb5', 1],\n",
       " ['8f191e71', 'c878e995', 0],\n",
       " ['d0b90d9f', 'ca4cd2d0', 1],\n",
       " ['d0b90d9f', '8bf7dfb5', 0],\n",
       " ['fb163203', '460df5f7', 0],\n",
       " ['fb163203', '5d3d89e1', 1],\n",
       " ['fb163203', 'ae860db6', 0],\n",
       " ['fb163203', '826b624a', 0],\n",
       " ['fb163203', '6583d7cf', 0],\n",
       " ['fb163203', '302daec4', 0],\n",
       " ['f889af48', 'ae860db6', 1],\n",
       " ['f889af48', '826b624a', 0],\n",
       " ['5481f265', 'ae860db6', 0],\n",
       " ['5481f265', '19a6b2db', 1],\n",
       " ['2c1fd81e', '6583d7cf', 0],\n",
       " ['2c1fd81e', '302daec4', 1],\n",
       " ['7a879e36', '826b624a', 1],\n",
       " ['7a879e36', 'efde5525', 0],\n",
       " ['7a879e36', '8bf7dfb5', 0],\n",
       " ['7f6344c1', 'b4c1e48e', 0],\n",
       " ['e25b2b2a', '460df5f7', 1],\n",
       " ['e25b2b2a', 'ca4cd2d0', 0],\n",
       " ['e25b2b2a', '7b78796e', 0],\n",
       " ['e25b2b2a', 'c878e995', 0],\n",
       " ['865a0b14', 'b9306c2c', 1],\n",
       " ['b93f36f5', '5dba6015', 0],\n",
       " ['b93f36f5', '0fd50bd1', 1],\n",
       " ['03fd7249', 'a3c06efd', 1],\n",
       " ['03fd7249', '96185336', 0],\n",
       " ['1462fe2d', 'b9dd9319', 0],\n",
       " ['1462fe2d', 'a7664eea', 0],\n",
       " ['1462fe2d', 'e59d7a27', 1],\n",
       " ['c9022046', '92bf3829', 0],\n",
       " ['c9022046', 'a9025bbe', 0],\n",
       " ['4a01d292', 'b9dd9319', 1],\n",
       " ['4a01d292', 'a3c06efd', 0],\n",
       " ['4a01d292', '0fd50bd1', 0],\n",
       " ['48f3e5c1', 'a3c06efd', 0],\n",
       " ['48f3e5c1', 'b9306c2c', 0],\n",
       " ['ca747aad', '221015b2', 0],\n",
       " ['ca747aad', 'ac47f2f2', 0],\n",
       " ['ca747aad', '5c9dac3b', 0],\n",
       " ['ca747aad', '4c9ff241', 1],\n",
       " ['ca747aad', 'd04035ea', 0],\n",
       " ['ca747aad', '4494e523', 0],\n",
       " ['cb6e2f09', 'f043f193', 0],\n",
       " ['cb6e2f09', '795ebebe', 0],\n",
       " ['cb6e2f09', '5c9dac3b', 1],\n",
       " ['cb6e2f09', '0d7bed1f', 0],\n",
       " ['cb6e2f09', 'f291547b', 0],\n",
       " ['e3e48606', '34afcde9', 0],\n",
       " ['e3e48606', 'bcc26792', 1],\n",
       " ['e3e48606', 'e764ae02', 0],\n",
       " ['e3e48606', '0d7bed1f', 0],\n",
       " ['fe946a9b', 'a1d3774c', 0],\n",
       " ['fe946a9b', '12df8e29', 1],\n",
       " ['fbc3628e', 'ba20b0e4', 1],\n",
       " ['fbc3628e', '486d6242', 0],\n",
       " ['fbc3628e', '58fe7a9c', 0],\n",
       " ['fbc3628e', 'e764ae02', 0],\n",
       " ['fbc3628e', '0d7bed1f', 0],\n",
       " ['fbc3628e', '1e95cfc0', 0],\n",
       " ['7954f4d9', 'd96303eb', 0],\n",
       " ['7954f4d9', 'b3d62a1d', 1],\n",
       " ['7954f4d9', 'f043f193', 0],\n",
       " ['7954f4d9', '12df8e29', 0],\n",
       " ['7954f4d9', 'd7bb2b6f', 0],\n",
       " ['7954f4d9', '24346bbb', 0],\n",
       " ['cca3fa5b', '5c9dac3b', 0],\n",
       " ['cca3fa5b', '58fe7a9c', 0],\n",
       " ['cca3fa5b', '0df458c1', 1],\n",
       " ['10294a1c', '00128465', 1],\n",
       " ['a6c0fb8d', '117259a6', 1],\n",
       " ['d4f1b5a4', 'af0d6b61', 1],\n",
       " ['3f722894', 'c5ac6e90', 0],\n",
       " ['15967937', '8bb7e032', 1],\n",
       " ['a2a42654', '77d5c4af', 0],\n",
       " ['a2a42654', 'c5ac6e90', 1],\n",
       " ['72d36517', '6e25c79a', 1],\n",
       " ['ae086644', '77d5c4af', 1],\n",
       " ['c86e1675', 'd588c19e', 0],\n",
       " ['c86e1675', '28cbae69', 1],\n",
       " ['c86e1675', '7a7f4f7d', 0],\n",
       " ['c86e1675', 'fd98c389', 0],\n",
       " ['c86e1675', '60e0c439', 0],\n",
       " ['c86e1675', 'd6c76a6f', 0],\n",
       " ['69605797', '779806e1', 0],\n",
       " ['69605797', 'fd98c389', 1],\n",
       " ['3a09ce83', '73668a3c', 1],\n",
       " ['3a09ce83', '6bf42393', 0],\n",
       " ['3a09ce83', 'f3f85cd9', 0],\n",
       " ['34b84b0f', 'dc1bf2e3', 1],\n",
       " ['34b84b0f', '779806e1', 0],\n",
       " ['ef7b9113', 'd588c19e', 0],\n",
       " ['ef7b9113', '86ee7bc5', 1],\n",
       " ['ef7b9113', '0d3f0522', 0],\n",
       " ['ef7b9113', 'd5f34f87', 0],\n",
       " ['ef7b9113', 'd512e517', 0],\n",
       " ['8daed3b5', '6bf42393', 1],\n",
       " ['8daed3b5', 'a519275d', 0],\n",
       " ['54ecc51e', 'cfa42aac', 0],\n",
       " ['54ecc51e', '60e0c439', 0],\n",
       " ['54ecc51e', 'dc0acbee', 1],\n",
       " ['54ecc51e', 'd6c76a6f', 0],\n",
       " ['54ecc51e', '22121e99', 0],\n",
       " ['1c1a114b', '4d5bfc0b', 1],\n",
       " ['1c1a114b', '73668a3c', 0],\n",
       " ['1c1a114b', '9278bf89', 0],\n",
       " ['1c1a114b', '3383b768', 0],\n",
       " ['1c1a114b', 'd5f34f87', 0],\n",
       " ['1c1a114b', 'd6c76a6f', 0],\n",
       " ['cc2f55fa', 'cfa42aac', 1],\n",
       " ['cc2f55fa', '779806e1', 0],\n",
       " ['cc2f55fa', '22121e99', 0],\n",
       " ['c71154d0', '6bf42393', 0],\n",
       " ['c71154d0', '60e0c439', 1],\n",
       " ['25e0db6f', '3007a89e', 0],\n",
       " ['25e0db6f', 'd5f34f87', 0],\n",
       " ['25e0db6f', 'd512e517', 1],\n",
       " ['9631545a', 'f5755cf7', 0],\n",
       " ['9631545a', 'd6c76a6f', 0],\n",
       " ['f033ec4a', '0cf8ad66', 0],\n",
       " ['f033ec4a', '9bd3c757', 1],\n",
       " ['f033ec4a', '467aba77', 0],\n",
       " ['f033ec4a', 'd9031789', 0],\n",
       " ['f033ec4a', '25f029e1', 0],\n",
       " ['b73636ab', '0cf8ad66', 1],\n",
       " ['b73636ab', '2e77b8d4', 0],\n",
       " ['b73636ab', '94744bda', 0],\n",
       " ['b73636ab', '99276ddc', 0],\n",
       " ['7fb58814', 'edeab1bd', 0],\n",
       " ['7fb58814', 'd5aa911b', 1],\n",
       " ['7fb58814', '3bfbaaf9', 0],\n",
       " ['7fb58814', 'aa669d4b', 0],\n",
       " ['7fb58814', '3bc51d8a', 0],\n",
       " ['ecaf3a5a', '4fe6a325', 1],\n",
       " ['083dceec', 'b4e07898', 0],\n",
       " ['083dceec', 'ba34dc93', 0],\n",
       " ['083dceec', '7dfb24af', 1],\n",
       " ['ee01a50c', 'd19139de', 1],\n",
       " ['ee01a50c', 'a1b3079d', 0],\n",
       " ['ea959649', '51e0a9f1', 1],\n",
       " ['ea959649', 'b5cc736d', 0],\n",
       " ['39cd16e9', '2cdd9f91', 0],\n",
       " ['39cd16e9', 'b5cc736d', 1],\n",
       " ['69cc277f', 'db29fb23', 1],\n",
       " ['61eab1c8', '2cdd9f91', 1],\n",
       " ['61eab1c8', 'b4e07898', 0],\n",
       " ['61eab1c8', 'db29fb23', 0],\n",
       " ['5827ff9f', 'ba34dc93', 1],\n",
       " ['5827ff9f', '7dfb24af', 0],\n",
       " ['c41f095c', '2cdd9f91', 0],\n",
       " ['c41f095c', '7dfb24af', 0],\n",
       " ['c41f095c', 'a1b3079d', 0],\n",
       " ['fd1aaef8', 'b5cc736d', 0],\n",
       " ['7bb19556', 'ee592fff', 1],\n",
       " ['7bb19556', 'b5cc736d', 0],\n",
       " ['c52a6a23', '9bde08c8', 1],\n",
       " ['38afd5a3', 'c01d0fc4', 0],\n",
       " ['38afd5a3', '1a7eb826', 1],\n",
       " ['38afd5a3', 'db29fb23', 0],\n",
       " ['5fde459d', 'd19139de', 0],\n",
       " ['5fde459d', '7dfb24af', 0],\n",
       " ['82a9b510', 'b4e07898', 0],\n",
       " ['82a9b510', 'e77982fe', 1],\n",
       " ['e7520d65', 'c4ea69e9', 0],\n",
       " ['6946d75b', 'c9024896', 0],\n",
       " ['6946d75b', '0c2dad1e', 0],\n",
       " ['6946d75b', 'a958bfcb', 1],\n",
       " ['4e6deb8b', 'b4f5f09c', 0],\n",
       " ['4e6deb8b', '5cfe684b', 0],\n",
       " ['625581d8', '5cfe684b', 1],\n",
       " ['80b827ef', 'c9024896', 0],\n",
       " ['80b827ef', '0c2dad1e', 1],\n",
       " ['80b827ef', '37613c0e', 0],\n",
       " ['568f9b30', 'db1cb572', 0],\n",
       " ['568f9b30', 'b4f5f09c', 0],\n",
       " ['568f9b30', 'a0f32aa9', 1],\n",
       " ['2ac76792', 'c4ea69e9', 1],\n",
       " ['2ac76792', '4800dadf', 0],\n",
       " ['251abd36', 'db1cb572', 0],\n",
       " ['251abd36', '37613c0e', 1],\n",
       " ['a4c2e4d4', '7f9304a6', 1],\n",
       " ['a4c2e4d4', 'b4f5f09c', 0],\n",
       " ['a4c2e4d4', 'a0f32aa9', 0],\n",
       " ['a4c2e4d4', '4dc7f2eb', 0],\n",
       " ['a4c2e4d4', 'c1918db4', 0],\n",
       " ['501fd057', 'd1e2af42', 0],\n",
       " ['4ac69404', 'db1cb572', 1],\n",
       " ['4ac69404', 'a958bfcb', 0],\n",
       " ['4ac69404', 'd1e2af42', 0],\n",
       " ['46da7795', 'c4ea69e9', 0],\n",
       " ['46da7795', 'a958bfcb', 0],\n",
       " ['fa858d63', 'cdb63c16', 1],\n",
       " ['e8892a08', 'c9024896', 1],\n",
       " ['e8892a08', '4dc7f2eb', 0],\n",
       " ['e8892a08', 'c1918db4', 0],\n",
       " ['029575b9', 'b62843e9', 1],\n",
       " ['029575b9', 'fb2cfdd7', 0],\n",
       " ['8eef5c79', 'fd83f613', 0],\n",
       " ['8eef5c79', 'aa003a73', 0],\n",
       " ['f05c7454', '728af292', 1],\n",
       " ['66802a75', 'fb2cfdd7', 1],\n",
       " ['f402e2c0', 'fd83f613', 0],\n",
       " ['f402e2c0', 'c1511619', 0],\n",
       " ['f402e2c0', 'b80c1c38', 1],\n",
       " ['dd44df15', 'fd83f613', 0],\n",
       " ['dd44df15', '390a703b', 1],\n",
       " ['dd44df15', '6ad474a3', 0],\n",
       " ['d32fbcbf', '0bd60a54', 1],\n",
       " ['d32fbcbf', '340f88a6', 0],\n",
       " ['f481fecf', '0bd60a54', 0],\n",
       " ['f481fecf', 'fd83f613', 1],\n",
       " ['beb352ac', 'd4eaa1ce', 0],\n",
       " ['beb352ac', 'aa003a73', 1],\n",
       " ['e8dde324', '0bd60a54', 0],\n",
       " ['e8dde324', '340f88a6', 0],\n",
       " ['e8dde324', 'c1511619', 1],\n",
       " ['e8dde324', 'b62843e9', 0],\n",
       " ['699874eb', '40e3a551', 0],\n",
       " ['699874eb', 'b62843e9', 0],\n",
       " ['699874eb', 'ad22c336', 1],\n",
       " ['626f2a56', '390a703b', 0],\n",
       " ['626f2a56', 'aa003a73', 0],\n",
       " ['3b5af627', 'd4eaa1ce', 1],\n",
       " ['3b5af627', 'b2cb25ad', 0],\n",
       " ['03d74874', '6ad474a3', 1],\n",
       " ['03d74874', 'aa003a73', 0],\n",
       " ['57e4065d', 'b2cb25ad', 1],\n",
       " ['57e4065d', 'aa003a73', 0],\n",
       " ['86ddc034', 'fd83f613', 0],\n",
       " ['c1e2f0c4', '4045f388', 1],\n",
       " ['c1e2f0c4', '1bae3f1f', 0],\n",
       " ['c1e2f0c4', '5828e58b', 0],\n",
       " ['c1e2f0c4', '626e9aa7', 0],\n",
       " ['d4e1a9cf', '9c36a1b0', 0],\n",
       " ['d4e1a9cf', '4038e62d', 1],\n",
       " ['99da21f8', '5a3449b5', 0],\n",
       " ['99da21f8', '4038e62d', 0],\n",
       " ['99da21f8', 'b166e441', 0],\n",
       " ['99da21f8', '2886a196', 0],\n",
       " ['99da21f8', 'c71b4495', 1],\n",
       " ['7437fe2d', '5151923d', 0],\n",
       " ['7437fe2d', '257d8025', 1],\n",
       " ['7437fe2d', '3557048c', 0],\n",
       " ['3c84f64e', '9da3e24b', 0],\n",
       " ['3c84f64e', '2d7ba0c7', 0],\n",
       " ['3c84f64e', '54667867', 1],\n",
       " ['04705bb9', '0ce1e568', 0],\n",
       " ['04705bb9', '5150b310', 1],\n",
       " ['14c8258e', '2886a196', 1],\n",
       " ['14c8258e', '54667867', 0],\n",
       " ['dbeea067', '5151923d', 1],\n",
       " ['dbeea067', '4038e62d', 0],\n",
       " ['dbeea067', '4a17dd4b', 0],\n",
       " ['dbeea067', '9b4a0a28', 0],\n",
       " ['dbeea067', 'fea21eb2', 0],\n",
       " ['4ef2aa43', '9c36a1b0', 0],\n",
       " ['4ef2aa43', '62018c21', 0],\n",
       " ['4ef2aa43', 'b166e441', 1],\n",
       " ['4ef2aa43', '257d8025', 0],\n",
       " ['4ef2aa43', '9b4a0a28', 0],\n",
       " ['4ef2aa43', '54667867', 0],\n",
       " ['fd2cb432', '5151923d', 0],\n",
       " ['fd2cb432', '62018c21', 1],\n",
       " ['fd2cb432', '290e7ade', 0],\n",
       " ['fd2cb432', '342dbb27', 0],\n",
       " ['fd2cb432', '0ce1e568', 0],\n",
       " ['fd2cb432', 'fea21eb2', 0],\n",
       " ['fd2cb432', 'd5a884e1', 0],\n",
       " ['03e11b5a', '4a17dd4b', 1],\n",
       " ['7f740633', '5572345c', 0],\n",
       " ['7f740633', '749fe3f0', 0],\n",
       " ['7f740633', '6461987d', 0],\n",
       " ['7f740633', '546b3a54', 0],\n",
       " ['7f740633', '761bd9c9', 0],\n",
       " ['847b28dc', '749fe3f0', 1],\n",
       " ['847b28dc', '546b3a54', 0],\n",
       " ['847b28dc', '761bd9c9', 0],\n",
       " ['6352dc4e', '5572345c', 0],\n",
       " ['6352dc4e', '5470adae', 0],\n",
       " ['6352dc4e', 'f6d7a421', 0],\n",
       " ['d9691db9', '5470adae', 1],\n",
       " ['d9691db9', 'd89471db', 0],\n",
       " ['d9691db9', '5e903465', 0],\n",
       " ['62283e3c', '749fe3f0', 0],\n",
       " ['62283e3c', '5e903465', 1],\n",
       " ['62283e3c', '761bd9c9', 0],\n",
       " ['62283e3c', '1851e4c5', 0],\n",
       " ['4433de42', 'd4000564', 0],\n",
       " ['4433de42', '677e5b80', 0],\n",
       " ['4433de42', '421c60e2', 0],\n",
       " ['4433de42', '241327e9', 0],\n",
       " ['4433de42', '546b3a54', 0],\n",
       " ['4433de42', '53fe11f5', 1],\n",
       " ['17b233cd', '421c60e2', 0],\n",
       " ['17b233cd', '546b3a54', 0],\n",
       " ['60228590', 'd4000564', 0],\n",
       " ['60228590', 'f2fe6a17', 0],\n",
       " ['60228590', '45777470', 1],\n",
       " ['8046ddc4', 'f6d7a421', 0],\n",
       " ['8046ddc4', '5e903465', 0],\n",
       " ['a6d91a9c', '5470adae', 0],\n",
       " ['a6d91a9c', '677e5b80', 1],\n",
       " ['a6d91a9c', 'f6d7a421', 0],\n",
       " ['1f0a14c4', 'e7589012', 1],\n",
       " ['1f0a14c4', '45777470', 0],\n",
       " ['1f0a14c4', 'f0d17f79', 0],\n",
       " ['617cd0e5', 'e7589012', 0],\n",
       " ['617cd0e5', '677e5b80', 0],\n",
       " ['617cd0e5', '6461987d', 1],\n",
       " ['617cd0e5', '53fe11f5', 0],\n",
       " ['f1752231', 'd4000564', 1],\n",
       " ['f1752231', '6461987d', 0],\n",
       " ['9ede5940', '749fe3f0', 0],\n",
       " ['9ede5940', 'd89471db', 1],\n",
       " ['9ede5940', 'f0d17f79', 0],\n",
       " ['9ede5940', 'bb37fef7', 0],\n",
       " ['9ede5940', '1851e4c5', 0],\n",
       " ['59a7ac64', '241327e9', 0],\n",
       " ['59a7ac64', '1851e4c5', 0],\n",
       " ['f33acb49', '5572345c', 0],\n",
       " ['f33acb49', '1851e4c5', 0],\n",
       " ['35b4ff8c', '749fe3f0', 0],\n",
       " ['35b4ff8c', '5470adae', 0],\n",
       " ['35b4ff8c', 'e78c064c', 0],\n",
       " ['35b4ff8c', '761bd9c9', 0],\n",
       " ['4012a3dc', 'e7589012', 0],\n",
       " ['4012a3dc', '761bd9c9', 0],\n",
       " ['4012a3dc', '3861016f', 1],\n",
       " ['c9af2505', '5470adae', 0],\n",
       " ['fdf8cb2c', '5572345c', 1],\n",
       " ['fdf8cb2c', '5e903465', 0],\n",
       " ['fdf8cb2c', 'bb37fef7', 0],\n",
       " ['d4c69cec', '5470adae', 0],\n",
       " ['d4c69cec', '241327e9', 1],\n",
       " ['d4c69cec', 'f2fe6a17', 0],\n",
       " ['414da35c', 'e26ac330', 0],\n",
       " ['414da35c', '9466b2c5', 0],\n",
       " ['b43dc5ad', '4b3390be', 0],\n",
       " ['b43dc5ad', '09e8df3c', 1],\n",
       " ['b43dc5ad', 'c3b31640', 0],\n",
       " ['b43dc5ad', '3e289cca', 0],\n",
       " ['b43dc5ad', 'eb84ec44', 0],\n",
       " ['fcd9586c', '7147f794', 0],\n",
       " ['fcd9586c', '664f674f', 1],\n",
       " ['fcd9586c', '5b426fb2', 0],\n",
       " ['fcd9586c', '8a854e05', 0],\n",
       " ['3a31c74e', '7e8aaaff', 0],\n",
       " ['3a31c74e', 'cbc054d4', 1],\n",
       " ['3a31c74e', 'c2f1bc3f', 0],\n",
       " ['3a31c74e', 'eb84ec44', 0],\n",
       " ['c59ee953', '1345d7ca', 0],\n",
       " ['c59ee953', '4f1dae94', 0],\n",
       " ['c59ee953', '7e73b091', 0],\n",
       " ['c59ee953', '09e8df3c', 0],\n",
       " ['c59ee953', 'f4bec5e3', 0],\n",
       " ['c59ee953', '58631026', 0],\n",
       " ['42143d30', '86664b5d', 0],\n",
       " ['42143d30', '4b3390be', 1],\n",
       " ['42143d30', '8a854e05', 0],\n",
       " ['42143d30', '4a34e604', 0],\n",
       " ['bc685ff0', '8608de0d', 1],\n",
       " ['bc685ff0', '9919f572', 0],\n",
       " ['bc685ff0', '4b3390be', 0],\n",
       " ['bc685ff0', '0002fd8e', 0],\n",
       " ['bc685ff0', '4f95c9f7', 0],\n",
       " ['bc685ff0', '56ffd650', 0],\n",
       " ['f2f61ed0', '4f1dae94', 0],\n",
       " ['f2f61ed0', '7147f794', 0],\n",
       " ['f2f61ed0', '664f674f', 0],\n",
       " ['f2f61ed0', '5b426fb2', 0],\n",
       " ['f2f61ed0', '09e8df3c', 0],\n",
       " ['f2f61ed0', '58631026', 0],\n",
       " ['2ec33fd4', 'e26ac330', 0],\n",
       " ['2ec33fd4', '9466b2c5', 0],\n",
       " ['2ec33fd4', '4a34e604', 1],\n",
       " ['2ec33fd4', '56ffd650', 0],\n",
       " ['2ec33fd4', '0b0a8689', 0],\n",
       " ['e81152a0', 'febf0ee9', 0],\n",
       " ['e81152a0', '7e8aaaff', 0],\n",
       " ['e81152a0', 'e26ac330', 1],\n",
       " ['e81152a0', '9466b2c5', 0],\n",
       " ['e81152a0', '0b0a8689', 0],\n",
       " ['e81152a0', 'dd6adea4', 0],\n",
       " ['40f6947c', '7e8aaaff', 0],\n",
       " ['40f6947c', 'cbc054d4', 0],\n",
       " ['40f6947c', '8a854e05', 1],\n",
       " ['40f6947c', '4a34e604', 0],\n",
       " ['40f6947c', '3e289cca', 0],\n",
       " ['40f6947c', '56ffd650', 0],\n",
       " ['158ad323', '4f1dae94', 0],\n",
       " ['158ad323', '7147f794', 1],\n",
       " ['158ad323', '664f674f', 0],\n",
       " ['158ad323', '58631026', 0],\n",
       " ['158ad323', 'd5d0b74b', 0],\n",
       " ['8f4debfb', '4f1dae94', 1],\n",
       " ['8f4debfb', '8a854e05', 0],\n",
       " ['8f4debfb', '0b0a8689', 0],\n",
       " ['6f5db5d7', '8608de0d', 0],\n",
       " ['6f5db5d7', '9919f572', 0],\n",
       " ['6f5db5d7', '7e73b091', 0],\n",
       " ['6f5db5d7', 'd14ba2f6', 0],\n",
       " ['6f5db5d7', '4f95c9f7', 1],\n",
       " ['6f5db5d7', '1127ae4a', 0],\n",
       " ['ca8e00bb', 'c93bda0e', 1],\n",
       " ['ca8e00bb', '1345d7ca', 0],\n",
       " ['ca8e00bb', 'ec3ea6c5', 0],\n",
       " ['69e2fcf6', 'c93bda0e', 0],\n",
       " ['69e2fcf6', '9ea8c034', 0],\n",
       " ['69e2fcf6', 'c2f1bc3f', 1],\n",
       " ['69e2fcf6', 'ec3ea6c5', 0],\n",
       " ['69e2fcf6', 'eb84ec44', 0],\n",
       " ['69e2fcf6', '0b0a8689', 0],\n",
       " ['7bc2c761', '4f1dae94', 0],\n",
       " ['7bc2c761', '1eb451de', 1],\n",
       " ['7bc2c761', 'cbc054d4', 0],\n",
       " ['7bc2c761', '09e8df3c', 0],\n",
       " ['7bc2c761', 'd14ba2f6', 0],\n",
       " ['a6146c9b', 'febf0ee9', 0],\n",
       " ['a6146c9b', '1eb451de', 0],\n",
       " ['4fa21bf0', '9919f572', 1],\n",
       " ['4fa21bf0', '4b3390be', 0],\n",
       " ['4fa21bf0', '58631026', 0],\n",
       " ['dd9919f9', '0002fd8e', 0],\n",
       " ['dd9919f9', '58631026', 1],\n",
       " ['67332f12', '7147f794', 0],\n",
       " ['67332f12', '8a854e05', 0],\n",
       " ['67332f12', 'd5d0b74b', 1],\n",
       " ['ba52ceea', '7e8aaaff', 0],\n",
       " ['ba52ceea', '86664b5d', 0],\n",
       " ['ba52ceea', '9919f572', 0],\n",
       " ['ba52ceea', '7e73b091', 0],\n",
       " ['ba52ceea', 'c2f1bc3f', 0],\n",
       " ['ba52ceea', '56ffd650', 1],\n",
       " ['fa73ca84', '9466b2c5', 1],\n",
       " ['fa73ca84', '86664b5d', 0],\n",
       " ['fa73ca84', '0b0a8689', 0],\n",
       " ['69a3cae8', '1345d7ca', 0],\n",
       " ['69a3cae8', '664f674f', 0],\n",
       " ['69a3cae8', '7e73b091', 1],\n",
       " ['69a3cae8', 'c3b31640', 0],\n",
       " ['69a3cae8', 'ec3ea6c5', 0],\n",
       " ['69a3cae8', '58631026', 0],\n",
       " ['66073621', 'c93bda0e', 0],\n",
       " ['66073621', '4f1dae94', 0],\n",
       " ['66073621', '9466b2c5', 0],\n",
       " ['66073621', '86664b5d', 1],\n",
       " ['66073621', 'ec3ea6c5', 0],\n",
       " ['66073621', '378a2cdd', 0],\n",
       " ['66073621', 'd5d0b74b', 0],\n",
       " ['921060fb', 'c3b31640', 1],\n",
       " ['d130b6c6', 'e26ac330', 0],\n",
       " ['d130b6c6', 'd14ba2f6', 0],\n",
       " ['d130b6c6', 'c3b31640', 0],\n",
       " ['d130b6c6', '56ffd650', 0],\n",
       " ['d130b6c6', '378a2cdd', 0],\n",
       " ['d802b721', 'febf0ee9', 0],\n",
       " ['d802b721', '4f1dae94', 0],\n",
       " ['d802b721', '9466b2c5', 0],\n",
       " ['d802b721', '09e8df3c', 0],\n",
       " ['d802b721', 'ec3ea6c5', 0],\n",
       " ['d802b721', '4f95c9f7', 0],\n",
       " ['d802b721', '1127ae4a', 0],\n",
       " ['ac15ffcb', '7147f794', 0],\n",
       " ['ac15ffcb', 'e26ac330', 0],\n",
       " ['ac15ffcb', 'cbc054d4', 0],\n",
       " ['ac15ffcb', '5b426fb2', 1],\n",
       " ['7bc1b9a7', '1eb451de', 0],\n",
       " ['7bc1b9a7', '9919f572', 0],\n",
       " ['7bc1b9a7', '09e8df3c', 0],\n",
       " ['7bc1b9a7', '58631026', 0],\n",
       " ['7bc1b9a7', '378a2cdd', 0],\n",
       " ['7bc1b9a7', 'd5d0b74b', 0],\n",
       " ['9578c44a', '8608de0d', 0],\n",
       " ['9578c44a', 'febf0ee9', 1],\n",
       " ['9578c44a', '9ea8c034', 0],\n",
       " ['9578c44a', '86664b5d', 0],\n",
       " ['9578c44a', '5b426fb2', 0],\n",
       " ['9578c44a', '4a34e604', 0],\n",
       " ['9578c44a', 'e3bfda37', 0],\n",
       " ['9578c44a', '3e289cca', 0],\n",
       " ['9578c44a', '378a2cdd', 0],\n",
       " ['9578c44a', 'dd6adea4', 0],\n",
       " ['0779eb73', '7e73b091', 0],\n",
       " ['0779eb73', 'd14ba2f6', 1],\n",
       " ['0779eb73', 'c2f1bc3f', 0],\n",
       " ['0779eb73', 'dd6adea4', 0],\n",
       " ['bdc2ad27', 'a9ac0c00', 1],\n",
       " ['bdc2ad27', '23c1ff32', 0],\n",
       " ['605aaf4a', '88fc381c', 0],\n",
       " ['605aaf4a', '3ee8d131', 1],\n",
       " ['605aaf4a', '23c1ff32', 0],\n",
       " ['cac2259d', 'b5131ff3', 1],\n",
       " ['ffff74a6', 'd09c76ed', 1],\n",
       " ['ffff74a6', 'a14b2f2a', 0],\n",
       " ['2a6e2cc2', '347676a2', 0],\n",
       " ['2a6e2cc2', 'a9ac0c00', 0],\n",
       " ['3fe591d8', '88fc381c', 0],\n",
       " ['3fe591d8', '8afbe973', 0],\n",
       " ['3fe591d8', '51410483', 1],\n",
       " ['da5b9e71', '88fc381c', 1],\n",
       " ['da5b9e71', 'b5131ff3', 0],\n",
       " ['da5b9e71', 'd09c76ed', 0],\n",
       " ['da5b9e71', '15405cf3', 0],\n",
       " ['a382a76b', 'b5131ff3', 0],\n",
       " ['f6ed05fe', '347676a2', 1],\n",
       " ['e0319b48', '68593e37', 1],\n",
       " ['e0319b48', '15405cf3', 0],\n",
       " ['8f2fbf1a', '23c1ff32', 1],\n",
       " ['8f2fbf1a', '51410483', 0],\n",
       " ['71320928', '23c1ff32', 0],\n",
       " ['71320928', 'a14b2f2a', 1],\n",
       " ['cb90cc13', '23c1ff32', 0],\n",
       " ['cb90cc13', '15405cf3', 1],\n",
       " ['9da9fcb5', '29bd168d', 0],\n",
       " ['9da9fcb5', '063cc573', 1],\n",
       " ['9da9fcb5', '428e7c9c', 0],\n",
       " ['9da9fcb5', '4d29e30f', 0],\n",
       " ['9da9fcb5', 'f59d0d52', 0],\n",
       " ['9da9fcb5', '0619eab1', 0],\n",
       " ['9da9fcb5', 'f4f63f50', 0],\n",
       " ['9da9fcb5', 'e3fd535c', 0],\n",
       " ['9da9fcb5', '9e4f8d93', 0],\n",
       " ['9da9fcb5', '66f95ee6', 0],\n",
       " ['9da9fcb5', 'fa6e5eac', 0],\n",
       " ['c6d6b120', 'e53f6e1b', 0],\n",
       " ['c6d6b120', '40c2ba2f', 0],\n",
       " ['c6d6b120', 'f59d0d52', 0],\n",
       " ['c6d6b120', '7eff1cb3', 0],\n",
       " ['c6d6b120', 'a9c441d5', 0],\n",
       " ['c6d6b120', '52b99c8f', 0],\n",
       " ['c6d6b120', '8463c385', 0],\n",
       " ['c6d6b120', 'f4f63f50', 1],\n",
       " ['c6d6b120', 'aec6bb74', 0],\n",
       " ['c6d6b120', '4c880c8d', 0],\n",
       " ['48104aaf', 'f59d0d52', 0],\n",
       " ['48104aaf', '7ce6cda0', 0],\n",
       " ['48104aaf', '7db61e9d', 1],\n",
       " ['48104aaf', '2391e32a', 0],\n",
       " ['48104aaf', 'aec6bb74', 0],\n",
       " ['48104aaf', '06c514ff', 0],\n",
       " ['48104aaf', 'c0f8a1de', 0],\n",
       " ['48104aaf', '504670ef', 0],\n",
       " ['6c761c48', '28525e7b', 0],\n",
       " ['6c761c48', '02225a68', 0],\n",
       " ['6c761c48', 'f59d0d52', 1],\n",
       " ['6c761c48', '3b6fe1c3', 0],\n",
       " ['6c761c48', 'f7c482e4', 0],\n",
       " ['6c761c48', 'aec6bb74', 0],\n",
       " ['6c761c48', 'e3fd535c', 0],\n",
       " ['6c761c48', '35830f5e', 0],\n",
       " ['6c761c48', '504670ef', 0],\n",
       " ['f1424035', 'ea0348cf', 0],\n",
       " ['f1424035', '28525e7b', 0],\n",
       " ['f1424035', 'd493a2cc', 1],\n",
       " ['f1424035', 'e564a46a', 0],\n",
       " ['f1424035', '2cc2656f', 0],\n",
       " ['0b8a1552', 'e53f6e1b', 0],\n",
       " ['0b8a1552', 'b56806e2', 0],\n",
       " ['0b8a1552', 'd493a2cc', 0],\n",
       " ['0b8a1552', '2d835c8a', 0],\n",
       " ['0b8a1552', 'f59d0d52', 0],\n",
       " ['0b8a1552', 'f7c482e4', 0],\n",
       " ['0b8a1552', 'e3fd535c', 0],\n",
       " ['0b8a1552', '8d64bce9', 0],\n",
       " ['0b8a1552', '66f95ee6', 0],\n",
       " ['0b8a1552', 'c0f8a1de', 0],\n",
       " ['0b8a1552', '504670ef', 1],\n",
       " ['c26df6f2', '3d55fc1e', 0],\n",
       " ['c26df6f2', '785ecc04', 0],\n",
       " ['c26df6f2', 'b56806e2', 0],\n",
       " ['c26df6f2', '2d835c8a', 0],\n",
       " ['c26df6f2', 'a9c441d5', 0],\n",
       " ['c26df6f2', '52b99c8f', 1],\n",
       " ['c26df6f2', '2391e32a', 0],\n",
       " ['0a0635b9', '2d835c8a', 0],\n",
       " ['0a0635b9', '0619eab1', 0],\n",
       " ['0a0635b9', '4703dc7c', 0],\n",
       " ['d5c822d6', '3d55fc1e', 0],\n",
       " ['d5c822d6', '29bd168d', 0],\n",
       " ['d5c822d6', '063cc573', 0],\n",
       " ['d5c822d6', '4d29e30f', 0],\n",
       " ['d5c822d6', '6e6ec51d', 1],\n",
       " ['d5c822d6', '52b99c8f', 0],\n",
       " ['d5c822d6', '9e4f8d93', 0],\n",
       " ['d5c822d6', '924af5b4', 0],\n",
       " ['bc96d1bf', '063cc573', 0],\n",
       " ['bc96d1bf', 'f7c482e4', 0],\n",
       " ['bc96d1bf', '7eff1cb3', 0],\n",
       " ['bc96d1bf', 'd68094ed', 0],\n",
       " ['fdf633ee', '063cc573', 0],\n",
       " ['fdf633ee', 'e53f6e1b', 0],\n",
       " ['fdf633ee', '785ecc04', 0],\n",
       " ['fdf633ee', 'a9c441d5', 1],\n",
       " ['fdf633ee', '2cc2656f', 0],\n",
       " ['fdf633ee', '35830f5e', 0],\n",
       " ['39154dc0', '5434768b', 0],\n",
       " ['39154dc0', '428e7c9c', 0],\n",
       " ['39154dc0', '52b99c8f', 0],\n",
       " ['39154dc0', '9e4f8d93', 0],\n",
       " ['8790249b', '21af6791', 0],\n",
       " ['8790249b', 'de2d36e7', 0],\n",
       " ['8790249b', '7eff1cb3', 0],\n",
       " ['8790249b', '8463c385', 0],\n",
       " ['8790249b', 'f4f63f50', 0],\n",
       " ['8790249b', 'aec6bb74', 1],\n",
       " ['8790249b', '2cc2656f', 0],\n",
       " ['8790249b', '924af5b4', 0],\n",
       " ['8790249b', '35830f5e', 0],\n",
       " ['42064b08', '063cc573', 0],\n",
       " ['42064b08', '28525e7b', 0],\n",
       " ['42064b08', '3b6fe1c3', 0],\n",
       " ['42064b08', '9e4f8d93', 0],\n",
       " ['b191086c', '2555ce32', 0],\n",
       " ['b191086c', '6e6ec51d', 0],\n",
       " ['b191086c', '2391e32a', 0],\n",
       " ['b191086c', 'f4f63f50', 0],\n",
       " ['b191086c', 'e3fd535c', 0],\n",
       " ['b191086c', '06c514ff', 0],\n",
       " ['b191086c', 'c0f8a1de', 0],\n",
       " ['b191086c', 'fa6e5eac', 1],\n",
       " ['746f63e4', '4cbc126b', 0],\n",
       " ['746f63e4', '60b987de', 1],\n",
       " ['746f63e4', '06f70dd4', 0],\n",
       " ['746f63e4', 'a9c441d5', 0],\n",
       " ['746f63e4', '24131b97', 0],\n",
       " ['746f63e4', '4703dc7c', 0],\n",
       " ['4dac0d39', '2555ce32', 0],\n",
       " ['4dac0d39', '09e3e3d4', 1],\n",
       " ['4dac0d39', 'db3d42e5', 0],\n",
       " ['4dac0d39', 'ea0348cf', 0],\n",
       " ['4dac0d39', '428e7c9c', 0],\n",
       " ['4dac0d39', '8463c385', 0],\n",
       " ['4dac0d39', '9e4f8d93', 0],\n",
       " ['4dac0d39', '66f95ee6', 0],\n",
       " ['4dac0d39', '2cc2656f', 0],\n",
       " ['c48c99f5', '4cbc126b', 0],\n",
       " ['c48c99f5', '28525e7b', 0],\n",
       " ['c48c99f5', '785ecc04', 0],\n",
       " ['c48c99f5', 'f7c482e4', 0],\n",
       " ['c48c99f5', 'a9c441d5', 0],\n",
       " ['c48c99f5', 'aec6bb74', 0],\n",
       " ['c48c99f5', '8d64bce9', 0],\n",
       " ['248839e5', '3d55fc1e', 0],\n",
       " ['248839e5', '5434768b', 0],\n",
       " ['248839e5', '4d29e30f', 1],\n",
       " ['248839e5', 'd493a2cc', 0],\n",
       " ['248839e5', '2d835c8a', 0],\n",
       " ['248839e5', '21af6791', 0],\n",
       " ['248839e5', '021a0530', 0],\n",
       " ['bc2563c0', '40c2ba2f', 1],\n",
       " ['bc2563c0', '5434768b', 0],\n",
       " ['bc2563c0', '2391e32a', 0],\n",
       " ['42c5e0c3', '4cbc126b', 0],\n",
       " ['42c5e0c3', 'e53f6e1b', 0],\n",
       " ['42c5e0c3', '40c2ba2f', 0],\n",
       " ['42c5e0c3', 'e3fd535c', 0],\n",
       " ['42c5e0c3', '35830f5e', 1],\n",
       " ['aa43ece3', '4cbc126b', 1],\n",
       " ['aa43ece3', '063cc573', 0],\n",
       " ['aa43ece3', 'b56806e2', 0],\n",
       " ['aa43ece3', 'e3fd535c', 0],\n",
       " ['aa43ece3', '9e4f8d93', 0],\n",
       " ['aa43ece3', '66f95ee6', 0],\n",
       " ['aa43ece3', 'c0f8a1de', 0],\n",
       " ['f517ef22', '6e6ec51d', 0],\n",
       " ['f517ef22', '856e25a5', 0],\n",
       " ['f517ef22', '7ce6cda0', 1],\n",
       " ['f517ef22', '35830f5e', 0],\n",
       " ['3a5ee050', '29bd168d', 0],\n",
       " ['3a5ee050', '60b987de', 0],\n",
       " ['3a5ee050', '02225a68', 0],\n",
       " ['3a5ee050', '428e7c9c', 1],\n",
       " ['3a5ee050', '6e6ec51d', 0],\n",
       " ['3a5ee050', '8463c385', 0],\n",
       " ['3a5ee050', '4c880c8d', 0],\n",
       " ['3a5ee050', '504670ef', 0],\n",
       " ['bb890835', 'e53f6e1b', 0],\n",
       " ['bb890835', 'de2d36e7', 0],\n",
       " ['bb890835', '3b6fe1c3', 0],\n",
       " ['bb890835', '7eff1cb3', 0],\n",
       " ['bb890835', '0619eab1', 0],\n",
       " ['bb890835', '24131b97', 0],\n",
       " ['bb890835', '4703dc7c', 0],\n",
       " ['bb890835', 'd68094ed', 0],\n",
       " ['bb890835', '66f95ee6', 0],\n",
       " ['bb890835', '924af5b4', 0],\n",
       " ['f39be485', '3d55fc1e', 1],\n",
       " ['f39be485', '02225a68', 0],\n",
       " ['f39be485', 'd493a2cc', 0],\n",
       " ['f39be485', '3b6fe1c3', 0],\n",
       " ['f39be485', '06f70dd4', 0],\n",
       " ['f39be485', '2391e32a', 0],\n",
       " ['f39be485', 'f4f63f50', 0],\n",
       " ['f39be485', 'aec6bb74', 0],\n",
       " ['f39be485', '06c514ff', 0],\n",
       " ['f39be485', '4c880c8d', 0],\n",
       " ['f39be485', '35830f5e', 0],\n",
       " ['965bd41d', 'f7c482e4', 0],\n",
       " ['965bd41d', '2391e32a', 0],\n",
       " ['965bd41d', 'aec6bb74', 0],\n",
       " ['52b7d2fc', '4cbc126b', 0],\n",
       " ['52b7d2fc', '28525e7b', 0],\n",
       " ['52b7d2fc', '60b987de', 0],\n",
       " ['52b7d2fc', '02225a68', 0],\n",
       " ['52b7d2fc', '3b6fe1c3', 0],\n",
       " ['52b7d2fc', 'e564a46a', 0],\n",
       " ['52b7d2fc', 'f4f63f50', 0],\n",
       " ['6896fb40', '29bd168d', 0],\n",
       " ['6896fb40', '02225a68', 1],\n",
       " ['6896fb40', '4d29e30f', 0],\n",
       " ['6896fb40', '856e25a5', 0],\n",
       " ['6896fb40', '7db61e9d', 0],\n",
       " ['6896fb40', '0b0fe6ca', 0],\n",
       " ['6896fb40', '0619eab1', 0],\n",
       " ['00271fdd', '3d55fc1e', 0],\n",
       " ['00271fdd', 'b56806e2', 0],\n",
       " ['00271fdd', '856e25a5', 0],\n",
       " ['00271fdd', 'de2d36e7', 0],\n",
       " ['00271fdd', '7eff1cb3', 0],\n",
       " ['00271fdd', 'a9c441d5', 0],\n",
       " ['00271fdd', '06c514ff', 0],\n",
       " ['00271fdd', '4fe7db3f', 0],\n",
       " ['00271fdd', 'd68094ed', 0],\n",
       " ['00271fdd', '35830f5e', 0],\n",
       " ['812fe9f1', '7db61e9d', 0],\n",
       " ['812fe9f1', 'e564a46a', 1],\n",
       " ['812fe9f1', 'd68094ed', 0],\n",
       " ['812fe9f1', '543073cd', 0],\n",
       " ['87e95b4c', '69bb2000', 0],\n",
       " ['87e95b4c', '785ecc04', 0],\n",
       " ['87e95b4c', 'f59d0d52', 0],\n",
       " ['87e95b4c', '8463c385', 1],\n",
       " ['87e95b4c', 'f4f63f50', 0],\n",
       " ['87e95b4c', '9e4f8d93', 0],\n",
       " ['87e95b4c', '06c514ff', 0],\n",
       " ['d93b71fa', '40c2ba2f', 0],\n",
       " ['d93b71fa', 'de2d36e7', 1],\n",
       " ['d93b71fa', '0b0fe6ca', 0],\n",
       " ['d93b71fa', 'e564a46a', 0],\n",
       " ['d93b71fa', '06c514ff', 0],\n",
       " ['d93b71fa', 'fa6e5eac', 0],\n",
       " ['48c9c272', '3d55fc1e', 0],\n",
       " ['48c9c272', '4c880c8d', 0],\n",
       " ['48c9c272', '924af5b4', 0],\n",
       " ['48c9c272', '66740633', 0],\n",
       " ['b23e5957', '28525e7b', 0],\n",
       " ['b23e5957', '6e6ec51d', 0],\n",
       " ['b23e5957', 'd68094ed', 1],\n",
       " ['b23e5957', '35830f5e', 0],\n",
       " ['c6f6611a', '2555ce32', 0],\n",
       " ['c6f6611a', 'b56806e2', 0],\n",
       " ['c6f6611a', 'f59d0d52', 0],\n",
       " ['c6f6611a', 'de2d36e7', 0],\n",
       " ['c6f6611a', 'a9c441d5', 0],\n",
       " ['c6f6611a', '543073cd', 0],\n",
       " ['c6f6611a', '66740633', 0],\n",
       " ['c6f6611a', '504670ef', 0],\n",
       " ['90fcad63', '69bb2000', 0],\n",
       " ['90fcad63', '063cc573', 0],\n",
       " ['90fcad63', '60b987de', 0],\n",
       " ['90fcad63', 'b56806e2', 0],\n",
       " ['90fcad63', '428e7c9c', 0],\n",
       " ['90fcad63', '7eff1cb3', 1],\n",
       " ['90fcad63', '24131b97', 0],\n",
       " ['90fcad63', 'c0f8a1de', 0],\n",
       " ['25c777c0', '063cc573', 0],\n",
       " ['25c777c0', '40c2ba2f', 0],\n",
       " ['25c777c0', '28525e7b', 0],\n",
       " ['25c777c0', '1557c42c', 0],\n",
       " ['25c777c0', '856e25a5', 1],\n",
       " ['25c777c0', '7ce6cda0', 0],\n",
       " ['25c777c0', '9e4f8d93', 0],\n",
       " ['25c777c0', '4703dc7c', 0],\n",
       " ['25c777c0', 'fa6e5eac', 0],\n",
       " ['39d5e315', 'de2d36e7', 0],\n",
       " ['39d5e315', '8463c385', 0],\n",
       " ['bb4a09e7', 'f7c482e4', 1],\n",
       " ['bb4a09e7', '4c880c8d', 0],\n",
       " ['5fb676e4', '063cc573', 0],\n",
       " ['5fb676e4', '5434768b', 0],\n",
       " ['5fb676e4', '2d835c8a', 1],\n",
       " ['5fb676e4', '9e4f8d93', 0],\n",
       " ['5fb676e4', '504670ef', 0],\n",
       " ['04e876c3', 'f59d0d52', 0],\n",
       " ['04e876c3', 'aec6bb74', 0],\n",
       " ['04e876c3', 'd68094ed', 0],\n",
       " ['b30d0039', '69bb2000', 0],\n",
       " ['b30d0039', 'e53f6e1b', 0],\n",
       " ['b30d0039', '785ecc04', 0],\n",
       " ['b30d0039', '2d835c8a', 0],\n",
       " ['b30d0039', '856e25a5', 0],\n",
       " ['b30d0039', '06c514ff', 0],\n",
       " ['b30d0039', '66f95ee6', 0],\n",
       " ['58a60f01', '09e3e3d4', 0],\n",
       " ['58a60f01', '1557c42c', 0],\n",
       " ['58a60f01', '6e6ec51d', 0],\n",
       " ['58a60f01', '2d835c8a', 0],\n",
       " ['58a60f01', '0b0fe6ca', 0],\n",
       " ['58a60f01', '2391e32a', 0],\n",
       " ['58a60f01', 'e3fd535c', 0],\n",
       " ['58a60f01', '06c514ff', 1],\n",
       " ['58a60f01', '2cc2656f', 0],\n",
       " ['58a60f01', '543073cd', 0],\n",
       " ['1c08c0d9', '3d55fc1e', 0],\n",
       " ['1c08c0d9', '28525e7b', 0],\n",
       " ['1c08c0d9', '785ecc04', 1],\n",
       " ['1c08c0d9', 'f59d0d52', 0],\n",
       " ['1c08c0d9', '856e25a5', 0],\n",
       " ['1c08c0d9', '0619eab1', 0],\n",
       " ['1c08c0d9', '66f95ee6', 0],\n",
       " ['1c08c0d9', '66740633', 0],\n",
       " ['9cd5caaf', 'db3d42e5', 0],\n",
       " ['9cd5caaf', '8463c385', 0],\n",
       " ['9cd5caaf', '2391e32a', 0],\n",
       " ['9cd5caaf', 'e564a46a', 0],\n",
       " ['a2dcbbd4', '2555ce32', 0],\n",
       " ['a2dcbbd4', '09e3e3d4', 0],\n",
       " ['a2dcbbd4', '29bd168d', 0],\n",
       " ['a2dcbbd4', '428e7c9c', 0],\n",
       " ['a2dcbbd4', 'd493a2cc', 0],\n",
       " ['a2dcbbd4', '2391e32a', 1],\n",
       " ['a2dcbbd4', '24131b97', 0],\n",
       " ['a2dcbbd4', 'c0f8a1de', 0],\n",
       " ['a2dcbbd4', 'fa6e5eac', 0],\n",
       " ['a2dcbbd4', '66740633', 0],\n",
       " ['d9af586a', '785ecc04', 0],\n",
       " ['d9af586a', 'b56806e2', 0],\n",
       " ['14617469', '3d55fc1e', 0],\n",
       " ['14617469', '29bd168d', 0],\n",
       " ['14617469', '785ecc04', 0],\n",
       " ['14617469', '428e7c9c', 0],\n",
       " ['14617469', '0b0fe6ca', 0],\n",
       " ['14617469', '66f95ee6', 0],\n",
       " ['cdffba79', 'ea0348cf', 0],\n",
       " ['cdffba79', '063cc573', 0],\n",
       " ['cdffba79', '28525e7b', 0],\n",
       " ['cdffba79', '21af6791', 0],\n",
       " ['cdffba79', 'aec6bb74', 0],\n",
       " ['cdffba79', '66740633', 0],\n",
       " ['beebcd1c', '09e3e3d4', 0],\n",
       " ['beebcd1c', '29bd168d', 0],\n",
       " ['beebcd1c', '40c2ba2f', 0],\n",
       " ['beebcd1c', '02225a68', 0],\n",
       " ['beebcd1c', '1557c42c', 0],\n",
       " ['beebcd1c', '21af6791', 0],\n",
       " ['beebcd1c', '7db61e9d', 0],\n",
       " ['beebcd1c', 'f4f63f50', 0],\n",
       " ['beebcd1c', '107f27e3', 0],\n",
       " ['beebcd1c', '06c514ff', 0],\n",
       " ['beebcd1c', '543073cd', 0],\n",
       " ['920104c6', '2555ce32', 1],\n",
       " ['920104c6', '7db61e9d', 0],\n",
       " ['920104c6', '9e4f8d93', 0],\n",
       " ['920104c6', 'fa6e5eac', 0],\n",
       " ['03f2fa3e', '28525e7b', 0],\n",
       " ['03f2fa3e', '60b987de', 0],\n",
       " ['03f2fa3e', '856e25a5', 0],\n",
       " ['03f2fa3e', 'a9c441d5', 0],\n",
       " ['03f2fa3e', 'd68094ed', 0],\n",
       " ['8dfc9296', '28525e7b', 0],\n",
       " ['8dfc9296', '7db61e9d', 0],\n",
       " ['8dfc9296', '2cc2656f', 0],\n",
       " ['8dfc9296', 'fa6e5eac', 0],\n",
       " ['ca20f5c9', '4cbc126b', 0],\n",
       " ['ca20f5c9', 'db3d42e5', 0],\n",
       " ['ca20f5c9', '063cc573', 0],\n",
       " ['ca20f5c9', '8d64bce9', 0],\n",
       " ['ca20f5c9', '06c514ff', 0],\n",
       " ['ca20f5c9', 'c0f8a1de', 1],\n",
       " ['a5c46c0a', 'db3d42e5', 0],\n",
       " ['a5c46c0a', 'ea0348cf', 0],\n",
       " ['a5c46c0a', '7ce6cda0', 0],\n",
       " ['a5c46c0a', '0b0fe6ca', 0],\n",
       " ['a5c46c0a', 'c0f8a1de', 0],\n",
       " ['a5c46c0a', 'fa6e5eac', 0],\n",
       " ['e7f074d3', '69bb2000', 0],\n",
       " ['e7f074d3', '3d55fc1e', 0],\n",
       " ['e7f074d3', '29bd168d', 0],\n",
       " ['e7f074d3', '02225a68', 0],\n",
       " ['e7f074d3', 'f59d0d52', 0],\n",
       " ['e7f074d3', '856e25a5', 0],\n",
       " ['e7f074d3', '0b0fe6ca', 1],\n",
       " ['e7f074d3', '021a0530', 0],\n",
       " ['2721e7db', '09e3e3d4', 0],\n",
       " ['2721e7db', '428e7c9c', 0],\n",
       " ['2721e7db', '856e25a5', 0],\n",
       " ['2721e7db', 'de2d36e7', 0],\n",
       " ['2721e7db', '4703dc7c', 0],\n",
       " ['2721e7db', '924af5b4', 1],\n",
       " ['b8bc92eb', 'd493a2cc', 0],\n",
       " ['b8bc92eb', '7db61e9d', 0],\n",
       " ['b8bc92eb', '0b0fe6ca', 0],\n",
       " ['b8bc92eb', 'e564a46a', 0],\n",
       " ['b8bc92eb', 'e3fd535c', 0],\n",
       " ['b8bc92eb', '4703dc7c', 0],\n",
       " ['b8bc92eb', '543073cd', 0],\n",
       " ['fb7db714', '2555ce32', 0],\n",
       " ['fb7db714', '29bd168d', 0],\n",
       " ['fb7db714', '0619eab1', 0],\n",
       " ['fb7db714', '52b99c8f', 0],\n",
       " ['fb7db714', '8463c385', 0],\n",
       " ['fb7db714', '4c880c8d', 0],\n",
       " ['b2c12d4c', '29bd168d', 0],\n",
       " ['b2c12d4c', '428e7c9c', 0],\n",
       " ['b2c12d4c', '0619eab1', 0],\n",
       " ['b2c12d4c', '52b99c8f', 0],\n",
       " ['b2c12d4c', 'e564a46a', 0],\n",
       " ['b2c12d4c', '4c880c8d', 0],\n",
       " ['b2c12d4c', '543073cd', 1],\n",
       " ['cf5f068a', '2d835c8a', 0],\n",
       " ['cf5f068a', 'e564a46a', 0],\n",
       " ['cf5f068a', 'aec6bb74', 0],\n",
       " ['cf5f068a', 'e3fd535c', 0],\n",
       " ['3e047252', '06775aa9', 1],\n",
       " ['3e047252', 'd8831af8', 0],\n",
       " ['3e047252', 'db5f48b9', 0],\n",
       " ['b97767fe', '93a0e603', 0],\n",
       " ['b97767fe', '4bd9d5dd', 0],\n",
       " ['b97767fe', '600cb994', 0],\n",
       " ['b97767fe', '43b4968e', 0],\n",
       " ['b97767fe', 'db5f48b9', 1],\n",
       " ['394b2536', 'de9229ef', 1],\n",
       " ['394b2536', '1b7d3038', 0],\n",
       " ['b31c37db', 'd7687ee6', 1],\n",
       " ['b31c37db', '1b7d3038', 0],\n",
       " ['b31c37db', '32a13b76', 0],\n",
       " ['9e358158', '27ee8d3a', 1],\n",
       " ['9e358158', '06775aa9', 0],\n",
       " ['9e358158', 'db5f48b9', 0],\n",
       " ['82218899', '43b4968e', 0],\n",
       " ['84762508', '2bc5fb09', 0],\n",
       " ['84762508', 'ea78869c', 0],\n",
       " ['84762508', '65cc98ed', 0],\n",
       " ['84762508', 'a5750bc8', 0],\n",
       " ['84762508', 'e87b8508', 1],\n",
       " ['84762508', 'dca56978', 0],\n",
       " ['84762508', '728c2b2c', 0],\n",
       " ['84762508', 'fe8d465d', 0],\n",
       " ['bb270083', '0dbe3f29', 0],\n",
       " ['bb270083', '728c2b2c', 1],\n",
       " ['473e430f', '15643fe8', 0],\n",
       " ['473e430f', 'a5750bc8', 1],\n",
       " ['473e430f', '728c2b2c', 0],\n",
       " ['71181d6d', '0bfaf060', 1],\n",
       " ['d442a546', 'c0858ec9', 1],\n",
       " ['d442a546', '8c513e58', 0],\n",
       " ['d442a546', '7fc28ddc', 0],\n",
       " ['d442a546', 'e5c6105a', 0],\n",
       " ['2bab8ecd', 'e2c26661', 1],\n",
       " ['9e7b02d3', '8a587ca9', 1],\n",
       " ['72e43ed0', 'e2c26661', 0],\n",
       " ['e6c08099', '9e8b9d5f', 1],\n",
       " ['c58a331a', 'e537c02e', 1],\n",
       " ['0c879d37', 'f4480a33', 0],\n",
       " ['0c879d37', 'dc3dd984', 0],\n",
       " ['0c879d37', '6dc66f70', 1],\n",
       " ['a75252bb', 'b6ef8fc1', 0],\n",
       " ['a75252bb', 'd8581cea', 0],\n",
       " ['82c1ebfd', 'd8581cea', 0],\n",
       " ['82c1ebfd', 'dc3dd984', 0],\n",
       " ['82c1ebfd', 'ab602931', 0],\n",
       " ['82c1ebfd', '17d8d0b1', 0],\n",
       " ['880d5855', '6749f71f', 1],\n",
       " ['880d5855', 'f4480a33', 0],\n",
       " ['880d5855', 'd0422737', 0],\n",
       " ['e73f0f7b', 'b6ef8fc1', 0],\n",
       " ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df 내 모든 마크다운셀에 대한 대응코드셀 triplet 과 랜덤(label==0) triplet 을 가져옴\n",
    "generate_triplet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "664f758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>source</th>\n",
       "      <th>rank</th>\n",
       "      <th>ancestor_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>pct_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>386d31f0</td>\n",
       "      <td>code</td>\n",
       "      <td>import numpy import panda import tensorflow import tf2_0_baseline_w_bert tf2baseline script import tf2_0_baseline_w_...</td>\n",
       "      <td>2</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>16435878</td>\n",
       "      <td>code</td>\n",
       "      <td>del_all_flags flag flags_dict flag _flags keys_list flags_dict keys_list flag __delattr__ del_all_flags absl flag fl...</td>\n",
       "      <td>4</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "      <td>0.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>f4bb282f</td>\n",
       "      <td>code</td>\n",
       "      <td>bert_config modeling bertconfig from_json_file flag bert_config_file tf2baseline validate_flags_or_throw bert_config...</td>\n",
       "      <td>6</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>4cc5ee5a</td>\n",
       "      <td>code</td>\n",
       "      <td>test_answers_df read_json kaggle working prediction json</td>\n",
       "      <td>8</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003f36ab2c577d</td>\n",
       "      <td>215ec8c0</td>\n",
       "      <td>code</td>\n",
       "      <td>create_short_answer entry entry short_answers_score return answer short_answer entry short_answers short_answer star...</td>\n",
       "      <td>10</td>\n",
       "      <td>8508be37</td>\n",
       "      <td>3bde8d65a3508b</td>\n",
       "      <td>0.47619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id   cell_id cell_type                                                                                                                   source rank ancestor_id  \\\n",
       "0  003f36ab2c577d  386d31f0      code  import numpy import panda import tensorflow import tf2_0_baseline_w_bert tf2baseline script import tf2_0_baseline_w_...    2    8508be37   \n",
       "1  003f36ab2c577d  16435878      code  del_all_flags flag flags_dict flag _flags keys_list flags_dict keys_list flag __delattr__ del_all_flags absl flag fl...    4    8508be37   \n",
       "2  003f36ab2c577d  f4bb282f      code  bert_config modeling bertconfig from_json_file flag bert_config_file tf2baseline validate_flags_or_throw bert_config...    6    8508be37   \n",
       "3  003f36ab2c577d  4cc5ee5a      code                                                                 test_answers_df read_json kaggle working prediction json    8    8508be37   \n",
       "4  003f36ab2c577d  215ec8c0      code  create_short_answer entry entry short_answers_score return answer short_answer entry short_answers short_answer star...   10    8508be37   \n",
       "\n",
       "        parent_id  pct_rank  \n",
       "0  3bde8d65a3508b  0.095238  \n",
       "1  3bde8d65a3508b  0.190476  \n",
       "2  3bde8d65a3508b  0.285714  \n",
       "3  3bde8d65a3508b  0.380952  \n",
       "4  3bde8d65a3508b   0.47619  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 6 8 10 11 12 14 16 18 20]\n",
      "[0 0 0 1 0 0 0 0 0 0 0]\n",
      "3e1430c4\n",
      "386d31f0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## generate_triplet 함수 TEST\n",
    "\n",
    "for id, df_tmp in tqdm(df.groupby('id')): #같은 노트북\n",
    "#     print(id)\n",
    "    display(df_tmp.head())\n",
    "    df_tmp_code = df_tmp[df_tmp['cell_type']=='code'] #코드셀\n",
    "#     print(df_tmp_code['rank'].values)\n",
    "    df_tmp_code_rank = df_tmp_code['rank'].values #코드셀 rank(순서)\n",
    "    print(df_tmp_code_rank)\n",
    "    df_tmp_code_cell_id = df_tmp_code['cell_id'].values #코드셀 셀id\n",
    "    \n",
    "    \n",
    "    df_tmp_markdown = df_tmp[df_tmp['cell_type']=='markdown'] #마크다운만\n",
    "#     print(df_tmp_markdown[['cell_id', 'rank']].values)\n",
    "    for cell_id, rank in df_tmp_markdown[['cell_id', 'rank']].values:\n",
    "        labels = np.array([(r==(rank+1)) for r in df_tmp_code_rank]).astype('int')\n",
    "        print(labels)\n",
    "        print(cell_id)\n",
    "        for cid, label in zip(df_tmp_code_cell_id, labels):\n",
    "            print(cid,label)\n",
    "            break\n",
    "        break\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b518ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric.py\n",
    "from bisect import bisect\n",
    "\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0  # twice the maximum possible inversions across all instances\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1d670d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "try:\n",
    "    from transformers import DistilBertModel, DistilBertTokenizer\n",
    "except:\n",
    "    !pip install transformers\n",
    "    !pip install sentencepiece\n",
    "    from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "\n",
    "\n",
    "MAX_LEN = 128\n",
    "    \n",
    "class MarkdownModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MarkdownModel, self).__init__()\n",
    "        # pretrained 된 distillbert 모델\n",
    "        self.distill_bert = AutoModel.from_pretrained(\"./input/mymodelbertsmallpretrained/checkpoint-120000\")\n",
    "        self.top = nn.Linear(512, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        x = self.distill_bert(ids, mask)[0]\n",
    "        x = self.dropout(x)\n",
    "        x = self.top(x[:, 0, :])\n",
    "        x = torch.sigmoid(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5384304e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101,  1001,  1001,  1001,  1001,  2085,  1010,  2057,  2735,  1036,\n",
       "         20932,  1012,  1046,  3385,  1036,  2046,  1037,  1036, 12339,  1012,\n",
       "         20116,  2615,  1036,  5371,  1012,   102,  3231,  1035,  6998,  1035,\n",
       "          1040,  2546,  1027, 22851,  1012,  3191,  1035,  1046,  3385,  1006,\n",
       "          1000,  1013, 10556, 24679,  1013,  2551,  1013, 20932,  1012,  1046,\n",
       "          3385,  1000,  1007,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "class MarkdownDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, max_len, mode='train'):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        # pretrained 된 bert-small 토크나이저\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-small\", do_lower_case=True)\n",
    "        self.mode=mode\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df[index]\n",
    "\n",
    "        label = row[-1]\n",
    "\n",
    "        txt = dict_cellid_source[row[0]] + '[SEP]' + dict_cellid_source[row[1]]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            txt,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = torch.LongTensor(inputs['input_ids'])\n",
    "        mask = torch.LongTensor(inputs['attention_mask'])\n",
    "\n",
    "        return ids, mask, torch.FloatTensor([label])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "train_ds = MarkdownDataset(triplets, max_len=MAX_LEN, mode='test') #input data 가 triplet label 이 학습 label이 됨\n",
    "\n",
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eae52333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, epoch):\n",
    "    if epoch < 1:\n",
    "        lr = 5e-5\n",
    "    elif epoch < 2:\n",
    "        lr = 5e-5\n",
    "    elif epoch < 5:\n",
    "        lr = 5e-5\n",
    "    else:\n",
    "        lr = 5e-5\n",
    "\n",
    "    for p in optimizer.param_groups:\n",
    "        p['lr'] = lr\n",
    "    return lr\n",
    "    \n",
    "def get_optimizer(net):\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=3e-4, betas=(0.9, 0.999),\n",
    "                                 eps=1e-8 ) #1e-08)\n",
    "    return optimizer\n",
    "\n",
    "BS = 128 + 128\n",
    "NW = 8\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BS, shuffle=True, num_workers=NW,\n",
    "                          pin_memory=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f580ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data):\n",
    "    return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n",
    "\n",
    "def validate(model, val_loader, mode='train'):\n",
    "    model.eval()\n",
    "    \n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "\n",
    "            pred = model(inputs[0], inputs[1])\n",
    "\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            if mode=='test':\n",
    "              labels.append(target.detach().cpu().numpy().ravel())\n",
    "    if mode=='test':\n",
    "        return np.concatenate(preds)\n",
    "    else:\n",
    "        return np.concatenate(labels), np.concatenate(preds)\n",
    "\n",
    "def train(model, train_loader, epochs, Type='markdown'):\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    optimizer = get_optimizer(model)\n",
    "\n",
    "    mixed_precision = True\n",
    "    try:  \n",
    "        from apex import amp\n",
    "    except:\n",
    "        mixed_precision = False  # not installed\n",
    "\n",
    "    # model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=1)\n",
    "    \n",
    "\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    \n",
    "    for e in range(epochs):   \n",
    "        model.train()\n",
    "        tbar = tqdm(train_loader, file=sys.stdout)\n",
    "        \n",
    "        lr = adjust_lr(optimizer, e)\n",
    "        \n",
    "        loss_list = []\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(inputs[0], inputs[1])\n",
    "\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "            labels.append(target.detach().cpu().numpy().ravel())\n",
    "            \n",
    "            avg_loss = np.round(np.mean(loss_list), 4)\n",
    "\n",
    "            tbar.set_description(f\"Epoch {e+1} Loss: {avg_loss} lr: {lr}\")\n",
    "\n",
    "        \n",
    "        output_model_file = f\"./my_own_model_{e}.bin\"\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ff774dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5256570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./input/mymodelbertsmallpretrained/checkpoint-120000 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./input/mymodelbertsmallpretrained/checkpoint-120000 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                  | 0/65 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|                                                                                                  | 0/65 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 4.75 GiB total capacity; 3.00 GiB already allocated; 62.00 MiB free; 3.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MarkdownModel()\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmarkdown\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, epochs, Type)\u001b[0m\n\u001b[1;32m     54\u001b[0m inputs, target \u001b[38;5;241m=\u001b[39m read_data(data)\n\u001b[1;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 57\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, target)\n\u001b[1;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mMarkdownModel.forward\u001b[0;34m(self, ids, mask)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids, mask):\n\u001b[0;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistill_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop(x[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:1017\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1008\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1010\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1011\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1012\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1016\u001b[0m )\n\u001b[0;32m-> 1017\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1030\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:606\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    597\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    598\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    599\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    604\u001b[0m     )\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 606\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    483\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    492\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    415\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    422\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 423\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    433\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py:312\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[0;32m--> 312\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    314\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# Further calls to cross_attention layer can then reuse all cross-attention\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# if encoder bi-directional self-attention `past_key_value` is always `None`\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 4.75 GiB total capacity; 3.00 GiB already allocated; 62.00 MiB free; 3.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = MarkdownModel()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "model = train(model, train_loader, epochs=1, Type='markdown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b504468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07825f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72f661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29400ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd7f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d99b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67aa9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5306907e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b261ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56c043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929ce5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
